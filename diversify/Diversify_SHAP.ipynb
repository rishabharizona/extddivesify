{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rishabharizona/extddivesify.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WJCjhGADIiQ",
        "outputId": "bd9e00a2-966c-4080-d49d-5bd49ffeafcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'extddivesify'...\n",
            "remote: Enumerating objects: 1317, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 1317 (delta 34), reused 11 (delta 11), pack-reused 1272 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1317/1317), 18.65 MiB | 7.24 MiB/s, done.\n",
            "Resolving deltas: 100% (746/746), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then install from local requirements file\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "DuunLRmSinDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd extddivesify/diversify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jyAKOgoDXTc",
        "outputId": "b57e5cbf-e1a0-4b5c-87f1-6da839269e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extddivesify/diversify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip\n",
        "!unzip diversity_emg.zip && mv emg data/\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p ./data/train_output/act/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX6NnuejXXlF",
        "outputId": "d4e2eff8-67a8-423e-ab36-80ad12c8a92f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-01 17:49:46--  https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip\n",
            "Resolving wjdcloud.blob.core.windows.net (wjdcloud.blob.core.windows.net)... 20.60.131.4\n",
            "Connecting to wjdcloud.blob.core.windows.net (wjdcloud.blob.core.windows.net)|20.60.131.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20237244 (19M) [application/zip]\n",
            "Saving to: ‘diversity_emg.zip’\n",
            "\n",
            "diversity_emg.zip   100%[===================>]  19.30M  4.48MB/s    in 5.6s    \n",
            "\n",
            "2025-07-01 17:49:52 (3.46 MB/s) - ‘diversity_emg.zip’ saved [20237244/20237244]\n",
            "\n",
            "Archive:  diversity_emg.zip\n",
            "   creating: emg/\n",
            "   creating: emg/20/\n",
            "  inflating: emg/20/1_raw_data_11-41_22.03.16.txt  \n",
            "  inflating: emg/20/2_raw_data_11-43_22.03.16.txt  \n",
            "   creating: emg/35/\n",
            "  inflating: emg/35/2_raw_data_10-05_13.04.16.txt  \n",
            "  inflating: emg/35/1_raw_data_10-03_13.04.16.txt  \n",
            "   creating: emg/09/\n",
            "  inflating: emg/09/1_raw_data_12-41_23.03.16.txt  \n",
            "  inflating: emg/09/2_raw_data_12-43_23.03.16.txt  \n",
            "   creating: emg/15/\n",
            "  inflating: emg/15/2_raw_data_08-51_13.04.16.txt  \n",
            "  inflating: emg/15/1_raw_data_08-49_13.04.16.txt  \n",
            "   creating: emg/22/\n",
            "  inflating: emg/22/1_raw_data_12-37_28.03.16.txt  \n",
            "  inflating: emg/22/2_raw_data_12-39_28.03.16.txt  \n",
            "   creating: emg/13/\n",
            "  inflating: emg/13/1_raw_data_13-26_21.03.16.txt  \n",
            "  inflating: emg/13/2_raw_data_13-29_21.03.16.txt  \n",
            "   creating: emg/30/\n",
            "  inflating: emg/30/1_raw_data_09-49_21.03.16.txt  \n",
            "  inflating: emg/30/2_raw_data_09-50_21.03.16.txt  \n",
            "   creating: emg/01/\n",
            "  inflating: emg/01/2_raw_data_13-13_22.03.16.txt  \n",
            "  inflating: emg/01/1_raw_data_13-12_22.03.16.txt  \n",
            "   creating: emg/28/\n",
            "  inflating: emg/28/1_raw_data_12-10_15.04.16.txt  \n",
            "  inflating: emg/28/2_raw_data_12-11_15.04.16.txt  \n",
            "  inflating: emg/README.txt          \n",
            "   creating: emg/34/\n",
            "  inflating: emg/34/2_raw_data_10-53_07.04.16.txt  \n",
            "  inflating: emg/34/1_raw_data_10-51_07.04.16.txt  \n",
            "   creating: emg/06/\n",
            "  inflating: emg/06/2_raw_data_10-40_11.04.16.txt  \n",
            "  inflating: emg/06/1_raw_data_10-38_11.04.16.txt  \n",
            "   creating: emg/25/\n",
            "  inflating: emg/25/2_raw_data_14-53_24.04.16.txt  \n",
            "  inflating: emg/25/1_raw_data_14-51_24.04.16.txt  \n",
            "   creating: emg/26/\n",
            "  inflating: emg/26/2_raw_data_10-23_29.03.16.txt  \n",
            "  inflating: emg/26/1_raw_data_10-22_29.03.16.txt  \n",
            "   creating: emg/36/\n",
            "  inflating: emg/36/1_raw_data_13-03_15.04.16.txt  \n",
            "  inflating: emg/36/2_raw_data_13-04_15.04.16.txt  \n",
            "   creating: emg/04/\n",
            "  inflating: emg/04/1_raw_data_18-02_24.04.16.txt  \n",
            "  inflating: emg/04/2_raw_data_18-03_24.04.16.txt  \n",
            "   creating: emg/14/\n",
            "  inflating: emg/14/2_raw_data_09-51_15.04.16.txt  \n",
            "  inflating: emg/14/1_raw_data_09-50_15.04.16.txt  \n",
            "   creating: emg/02/\n",
            "  inflating: emg/02/2_raw_data_14-21_22.03.16.txt  \n",
            "  inflating: emg/02/1_raw_data_14-19_22.03.16.txt  \n",
            "   creating: emg/27/\n",
            "  inflating: emg/27/1_raw_data_12-19_06.04.16.txt  \n",
            "  inflating: emg/27/2_raw_data_12-20_06.04.16.txt  \n",
            "   creating: emg/17/\n",
            "  inflating: emg/17/2_raw_data_11-20_23.03.16.txt  \n",
            "  inflating: emg/17/1_raw_data_11-19_23.03.16.txt  \n",
            "  inflating: emg/emg_x.npy           \n",
            "   creating: emg/03/\n",
            "  inflating: emg/03/1_raw_data_09-32_11.04.16.txt  \n",
            "  inflating: emg/03/2_raw_data_09-34_11.04.16.txt  \n",
            "   creating: emg/31/\n",
            "  inflating: emg/31/2_raw_data_11-16_11.04.16.txt  \n",
            "  inflating: emg/31/1_raw_data_11-15_11.04.16.txt  \n",
            "   creating: emg/11/\n",
            "  inflating: emg/11/1_raw_data_13-11_18.03.16.txt  \n",
            "  inflating: emg/11/2_raw_data_13-13_18.03.16.txt  \n",
            "   creating: emg/21/\n",
            "  inflating: emg/21/2_raw_data_20-30_24.04.16.txt  \n",
            "  inflating: emg/21/1_raw_data_20-28_24.04.16.txt  \n",
            "   creating: emg/10/\n",
            "  inflating: emg/10/1_raw_data_11-08_21.03.16.txt  \n",
            "  inflating: emg/10/2_raw_data_11-10_21.03.16.txt  \n",
            "   creating: emg/05/\n",
            "  inflating: emg/05/2_raw_data_10-29_30.03.16.txt  \n",
            "  inflating: emg/05/1_raw_data_10-28_30.03.16.txt  \n",
            "   creating: emg/19/\n",
            "  inflating: emg/19/1_raw_data_12-10_26.04.16.txt  \n",
            "  inflating: emg/19/2_raw_data_12-11_26.04.16.txt  \n",
            "   creating: emg/08/\n",
            "  inflating: emg/08/1_raw_data_12-14_23.03.16.txt  \n",
            "  inflating: emg/08/2_raw_data_12-16_23.03.16.txt  \n",
            "   creating: emg/12/\n",
            "  inflating: emg/12/2_raw_data_11-36_28.03.16.txt  \n",
            "  inflating: emg/12/1_raw_data_11-35_28.03.16.txt  \n",
            "   creating: emg/16/\n",
            "  inflating: emg/16/2_raw_data_12-14_25.04.16.txt  \n",
            "  inflating: emg/16/1_raw_data_12-12_25.04.16.txt  \n",
            "   creating: emg/24/\n",
            "  inflating: emg/24/1_raw_data_10-16_12.04.16.txt  \n",
            "  inflating: emg/24/2_raw_data_10-17_12.04.16.txt  \n",
            "   creating: emg/33/\n",
            "  inflating: emg/33/2_raw_data_09-50_12.04.16.txt  \n",
            "  inflating: emg/33/1_raw_data_09-49_12.04.16.txt  \n",
            "   creating: emg/07/\n",
            "  inflating: emg/07/2_raw_data_18-50_22.03.16.txt  \n",
            "  inflating: emg/07/1_raw_data_18-48_22.03.16.txt  \n",
            "   creating: emg/32/\n",
            "  inflating: emg/32/2_raw_data_12-06_27.04.16.txt  \n",
            "  inflating: emg/32/1_raw_data_12-04_27.04.16.txt  \n",
            "   creating: emg/18/\n",
            "  inflating: emg/18/1_raw_data_12-35_21.03.16.txt  \n",
            "  inflating: emg/18/2_raw_data_12-37_21.03.16.txt  \n",
            "   creating: emg/23/\n",
            "  inflating: emg/23/1_raw_data_13-18_05.04.16.txt  \n",
            "  inflating: emg/23/2_raw_data_13-19_05.04.16.txt  \n",
            "   creating: emg/29/\n",
            "  inflating: emg/29/2_raw_data_10-18_15.04.16.txt  \n",
            "  inflating: emg/29/1_raw_data_10-17_15.04.16.txt  \n",
            "  inflating: emg/emg_y.npy           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip diversity_emg.zip\n",
        "!mkdir -p ./data/emg\n",
        "!mv emg/* ./data/emg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEk8bPR0XbuD",
        "outputId": "8ec47524-f7c9-4e2b-9f16-4a6b90d7c7c3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  diversity_emg.zip\n",
            "   creating: emg/\n",
            "   creating: emg/20/\n",
            "  inflating: emg/20/1_raw_data_11-41_22.03.16.txt  \n",
            "  inflating: emg/20/2_raw_data_11-43_22.03.16.txt  \n",
            "   creating: emg/35/\n",
            "  inflating: emg/35/2_raw_data_10-05_13.04.16.txt  \n",
            "  inflating: emg/35/1_raw_data_10-03_13.04.16.txt  \n",
            "   creating: emg/09/\n",
            "  inflating: emg/09/1_raw_data_12-41_23.03.16.txt  \n",
            "  inflating: emg/09/2_raw_data_12-43_23.03.16.txt  \n",
            "   creating: emg/15/\n",
            "  inflating: emg/15/2_raw_data_08-51_13.04.16.txt  \n",
            "  inflating: emg/15/1_raw_data_08-49_13.04.16.txt  \n",
            "   creating: emg/22/\n",
            "  inflating: emg/22/1_raw_data_12-37_28.03.16.txt  \n",
            "  inflating: emg/22/2_raw_data_12-39_28.03.16.txt  \n",
            "   creating: emg/13/\n",
            "  inflating: emg/13/1_raw_data_13-26_21.03.16.txt  \n",
            "  inflating: emg/13/2_raw_data_13-29_21.03.16.txt  \n",
            "   creating: emg/30/\n",
            "  inflating: emg/30/1_raw_data_09-49_21.03.16.txt  \n",
            "  inflating: emg/30/2_raw_data_09-50_21.03.16.txt  \n",
            "   creating: emg/01/\n",
            "  inflating: emg/01/2_raw_data_13-13_22.03.16.txt  \n",
            "  inflating: emg/01/1_raw_data_13-12_22.03.16.txt  \n",
            "   creating: emg/28/\n",
            "  inflating: emg/28/1_raw_data_12-10_15.04.16.txt  \n",
            "  inflating: emg/28/2_raw_data_12-11_15.04.16.txt  \n",
            "  inflating: emg/README.txt          \n",
            "   creating: emg/34/\n",
            "  inflating: emg/34/2_raw_data_10-53_07.04.16.txt  \n",
            "  inflating: emg/34/1_raw_data_10-51_07.04.16.txt  \n",
            "   creating: emg/06/\n",
            "  inflating: emg/06/2_raw_data_10-40_11.04.16.txt  \n",
            "  inflating: emg/06/1_raw_data_10-38_11.04.16.txt  \n",
            "   creating: emg/25/\n",
            "  inflating: emg/25/2_raw_data_14-53_24.04.16.txt  \n",
            "  inflating: emg/25/1_raw_data_14-51_24.04.16.txt  \n",
            "   creating: emg/26/\n",
            "  inflating: emg/26/2_raw_data_10-23_29.03.16.txt  \n",
            "  inflating: emg/26/1_raw_data_10-22_29.03.16.txt  \n",
            "   creating: emg/36/\n",
            "  inflating: emg/36/1_raw_data_13-03_15.04.16.txt  \n",
            "  inflating: emg/36/2_raw_data_13-04_15.04.16.txt  \n",
            "   creating: emg/04/\n",
            "  inflating: emg/04/1_raw_data_18-02_24.04.16.txt  \n",
            "  inflating: emg/04/2_raw_data_18-03_24.04.16.txt  \n",
            "   creating: emg/14/\n",
            "  inflating: emg/14/2_raw_data_09-51_15.04.16.txt  \n",
            "  inflating: emg/14/1_raw_data_09-50_15.04.16.txt  \n",
            "   creating: emg/02/\n",
            "  inflating: emg/02/2_raw_data_14-21_22.03.16.txt  \n",
            "  inflating: emg/02/1_raw_data_14-19_22.03.16.txt  \n",
            "   creating: emg/27/\n",
            "  inflating: emg/27/1_raw_data_12-19_06.04.16.txt  \n",
            "  inflating: emg/27/2_raw_data_12-20_06.04.16.txt  \n",
            "   creating: emg/17/\n",
            "  inflating: emg/17/2_raw_data_11-20_23.03.16.txt  \n",
            "  inflating: emg/17/1_raw_data_11-19_23.03.16.txt  \n",
            "  inflating: emg/emg_x.npy           \n",
            "   creating: emg/03/\n",
            "  inflating: emg/03/1_raw_data_09-32_11.04.16.txt  \n",
            "  inflating: emg/03/2_raw_data_09-34_11.04.16.txt  \n",
            "   creating: emg/31/\n",
            "  inflating: emg/31/2_raw_data_11-16_11.04.16.txt  \n",
            "  inflating: emg/31/1_raw_data_11-15_11.04.16.txt  \n",
            "   creating: emg/11/\n",
            "  inflating: emg/11/1_raw_data_13-11_18.03.16.txt  \n",
            "  inflating: emg/11/2_raw_data_13-13_18.03.16.txt  \n",
            "   creating: emg/21/\n",
            "  inflating: emg/21/2_raw_data_20-30_24.04.16.txt  \n",
            "  inflating: emg/21/1_raw_data_20-28_24.04.16.txt  \n",
            "   creating: emg/10/\n",
            "  inflating: emg/10/1_raw_data_11-08_21.03.16.txt  \n",
            "  inflating: emg/10/2_raw_data_11-10_21.03.16.txt  \n",
            "   creating: emg/05/\n",
            "  inflating: emg/05/2_raw_data_10-29_30.03.16.txt  \n",
            "  inflating: emg/05/1_raw_data_10-28_30.03.16.txt  \n",
            "   creating: emg/19/\n",
            "  inflating: emg/19/1_raw_data_12-10_26.04.16.txt  \n",
            "  inflating: emg/19/2_raw_data_12-11_26.04.16.txt  \n",
            "   creating: emg/08/\n",
            "  inflating: emg/08/1_raw_data_12-14_23.03.16.txt  \n",
            "  inflating: emg/08/2_raw_data_12-16_23.03.16.txt  \n",
            "   creating: emg/12/\n",
            "  inflating: emg/12/2_raw_data_11-36_28.03.16.txt  \n",
            "  inflating: emg/12/1_raw_data_11-35_28.03.16.txt  \n",
            "   creating: emg/16/\n",
            "  inflating: emg/16/2_raw_data_12-14_25.04.16.txt  \n",
            "  inflating: emg/16/1_raw_data_12-12_25.04.16.txt  \n",
            "   creating: emg/24/\n",
            "  inflating: emg/24/1_raw_data_10-16_12.04.16.txt  \n",
            "  inflating: emg/24/2_raw_data_10-17_12.04.16.txt  \n",
            "   creating: emg/33/\n",
            "  inflating: emg/33/2_raw_data_09-50_12.04.16.txt  \n",
            "  inflating: emg/33/1_raw_data_09-49_12.04.16.txt  \n",
            "   creating: emg/07/\n",
            "  inflating: emg/07/2_raw_data_18-50_22.03.16.txt  \n",
            "  inflating: emg/07/1_raw_data_18-48_22.03.16.txt  \n",
            "   creating: emg/32/\n",
            "  inflating: emg/32/2_raw_data_12-06_27.04.16.txt  \n",
            "  inflating: emg/32/1_raw_data_12-04_27.04.16.txt  \n",
            "   creating: emg/18/\n",
            "  inflating: emg/18/1_raw_data_12-35_21.03.16.txt  \n",
            "  inflating: emg/18/2_raw_data_12-37_21.03.16.txt  \n",
            "   creating: emg/23/\n",
            "  inflating: emg/23/1_raw_data_13-18_05.04.16.txt  \n",
            "  inflating: emg/23/2_raw_data_13-19_05.04.16.txt  \n",
            "   creating: emg/29/\n",
            "  inflating: emg/29/2_raw_data_10-18_15.04.16.txt  \n",
            "  inflating: emg/29/1_raw_data_10-17_15.04.16.txt  \n",
            "  inflating: emg/emg_y.npy           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./data/ \\\n",
        "  --task cross_people \\\n",
        "  --test_envs 0 \\\n",
        "  --dataset emg \\\n",
        "  --algorithm diversify \\\n",
        "  --latent_domain_num 10 \\\n",
        "  --alpha1 1.0 \\\n",
        "  --alpha 1.0 \\\n",
        "  --lam 0.0 \\\n",
        "  --local_epoch 3 \\\n",
        "  --max_epoch 1 \\\n",
        "  --lr 0.01 \\\n",
        "  --output ./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01 \\\n",
        "  --enable_shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XtUQLB8ECfq",
        "outputId": "2bca4b06-f404-453f-f312-bc91d7fbbb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:1.0\n",
            "alpha1:1.0\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:emg\n",
            "data_dir:./data/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:10\n",
            "local_epoch:3\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:1\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[0]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01\n",
            "weight_decay:0.0005\n",
            "enable_shap:True\n",
            "resume:None\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'emg': [0]}\n",
            "select_channel:{'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list:{'emg': 1000}\n",
            "act_people:{'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "num_classes:6\n",
            "input_shape:(8, 1, 200)\n",
            "grid_size:10\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5679172873    \n",
            "1                0.6139680147    \n",
            "2                0.4868124723    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.9672871232     0.9480240941     0.0192630403    \n",
            "1                0.8150575161     0.8124972582     0.0025602651    \n",
            "2                0.8276174664     0.8267637491     0.0008537344    \n",
            "Counter({np.int64(7): 4144})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.2733078301     0.0018004910     0.2751083076     0.8682432432     0.8425120773     0.7312206573     1.8655016422    \n",
            "1                0.4176126122     0.0011133023     0.4187259078     0.8614864865     0.8347826087     0.7048122066     4.2536351681    \n",
            "2                0.3460644186     0.0004498536     0.3465142846     0.8723455598     0.8405797101     0.6825117371     6.0441157818    \n",
            "\n",
            "🎯 Final Target Accuracy: 0.7312\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "[WARN] Adjusting flat_inputs from 1600 to 9600\n",
            "/content/extddivesify/diversify/shap_utils.py:47: FutureWarning:\n",
            "\n",
            "The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "\n",
            "[SHAP] Accuracy Drop: 0.0000\n",
            "[SHAP] Flip Rate: 0.0000\n",
            "[SHAP] Confidence Δ: -0.0035\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 1.6291\n",
            "[SHAP] Coherence: 0.0156\n",
            "[SHAP] Jaccard: 0.0000\n",
            "[SHAP] Kendall’s Tau: 0.0544\n",
            "[SHAP] Cosine Sim: -0.1592\n",
            "Signal shape before reshape: (8, 1, 200)\n",
            "SHAP value shape before reshape: (8, 1, 200, 6)\n",
            "{'text/html': '<html>\\n<head><meta charset=\"utf-8\" /></head>\\n<body>\\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: \\'local\\'};</script>\\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"54826355-656d-49bd-a4fe-9defb4782b10\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"54826355-656d-49bd-a4fe-9defb4782b10\")) {                    Plotly.newPlot(                        \"54826355-656d-49bd-a4fe-9defb4782b10\",                        [{\"hovertemplate\":\"Time=%{x}\\\\u003cbr\\\\u003eChannel=%{y}\\\\u003cbr\\\\u003eSignal=%{z}\\\\u003cbr\\\\u003eSHAP Importance=%{marker.color}\\\\u003cextra\\\\u003e\\\\u003c\\\\u002fextra\\\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0.00011250652945212399,0.00005717559057908753,0.00006948629743419588,0.000101543419683973,0.0001836117883916207,0.00018383504842252782,0.00003194093490795543,0.00019250204786658287,0.00008964417672056395,0.00033425460181509453,0.00025997993846734363,0.0001815562427509576,0.00010179818976515283,0.00010995434422511607,-0.00009924115147441626,0.00020050764821159342,0.00008559339039493352,0.00006455976593618591,0.00006571443130572636,0.00011367182802738778,-0.00005845079431310296,0.00010107726120622829,0.00010337291557031374,0.00016820081752181673,0.000029880551058643807,0.00014275503175061507,0.000022812998698403437,0.00007978267967700958,0.000040899307047463175,0.000028624732901031773,-0.00020548452933629355,-0.0008058171176041166,0.00013607480408002934,-0.0002313577375995616,0.00009760495352869232,0.0002503811459367474,-0.0001593598281033337,-0.00025795839610509574,0.0007635801060435673,0.00003603922474818925,0.0008703876325550178,0.00002963910810649395,0.0008686450116025904,0.00047464084733898443,0.0010415075958007947,0.0003958815068472177,0.0009948194880659382,0.00045755062213477987,0.0008182352952038249,0.00041742258084317047,0.0011099497011552255,0.00042499594079951447,0.0010156421340070665,0.00032495132957895595,0.0011722338967956603,0.0003899531826997797,0.0010452110824796061,0.0004403618707632025,0.0010892254843687017,0.000549707212485373,0.0012469738479315613,0.0006663831688153247,0.0013846291185473092,0.0007952176771747569,0.0015911556547507644,0.000919647243184348,0.0011869582182650145,0.0013211247278377414,0.0007201150098505119,0.0008660194289404899,0.0006043252263528606,0.001344104860133181,0.0010964982987691958,-0.00018853709722558656,-0.00023550054659911743,0.000536773819476366,0.00021378261347611746,0.0004009594364712636,0.00016975486748075733,0.0001953756897516238,-0.0029405789294590554,-0.002053578927492102,-0.001096610038075596,-0.0023845874250885877,-0.0008902815170586109,-0.0004928744019707665,-0.0007500526577738734,0.00042199289115766686,-0.00031031663093017414,0.0002561546377061556,-0.0002889217381986479,0.00006649755717565616,-0.00011830416042357683,0.00023414163539807,-0.000263991387328133,-0.0004436147476856907,-0.000348598036604623,-0.0002305538297756963,0.00005621876819835355,-0.0003160720007144846,0.00011444531264714897,0.00009153984137810767,-0.00024639730448446545,-0.00013950564122448364,0.00014880899107083678,-0.00027680637625356513,0.00034501847524855595,-0.00026881500768164795,0.00003089064072507123,-0.0001476174220442772,-0.00007166632955583434,-0.00041934366648395854,-0.00023374679343154034,-0.0006124856881797314,-0.00040514502325095236,-0.00040934831243551645,-0.0005594802690514674,-0.0007012362087455889,-0.002120329370275916,-0.0021513244040155164,0.00009387366784115632,0.00009605062465804319,-0.0000944877028814517,0.00012484864661625275,0.0004920919115344683,0.00030980495891223353,0.00032436711868892115,0.00040234538028016686,0.00030748014493534964,0.00024840991439608234,0.0002322865960498651,0.0005007915121192733,-0.000067073250344644,0.0004698360086573909,0.000036714152277757726,-0.00012935923102001348,0.00023386268488441905,1.3745739124715328e-6,0.0006290917129566272,-0.0005936040543019772,-0.00028682001478349167,-0.0006927728245500475,-0.0005515286587372733,-0.0006758022160890201,-0.0006708231570276743,-0.0005130280842422508,-0.00047430788981728256,-0.0005707543653746446,-0.0006163979996927083,-0.0003951950347982347,-0.0004673504445236176,-0.0006487242426373996,-0.00040819793260501075,-0.00045548698714507435,-0.00028627015368935343,0.0006857249051487694,0.0003815365162154194,0.0006197473655144373,0.0003017754170286935,0.00039700394942580414,-0.000923935139629369,-0.0014619505527662113,-0.0009476981260831963,-0.003213966886202494,-0.0012415980454534292,-0.0016114581036769475,-0.0010945065684306126,-0.0015588302242880066,-0.0016782900590139131,-0.001524209418372872,-0.0003563513746485114,-0.0004900960387506833,-0.00026577234772654873,-0.0003052017903731515,-0.0001376722939312458,-0.0018091588281095028,-0.0027857900131493807,0.0022324791255717478,0.0009354364883620292,0.0016381369981293876,0.0005275033181533217,-0.00038534113749240834,0.0011778253635081153,-0.0009201543095211188,0.0004318915113496284,-0.00029070276650600135,0.00022109694934139648,0.00006375764011560629,0.00021489046048372984,0.00008426246252687027,0.000054080854170024395,-0.00017281439310560623,-0.0002553541756545504,-0.00013815638764450947,-0.0004597816538686554,-0.00014627078780904412,-0.00031283261099209386,-0.00010049464132559176,-0.000030946830520406365,0.000024414619777720265,-0.000013149035415456941,0.000035010808763521105,0.000049338552344124764,0.00003883741495277112,0.00013210191294395676,0.00012372267519822344,0.0002177534915972501,0.00021408474034008881,0.00022586235960867876,0.0002488317222741898,0.00032203642088764656,0.0002522015929571353,0.00028104280014910427,0.0001861120011502256,0.00006295052480709273,0.0002425362035864964,0.0001621916627906709,0.00012528468990543237,0.00017756303350324742,0.00016828101070132107,0.00019222493089425066,0.00008343847609163883,-0.000015094066232753297,-0.00002251759481926759,0.00007083068097320695,-0.0000839808490127325,-0.0000433477713765266,-0.000014112054486759007,0.00029379059560596943,0.00011789671649845938,-0.00009426794349565171,-0.00034115676923344534,-0.0001953524382164081,-0.0002165290885992969,-0.00039241742342710495,-0.00005860170737529794,-0.00006300506114106004,0.0004570692447790255,0.00010519212810322642,0.0001188902339587609,0.0000963617058005184,0.0001519381839898415,0.00012193463044241071,0.00009057909483090043,0.00026078487280756235,0.00021152335102669895,0.00024692428996786475,0.00020259161829017103,0.0003208526565382878,0.00024112410756060854,0.00040600924209381145,0.0002845520502887666,0.0005084589938633144,0.0003670709944951038,0.0004386027770427366,0.0002849329181723685,0.0007339740986935794,0.0006253407336771488,0.0008250212413258851,0.0006178522986980776,0.0004944901447743177,0.00041713487977782887,0.0003581313455166916,0.0002969771157950163,0.00037330193057035405,0.00025646889116615057,0.0003134866322701176,0.0005470608581769435,0.00031327671604231,0.00046859085947896045,0.0002541507516677181,0.0007767258017944793,0.000295703003454643,0.0002982763883968194,0.0006529543607030064,0.0012666719097372454,0.0006466460811983173,0.0011175269610248506,0.0012385638353104393,0.0008872208685109703,-0.00041469784870666143,-0.0003699054941534996,-0.0002686916280557246,-0.0002707310098533829,-0.00038485285282755893,-0.00009836463141255081,-0.00023262648513385406,-0.00041112559847533703,-0.000358819243653367,-0.00018002539097021023,-0.00018807147474338612,-0.00029283670786147314,-0.0001873007083001236,-0.00017152662621811032,0.00009198408345885885,0.00008448981194912146,0.0002027503214776516,-0.000018901366274803877,0.00020275102967085937,0.00004320990410633385,0.00031233657500706613,0.00001713297873114546,0.00043981730414088815,0.0000602683382264028,0.0003789069281386522,0.00018713253181582937,0.00026521566906012595,0.00021167493529598383,0.0002392528452522432,0.00020439826766960323,0.00034470273870586726,0.000060107733588665724,0.0004047929251100868,0.00030730040937972564,0.00020598675958657017,0.000555663340492174,0.0004301034981229653,0.0005880737080588005,-0.0000434865857338688,7.82747035070012e-6,-0.0006871394871268421,-0.0010313391297434766,-0.0010780174149355541,-0.0009987536274517577,-0.0017452031218757231,-0.0011641480959951878,-0.0008849891358598446,-0.0011859572453734775,-0.0011540156859458268,-0.0006881918816361576,6.576689581076304e-6,0.000034671955897162356,0.000060497899539768696,-0.00016156497198001793,-0.00019829412728237608,0.000018019364991535742,0.00007269308359051745,0.00016427655161047974,0.00015967775834724307,0.0003547667292878032,0.0005398901606289049,0.00042459900335719186,0.0007123221294023097,0.0008302924688905478,0.001099582276462267,0.0010255767847411335,0.0013491030937681596,0.0013540355721488595,0.0017179001006297767,0.0014724957291036844,0.0021180994420622787,0.001602338394150138,0.002007247181609273,0.0013080927698562543,0.001466121825311954,0.00005444727503345348,0.00023746759203883508,0.00020260616050412258,0.00005528694479532229,0.00020151087664999068,-0.0009771167242433876,-0.0029773700516670942,-0.003073914054160317,-0.003307985253438043,-0.004522018794280787,-0.003148935977757598,-0.0040372231548341615,-0.003240074249333702,-0.005041736100489895,-0.0036154208428342827,-0.003481275091568629,-0.0030611356099446616,-0.002543831088890632,-0.0008697237547797462,-0.001417706604115665,0.0015532096391931798,0.00019195243658032268,0.00013641165666437396,-0.000010126716612527767,-0.00011746901630734403,-0.00023464655775266388,-0.00019800857504984984,0.0000194193950543801,0.000033884154011805855,-0.00024331750076574585,0.000042018778913188726,-0.00017882806908649704,0.000018063864748304088,-0.0001584211034545054,0.00005573331145569682,-0.00015132379485294223,0.00007166975895718981,-0.00007246153351540367,0.0000509057548091126,-0.00003539424202851175,3.284432750660926e-6,-0.000025502885667568382,-0.000013566304915002547,0.00003947547975258203,-6.581644205046662e-6,-0.00006731085522915237,3.1981259477712837e-7,3.887701192676711e-6,-5.563497021891332e-6,0.000030418484925576195,-0.00004649967255924518,0.00006229571469399768,-0.00001899676378040264,0.00005734553027044361,0.00001683339208587616,0.00012648655441201603,0.00007309523410488812,0.0001164091081591323,0.00010258464802366991,0.00019341686856932938,0.00016748094276408665,0.00020683886153468242,0.00008084004154322126,0.00019619916080652425,0.00018659521204729876,0.00009923969355440931,0.0001489854245543635,0.00015071663559259227,0.00026065283721739735,0.00022503137976552048,0.000247270271453696,0.000133849685274375,0.00039769306022208184,0.00040955243942638236,-0.0009885255810028564,-0.0004118975703022443,-0.0004542004705096285,-0.0006002516020089388,-0.000306678528431803,-0.0012512529404678692,-0.0005955712404102087,0.0010746492771431804,-0.0007015554971682528,0.000450849892028297,0.00030871044388428953,0.0003989805530485076,0.00027303080666267004,0.00035021706510936684,0.0002023571632889798,0.00037489804284026224,0.0000694825187868749,0.00035649852361530066,0.0001462924701627344,0.00033786568383220583,0.00025840141764395713,0.00037780038352745277,0.0002212656157401701,0.0003226290476353218,0.00024179638906692466,0.00028595441350868594,0.00027161311300005764,0.00030868445052571286,0.00023993340437300503,0.00043697241926565766,0.0002342015407824268,0.0005294948350638151,0.00011147108913670915,0.0006152327987365425,0.00037131868399834883,0.0005592751549556851,0.00034617755833702785,0.00046273441209147376,0.00035216556473945576,0.0005406037089414895,0.00040511628806901473,0.0003373640065547079,0.00046559675441433984,0.0003585266373799338,0.0045177029581585275,-0.00023713652626611292,-0.006177004339406267,-0.0071684640521804495,-0.00635430224550267,-0.011483592602113882,-0.010117810219526291,0.003459298051893711,0.004314284926901261,0.003150669469808539,0.004417296964675188,0.00227556210787346,0.0037566859585543475,0.00199476241444548,0.0019100192002952099,0.0027207452027748027,0.0022356336315472922,0.0018588750002284844,0.002705630303050081,0.0029907898666958013,0.0019061746424995363,0.0019426567014306784,0.0019116944167762995,-0.0003161442291457206,-0.0002633309147010247,-0.0004443992705394824,-0.00001232960494235158,-0.00026984848470116657,-0.00020847758666301766,0.0001980956585612148,-0.0002483174515267213,0.000312398779594029,-0.000265011719117562,0.00046960365822693956,-0.000134988008843114,0.0007954768710381662,-0.00010043445702952643,0.0009746573535570254,0.0002631729197067519,0.0011090553792503972,0.0004042676543273653,0.0011085480606804292,0.0010439491306897253,0.000957514731756722,0.001552078594007374,-0.0005717425180288652,-0.00047901208987847593,-0.004126825137063861,-0.002740115586978694,-0.004298045261142154,-0.00375053787198946,-0.0055079099256545305,-0.003565544068502883,-0.0034422385318369684,-0.002901995942617456,-0.0035346060370405516,-0.0013880592111187677,0.00030966611423840124,0.0001681787301398193,0.0002946433851320762,0.00011222915418329649,0.0001991233196652805,0.0002577677951194346,0.00015214627577127734,0.00023940192113514058,3.7103406308839717e-6,-0.0016664080321788788,-0.0015664056021099289,-0.0014050883085777361,-0.001918499474413693,-0.0019740711237924793,-0.002508825908686655,-0.0016707785932036738,-0.002686589849569524,-0.0014609347951288025,-0.003596039100860556,-0.0023499603072802224,-0.004098804046710332,-0.002965734965982847,-0.004295024050710102,-0.002897913296086093,-0.0040955649844060344,0.0018694755660059552,0.0023497317257958152,0.0018583552324950385,0.0013631242715443175,0.001600187582274278,0.0032926747420181832,-0.0009100589280327162,0.004783414304256439,-0.003655917476862669,0.001614789788921674,-0.0011397640531261761,0.0008768920476237932,-0.0005948822945356369,0.002084721423064669,-0.0030765623475114503,0.0013761001949508984,-0.002862463860462109,0.0020075669744983315,-0.0015791428741067648,-0.0027274596504867077,0.00008323338503638904,0.0008954067719362987,0.00013078296615276486,0.00012101020547561347,0.00015174061506210515,0.0004008590270435282,0.00016003769633243792,0.0003439903472705434,0.00011590403543474774,0.0002112109650624916,0.0000686316658781531,0.00010270465766855826,0.0001418150519990983,0.0001150644044779862,0.00012283700076901974,0.00009889565383976635,0.00011073880038262966,0.0000636425308281711,0.00015478541657406217,0.00010249165886004145,0.0000527217932055161,0.000056413368383800844,9.366631275042892e-6,0.000013362227036850527,-4.7660341806476936e-6,0.0005701610401350384,-0.000020456259032168116,-0.000150370943932406,-0.00016668954534300914,-0.00010149049436828743,-0.00018545851586774612,-0.0003181148664831805,-0.0003508656906584899,-0.00015296901013546935,-0.00042240304173901677,-0.0005394817950824896,-0.0003833627197309397,-0.0008464840890762085,-0.0009026741184546457,-0.0010589153031711855,-0.0013686922563162323,-0.0013792106474284083,-0.000865207596992453,-0.001380464023289581,-0.0010142997659083146,-0.001089834685747822,-0.0014901397031887125,-0.0022158193945263824,-0.0010876855230890214,-0.0014896744881601383,-0.0018926445627585053,-0.0018198284281728168,-0.0017792930787739654,-0.002161026505442957,-0.0016209910778949659,-0.0014531506652322908,-0.0009992555666637297,-0.0008875717564175526,-0.0011731398408301175,-0.0006461313460022211,-0.00108156050555408,-0.002802133560180664,-0.0036565004071841636,0.0017395490043175716,0.003133055210734407,0.0027975206030532718,0.0026899107227412364,0.003008160895357529,0.002091370969234655,0.0030327305818597474,0.0032556301448494196,0.002636674248302976,0.0021636449188614884,0.0024091299468030534,0.0022817433443075665,0.0022886504496758184,0.0018450633021226774,0.0022893460312237344,0.00249143709273388,0.0022355684098632387,0.0024658950278535485,0.0027630660333670676,0.002628387196940215,0.0026847264962270856,0.0028349371374739953,0.0027388486438818895,0.002309289644472301,0.002668281755177304,0.0035201782981554666,0.0022018400098507604,0.004185897686208288,0.00244492517473797,0.0035500777109215655,0.0014434880577027798,0.0020264424844450937,0.001356218340030561,0.00165063976116168,-0.00116157941132163,-0.0007093917035187284,-0.003035532853876551,-0.0025921994044135013,-0.0008479456494872769,0.000018710619769990444,0.0011962454688424866,-0.00008962916520734628,-0.0003301581309642643,0.0005676561462072035,-0.0016231060532542567,0.0009884356792705755,-0.0001824895152822137,0.00017056589907345673,-0.0012986106254781287,-0.0013742675218963996,-0.0017065649153664708,-0.0007160248157257835,-0.000577763241987365,-0.0005073238571640104,-0.0008508261234965175,-0.0009128890039088825,-0.0007302155766713744,-0.002790403629963597,-0.002257913506279389,-0.0015353483128516625,-0.001867422057936589,-0.0027848196138317385,-0.0025798759112755456,-0.00238832930335775,-0.0018857682589441538,-0.002205057954900743,-0.0015350093987459938,-0.0019308814371470362,-0.0020419330491373935,-0.002374504071970781,-0.0018235503230243921,-0.002205691998824477,-0.0014101715448002021,-0.0018539094210912783,-0.0011132420040667057,-0.0017699279511968296,-0.0012213735608384013,-0.0020747392748792968,-0.0012905227486044168,-0.0017925890473028023,-0.0015869108416760962,-0.0020923197347049913,0.001458557104342617,0.001245711711817421,0.0022431202232837677,0.0026271406289500496,0.002706974085109929,0.0031583347202589116,0.0017317357220842193,0.0031634471961297095,0.001743678158770005,0.0011923550628125668,-0.003311542250836889,-0.002150933646286527,-0.003931058570742607,-0.001748711025963227,-0.0026057281841834388,-0.001574507759263118,-0.0031244675628840923,-0.001999480649828911,-0.0023915122728794813,0.0011171424606194098,0.0021017037021617093,0.0015130575435856979,0.002396925895785292,0.0022874386437858143,0.0027693164302036166,0.002561773251121243,0.00258592547227939,0.003351531457155943,0.0018545015094180901,0.00239583229025205,0.0019140238291583955,0.002881328652923306,0.0026171699476738772,0.002610108039031426,0.003241059156910827,-0.00005498831160366535,-0.0008285044847677151,-0.00038695521652698517,-0.0016723519656807184,-0.0009635220825051268,-0.002593563661018076,-0.0023782998711491623,-0.002244435716420412,-0.0014434379215041797,-0.0008617974817752838,-0.00025686598382890224,-0.0011436274896065395,0.0007009928425153097,-0.0012825195056696732,0.00016618194058537483,-0.00004805997014045715,-0.000026273347126940887,-0.0027330716451009116,-0.002566360946123799,-0.0010856078879442066,-0.0008404293718437353,-0.0007034827334185442,-0.0032006320543587208,-0.0031542895982662835,-0.0033386898382256427,-0.0033130689213673272,-0.002935072872787714,-0.002860991982743144,-0.00265407410915941,-0.002214059563508878,-0.0018868192952747147,-0.0020446620571116605,-0.001612391182182667,-0.0011976698248569544,-0.0009368959484466662,-0.0008997413679026067,-0.00045674011926166713,-0.00038352288538590074,-0.00042702780532029766,-0.00030523182310086366,-0.00027057739104445017,-0.00019157084655792764,-0.00014128034429935118,-0.00004105585200401644,-0.00003045742535808434,0.00009668013080954552,0.00023325916845351458,0.0005808740279462654,0.0005622095195576549,0.0006236294187450161,0.0006940020296800261,0.000895426026545465,0.0009373439534101635,0.0005224363315695276,0.001334255871673425,0.0009007292780249069,0.001499975430003057,0.0007442012526250134,0.0011661570363988478,0.0004606057967369755,0.0013418524952915807,-0.00007517387469609578,0.000549610046922074,-0.0006870812891672055,0.00008188498516877492,-0.0009840795149405797,0.00027268305226850015,-0.0003792487962831122,0.0003996796003775671,-0.0007715187384746969,0.0011964880395680666,0.0008907085381603489,0.0014195504675929744,-0.004353729775175452,-0.004720415260332326,0.001879329977479453,0.002388470418130358,0.0022500239623089633,0.0019235179061070085,0.0029279999434947968,0.0026178271121655903,0.0038786724908277392,-0.0006526168823863069,0.0019298788101878017,0.0018534695457977552,0.002292862879888465,0.0022333812279005847,0.0017414125807893772,0.0019867498534343517,0.0014619043407340844,0.0014490177466844518,0.0009416769801949462,0.0012126567695910733,0.0011970700422049656,0.0012654551149656375,0.001326040830463171,0.0017559543096770842,0.0016433736151763394,0.0014814819830159347,0.0013727425830438733,0.0014988641681460042,0.0013004166539758444,0.0015839558327570558,0.0015891656124343474,0.0016576763591729105,0.0017826615270071973,0.002006768782545502,0.0014074890714255162,0.001874412128624196,0.0012567244072367127,0.0009843301765310268,0.0011058731615776196,0.001246742516135176,0.000941478181630373,0.0007493608475973209,0.0015216481406241655,0.0010947098295825224,0.0007162538434689244,0.0013270041672512889,0.0010800449429855992,-0.00002298209195335706,0.0002225640540321668,0.0018868162102686863,0.0023028155167897544,0.006061903977145751,-0.0016114700119942427,-0.006211036040137212,-0.0037607636768370867,-0.0038310967987248055,-0.004410012547547619,-0.0019535797958572707,-0.004918154290256401,-0.00020623086796452603,-0.003890893111626307,-0.0015850447428723176,-0.0046204572233061,-0.006089499530692895,-0.008016979942719141,-0.004701320702830951,-0.0067052500089630485,-0.0008926510733241836,0.002665421154233627,0.0016644470936929185,0.002795472275465727,0.0025245448923669755,0.0019349281598503392,0.0021760471475621066,0.0023179068618143597,0.001592878232865284,0.002090068223575751,0.001078530369947354,0.0014522991065556805,0.000816215566980342,0.001199473044835031,0.0010660852616031964,0.0019003782654181123,0.0005873179179616272,0.0011493742349557579,-0.0001730020303511992,0.0009126838607092699,-0.0005853119218954816,0.00026821230615799624,-0.00008077347107852499,0.0012915632881534596,0.001305175091450413,0.002471953611044834,0.0016099219986548026,0.001933946836894999,0.002593063904593388,0.0022526046571632228,0.003103509545326233,0.0017948502597088616,0.0026393753942102194,0.0012529393813262384,0.003984921262599528,0.002482533066843947,0.002452948441108068,0.0015657180338166654,0.0004871656031658252,0.0008870032809985181,0.0007749551829571525,-0.00038509808170298737,0.0004152023490557137,0.00029268604703247547,0.00014007911765171835,0.00044175414465523016,0.00042020999050388735,-0.00021645741071552038,0.0000680146428445975,-0.00022464179589102665,-0.0005516981861243645,-0.00010387688719977935,-0.0009545080053309599,-0.00017881874615947405,-0.0007891943290208777,0.00035850876399005455,-0.0006611526090030869,-0.000047591825326283775,-0.0016913894408692916,-0.0007556737012540301,-0.0022017177543602884,-0.002153392822947353,-0.0030881809846808514,-0.0014726338364804785,-0.0017752421554178,0.0019149402942275628,0.0003048731232411228,0.0007089168357197195,-0.00041536598776777584,0.0004209643617893259,-0.00012155416576812665,-0.0003980399342253804,0.0001515814607652525,-0.0005556670269773653,-0.00034373354477186996,-0.0010719932130693148,-0.0003137840928199391,-0.0009596057255597165,0.0005050359953505298,0.0000391064192323635,-0.0017018221163501341,0.001249127594443659,0.0009824827041787405,0.0014877686432252328,0.0006407899976087114,0.0005302855667347709,0.000012051556647444764,0.0006231634955232342,0.0008610041501621405,-0.0006006119074299932,0.0005487664214645823,-0.00016589335185320428,0.0007637484134950986,-0.0004174024640330269,0.0003804879476471494,-0.0003969877264656437,0.00034236499535230297,-0.0003373742923334551,-0.000022323326750968892,-0.0002712215355131775,0.00003950035898014903,-0.00008224359529170518,0.00004497895133681595,0.000057306746991040804,0.0000172907948581269,-0.0003724566947009104,-0.00025998830096796155,-0.0007436232408508658,-0.00030917445352921885,0.00018788010735685626,-0.0006948649728049835,0.000895200219626228,-0.0015842127613723278,-0.0010385915326575439,-0.0020199758000671864,0.0027946720365434885,-0.0020435418312748275,0.0027459155147274337,-0.0030694895734389624,0.0026299884193576872,-0.0016711664696534474,0.00352062052115798,-0.003124460888405641,0.0033838689172019563,-0.0023129197458426156,0.003305350740750631,-0.0030426656206448874,0.002399232548971971,-0.003737594156215588,0.0006892241848011812,-0.0012452630326151848,0.004604562923001747,-0.0005377683167656263,-0.0007492902999122938,-0.0025161494268104434,0.00007646350422874093,0.0001701739699152919,-0.00010066596344889452,-0.000038563296887635566,-0.00017270431756818047,-0.00027559196314541623,-0.0005666062643285841,-0.0004555391302953164,-0.002803191445612659,0.0012556293901676934,-0.0024429511977359653,0.0006274483942737182,-0.0029092212401640913,-0.0021502020535990596,-0.0003535791765898466,-0.0008168735463793079,-0.0008868234387288491,-0.0007649204247475913,-0.0012172516823435824,-0.00028728162093708914,-0.0012339787402500708,-0.0003747712156230894,-0.001492794059837858,0.0004193582572042942,-0.0016906137655799587,-0.0007602896657772362,-0.002090455013482521,0.0003603796358220279,-0.0011539152668168147,-0.0006844771172230443,-0.0009877653210423887,0.00034738735606273014,-0.0014272515351573627,-0.00047438155646280694,0.0004559655984242757,-0.0010999572308113177,-0.0003387707207972805,-0.0012791436941673358,0.0002806521442835219,-0.001934803285015126,-0.0002836966887116432,-0.002362412905010084,0.00009407809314628442,-0.0009612519206712022,-0.00023885249781111875,0.00007929171260911971,-0.000019258179236203432,-0.00030456519258829456,0.00010040705092251301,0.00008718007787441213,-0.001036049798130989,-0.0012702842553456624,0.000315551728514644,0.0008289137234290441,-0.00396306657542785,0.00002932812397678693,0.00035592277223865193,-0.0006891593026618162,-0.0020831950629750886,-0.0032561859115958214,-0.0023850700818002224,-0.007217052703102429,-0.005213438222805659,-0.00390590230623881,-0.0020859052116672197,-0.0004609898169292137,0.0017243384888085227,0.00041406663270511973,-0.0004162449234475692,-0.0002286368932497377,-0.0017992652137763798,-0.001227975915147302,-0.00190085734478392,-0.001589437781755502,-0.0018901477839487295,-0.002009898928614954,-0.0025486632948741317,-0.0017498211624721687,-0.0025178135644334057,-0.001974716581268391,-0.002390851906966418,-0.002268018181590984,-0.003104399424046278,-0.001216076818915705,-0.0035549398162402213,-0.0014847525647686173,-0.003117730423885708,-0.0019671707220065096,-0.0017777975929978613,-0.0009714333282317966,0.0011243754997849464,0.0009437198750674725,0.0009683905421600988,0.001284184535810103,0.0010040191749188427,0.0012587767366009455,0.0010636328952386975,0.0006083778959388534,0.0011382760712876916,0.0009075725780955205,0.0008015393589933714,0.0015321490354835987,0.0016734041685898167,0.00026177652762271464,0.000683969430004557,-0.0002111048330940927,0.0005474939631919066,-0.000521289010066539,-0.00020057762352128825,-0.0021115715038225367,-0.0008868382622798284,-0.0018950574643289049,-0.002723285307486852,-0.00253933539109615,-0.0020321050978964195,-0.0023107556723213443,-0.0024235577632983527,-0.0005832854852390786,-0.0016842740587890148,-0.001116565913738062,-0.0016823912543865542,-0.0011546489452787985,-0.0021899579054055116,-0.0009209394920617342,-0.0018435645227630932,-0.0016790300918122132,-0.001970467700933417,-0.0012602170075600345,0.00020415543500954905,-0.0008455904511113962,-0.002456707225064747,0.00013955884302655855,-0.0013450861442834139,-0.0013612320957084496,-0.002479864672447244,-0.0018341208924539387,-0.00012811770041783652,0.0010693253328402836,0.0004123118706047535,-0.0014098086394369602,-0.00017082163443168005,-0.001103670181085666,-0.00032622778477768105,0.0017488128990711023,0.0017837551422417164,-0.002534925239160657,-0.0019349797318379085,0.00200814290898658,0.001981974831627061,0.0020203401800245047,-0.002102311079700788,-0.001012515975162387,-0.0008126335839430491,0.0002587083727121353,0.00041861552745103836,0.0002959429984912276,0.0009635596846540769,-0.0005655442170488337,0.0013623111881315708,-0.00022389755273858705,0.001256322837434709,-0.00017040186503436416,0.001147275654754291,-0.00043981099346031743,0.0005848373014790317,-0.00043719659637038905,0.00006286326000311722,0.00006752856036958595,-0.00013314819201089753,-7.198059999306376e-6,-0.000031687412956671324,-0.0002533908118493855,-0.0005213683471083641,-0.0005759626704578599,-0.00132175930775702,-0.001591034544010957,-0.0014132796786725521,-0.002138624588648478,-0.0018592216074466705,-0.001812952881058057,-0.0010699440414706867,-0.0017878964232901733,-0.0016716074508925278,-0.0025692678367098174,-0.001298693163941304,-0.002622713645299276,-0.0018866248428821564,-0.003595422642926375,-0.002876590316494306,-0.004133081374069055,-0.002873683969179789,-0.004018973248700301,-0.0015255988885958989,-0.0038531345004836717,-0.0026121443758408227,-0.0019464955354730289,-0.0008726525120437145,-0.0020768968388438225,-0.0004825838841497898,-0.00045582983875647187,-0.0007988579260806242,0.0016568813783427079,0.0010173467453569174,0.0016955759686728318,0.0015296407509595156,0.0008323029809010526,-0.0012535628241797288,-0.00001481326762586832,0.00030688483578463394,0.0000642980351888885,0.0008821048638007293,0.00033982501190621406,0.0006651263878059884,0.0006812659169857701,0.0011696471483446658,0.0010170051367216122,0.0013109510570454101,0.0005981959451067572,0.0011478115678376828,0.00044145865831524134,0.0013510135856146614,0.0006452091038227081,0.0011279172807311018,0.0007723484692784647,0.001474495860747993,0.0010347090816746156,0.00169915403239429,0.001203103670074294,0.0018923537184794743,0.0011843482837624226,0.002062228973954916,0.0016717947631453474,0.002507333488514026,0.0016927846396962802,0.0024495312633613744,0.0019045663066208363,0.00198136898688972,0.0008757505565881729,0.0025853439777468643,0.0011015385389328003,0.0008647875705113014,0.00038326525827869773,0.0004347652041663726,-0.0006828668604915341,-0.0006459552872305115,-0.0002503655074785153,-0.0004984974317873517,-0.00021239765919744968,-0.0006441822818790873,-0.0005750094229976336,-0.003225705586373806,0.002553684947391351,-0.00031914251546065014,0.0002109141399463018,-0.0015548306206862132,0.0013296386847893398,0.001721803719798724,0.0015629536161820095,0.0006188812355200449,-0.002228288600842158,-0.001764640212059021,-0.003598190223177274,-0.0006018256147702535,-0.003290519118309021,-0.0018843331684668858,-0.0013506141840480268,-0.0004865108251882096,-0.0007025865294660131,-0.0004886321160787096,-0.0005009347660234198,-0.0002613931525653849,-0.0004357467284232068,-0.0004357002738591594,-0.00011711711219201486,-0.00034495966974645853,-0.00020033088124667606,-0.0005275053166163465,-0.00043830304639413953,-0.0005615368912306925,-0.0005362091954642286,-0.0007514549312569822,-0.0007380918298925584,-0.0005471840292254152,-0.0006736787909176201,-0.0006374672908956806,-0.0004344782209955156,-0.0004892443733600279,-0.00031687761444724555,-0.00043250544695183635,-0.0009326903576341768,-0.0014426155636707942,0.0003625138197094202,-0.002192887788017591,0.0006277923627446095,0.0005056061975968381,0.0036828475228200355,0.00085231630752484,0.002697340678423643,0.004372936634657283,0.0033447876727829375,0.00007460990552014361,0.00021024824430545172,0.00023222773840340474,0.0008528606267645955,0.0008550993031046042,-3.3322333668669066e-6,0.0009552378566392387,-7.681684413303932e-6,0.0004787876387126744,0.0004476740723475814,0.00005271622406629225,0.00036403928728153306,0.00027283143329744536,0.00043736263614846393,0.00026174497421986115,0.0004182810613807912,0.00042883850013216335,-0.00026046067553882796,-0.00015117473473461965,-0.0008690805649772907,-0.000483217843187352,-0.00110879111646985,-0.0010704924206947908,-0.0006401519446323315,0.00001547709689475596,-0.000740585538248221,-0.0005139310766632358,-0.0005725170097624263,-0.0008486002528419098,-0.0010215308211627416,-0.0013149721780791879,-0.001350241790836056,-0.0012607048847712576,-0.001096354496742909,-0.0012372315007572372,-0.0009655377895493681,-0.0007238050845141212,-0.0010355355868038412,-0.0009087774863777062,-0.0007414399005938321,-0.0005463300137004504,-0.0005652454080215344,-0.0008656650607008487,-0.0005831416783621535,-0.0007272480106621515,-0.000014536003921724236,-0.00041993753984570503,-0.001680672789613406,-0.0014451264093319576,-0.0019113911936680477,-0.0026298370212316513,-0.0011931974440813065,-0.002114256223042806,0.00032404065132141113,-0.0006747264415025711,-0.0010405766467253368,-0.0007984849313894907,-0.0009505928804477056,-0.0008121402934193611,-0.0015091896057128906,-0.0005032550543546677,-0.000813840888440609,-0.00046982988715171814,-0.0006906928805013498,-0.00014074821956455708,-0.00032276912437131006,0.00002236943691968918,0.00012070656521245837,-0.00002862019209715072,-0.00001748442749279396,0.00002133624578467182,-0.000010163277086879438,-0.000028088470571674407,-0.000052543160563800484,-0.00003416787270301332,-0.00010927124215716806,-0.000035190739557341054,-0.00020699581364169717,-0.00010097387712448835,-0.0005521833178742478,-0.00010845367493554174,-0.0006664085728213346,-0.00007295070099644363,-0.0005674674854769061,5.697826660859088e-6,-0.0004374722023688567,7.915009822075566e-6,-0.0006668839681272706,-0.00007965298451987717,-0.000724476354662329,-0.00015189076899938905,-0.001016785866037632,-0.00037137917630995315,-0.00041519091367566335,-0.0001796512418271353,-0.0006300698344906172,-0.00025149456147725385,-0.0000943154445849359,-0.0004521920879293854,-0.0003453160685846039,0.00021069273740674058,9.660696377977729e-6,-0.0005997976065070058,-0.0001013786531984806,-0.00035240167441467446,-0.00026711070677265525,0.00021399416921970746,0.00024596504711856443,-0.0013701145071536303,-0.00021220153818527857,-0.0023502770927734673,-0.0006811695639044046,-0.0015714099087441962,-0.0005431399428440878,-0.001869606562346841,-0.0002839922284086545,-0.0014762359981735547,-0.00039809833591183025,-0.001567363040521741,-0.00009119695945022006,-0.0015109632610498618,-0.00011332853076358636,-0.001631523957864071,-0.0002116372537178298,-0.0018831999429191153,-0.000556027739852046,-0.0021075680075834193,-0.0007775001383076111,-0.002179405651986599,-0.0007301027702245241,-0.0021703520324081182,-0.0008035290132587155,-0.002389647144203385,-0.00040591029877153534,-0.0025858484441414475,-0.000777385059336666,-0.0012282427536168445,-0.000282280418711404,-0.001201676786877215,-0.0003981508464979318,-0.00015582344106708965,0.00034994430219133693,-0.00031981188415860135,0.001016151579581977,0.001966855333497127,0.0013882364243424188,0.0017428625311974126,0.0016228791985971232,0.0016454023813518386,0.0013282294773186247,0.0012086333493546892,0.0010770191681028034,0.0008110099976571897,0.000914266313581417,0.0006239735909427205,0.0005844441378333917,0.00036849019428094226,0.00031032489399270463,0.0004302075831219554,0.00029024701022232574,0.0003939129601349123,0.00045655856956727803,0.0006181041923506806,0.0005564801782990495,0.0007895122980698943,0.0006885064455370108,0.001083178639722367,0.0007244955922942609,0.0013173044232341151,0.0010343365332422156,0.0011650254406655829,0.0007538765712524764,0.0011916244984604418,0.0009703261360603695,0.0009173805398556093,0.0013056363095529377,0.0010401826778737207,0.0010614186467137188,0.0011770538500665377,0.0009169684878240029,0.0011131691668803494,0.0010766677636032302,0.001010247622616589,0.0007002255351835629,0.0007191299227997661,0.00030637373371670645,0.0008159149438142776,0.00014994576728592315,-0.002406705568622177,-0.003061485088740786,-0.0006947043972710768,-0.0009348758030682802,-0.0008609176923831304,-0.0007054968931091329,-0.0005215399044876298,-0.0011410731143162896,-0.00023170532464670637,-0.0009214092327359443,0.0003537120840822657,-0.0006601497104081014,-0.002169750126389166,-0.0015301998937502503,-0.002673651402195295,-0.0029547845479100943,-0.0016137991333380342,-0.004142201893652479,-0.003012328020607432,-0.004017633385956287,-0.004080030135810375,-0.0010572936638103176,-0.0021072446446244917,-0.0011158454581163824,-0.0014259866050754983,-0.0012542155454866588,-0.0019222799455747008,-0.001314105686712234,-0.0016561283967651737,-0.001469602924771607,-0.0017442341583470504,-0.001249625871423632,-0.0016968334869792063,-0.0011229669714036088,-0.0019304593442939222,-0.0007199738950779041,-0.001375268950747947,-0.000665709678287385,-0.0006748729598863671,-0.0009139286218366275,-0.0002516758783409993,-0.0009699282963993028,0.001098321401514113,0.0006517727742902935,0.0010054665035568178,0.0007815742089102665,0.0011504817909250657,0.0009312645221749941,0.0012246552699555953,0.0010602486921319116,0.0014701936549196641,0.001023708008384953,0.001158750771234433,0.0011397054186090827,0.0012408449935416381,0.0008892514548885325,0.0013210730782399576,-0.00036623759660869837,-0.002300964193030571,0.0012218522606417537,0.0006333274456361929,0.00023180114415784678,0.00041541018678496283,0.0009209087972218791,-0.0007305634208023548,0.00041427602991461754,-0.0007347966699550549,0.000017027681072552998,-0.0007993468316271901,0.00009452652496596177,-0.0004903151808927456,0.00010334781836718321,-0.00040519328710312646,0.00024376041255891323,0.00006980130031782512,0.00024017181810146818,0.000017282475406924885,0.00021503768706073365,0.00009578820148211283,0.00008461956379809028,-9.602094602693493e-6,-3.6829672656798116e-6],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C0\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C1\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C2\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C3\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C4\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C5\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C6\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\",\"C7\"],\"z\":[0.4392157,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5058824,0.5058824,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.5176471,0.5137255,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5254902,0.5019608,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.47058824,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.43137255,0.43137255,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.6,0.6,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.48235294,0.5294118,0.46666667,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.53333336,0.53333336,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.49411765,0.5529412,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5294118,0.4862745,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.49019608,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.49411765,0.49411765,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.5372549,0.5372549,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.5176471,0.4862745,0.53333336,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.54509807,0.54509807,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.41960785,0.5568628,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.6,0.4392157,0.2627451,0.2627451,0.2627451,0.2627451,0.2627451,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.5647059,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5254902,0.5254902,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.5647059,0.5647059,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.47843137,0.46666667,0.654902,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.48235294,0.48235294,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.4509804,0.47058824,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5529412,0.4392157,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.5019608,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.45490196,0.45490196,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.6745098,0.6745098,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5411765,0.5019608,0.40784314,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.6901961,0.6901961,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.43137255,0.57254905,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.39607844,0.5803922,0.61960787,0.61960787,0.61960787,0.61960787,0.61960787,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3137255,0.3529412,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.49803922,0.49803922,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.6627451,0.6627451,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.6392157,0.47843137,0.26666668,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.12156863,0.69411767,0.69411767,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5372549,0.5921569,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.31764707,0.45882353,0.52156866,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.1882353,0.42745098,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5568628,0.5568628,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.32941177,0.72156864,0.72156864,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.29411766,0.74509805,0.49019608,0.5058824,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.5176471,0.5176471,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.36078432,0.5058824,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.5294118,0.48235294,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.3019608,0.47058824,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.48235294,0.48235294,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.47843137,0.47843137,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.5294118,0.5411765,0.4745098,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5294118,0.5294118,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.49411765,0.5058824,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.5411765,0.5294118,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.4,0.4,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.59607846,0.59607846,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.47058824,0.50980395],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"Channel\"}},\"zaxis\":{\"title\":{\"text\":\"Signal\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"SHAP Importance\"}},\"colorscale\":[[0.0,\"#000004\"],[0.1111111111111111,\"#1b0c41\"],[0.2222222222222222,\"#4a0c6b\"],[0.3333333333333333,\"#781c6d\"],[0.4444444444444444,\"#a52c60\"],[0.5555555555555556,\"#cf4446\"],[0.6666666666666666,\"#ed6925\"],[0.7777777777777778,\"#fb9b06\"],[0.8888888888888888,\"#f7d13d\"],[1.0,\"#fcffa4\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"4D EMG SHAP Visualization\"}},                        {\"responsive\": true}                    ).then(function(){\\n                            \\nvar gd = document.getElementById(\\'54826355-656d-49bd-a4fe-9defb4782b10\\');\\nvar x = new MutationObserver(function (mutations, observer) {{\\n        var display = window.getComputedStyle(gd).display;\\n        if (!display || display === \\'none\\') {{\\n            console.log([gd, \\'removed!\\']);\\n            Plotly.purge(gd);\\n            observer.disconnect();\\n        }}\\n}});\\n\\n// Listen for the removal of the full notebook cells\\nvar notebookContainer = gd.closest(\\'#notebook-container\\');\\nif (notebookContainer) {{\\n    x.observe(notebookContainer, {childList: true});\\n}}\\n\\n// Listen for the clearing of the current output cell\\nvar outputEl = gd.closest(\\'.output\\');\\nif (outputEl) {{\\n    x.observe(outputEl, {childList: true});\\n}}\\n\\n                        })                };                            </script>        </div>\\n</body>\\n</html>'}\n",
            "[INFO] Saved fallback HTML plot: 4D_EMG_SHAP_Visualization.html\n",
            "[INFO] Saved 4D SHAP surface plot to: shap_4d_surface.html\n",
            "[SHAP4D] Channel Variance: 0.0000\n",
            "[SHAP4D] Temporal Entropy: 2.1821\n",
            "[SHAP4D] Mutual Info: 0.2775\n",
            "[SHAP4D] PCA Alignment: 0.0000\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "Figure(640x480)\n",
            "[INFO] Saved SHAP heatmap to: shap_temporal_heatmap.png\n",
            "\n",
            "📊 Training baseline model for SHAP comparison...\n",
            "[INFO] Saved SHAP heatmap to: shap_heatmap_baseline.png\n",
            "\n",
            "🔍 Running ablation: shuffling SHAP-important segments...\n",
            "[Ablation] Accuracy post SHAP shuffle: 1.0000\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_histograms_impl.py:895: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n",
            "[SHAP Ablation] KL Divergence (Original vs Post-Ablation): nan\n",
            "/content/extddivesify/diversify/train.py:260: MatplotlibDeprecationWarning:\n",
            "\n",
            "The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "\n",
            "Figure(800x500)\n",
            "Figure(1000x500)\n",
            "[SHAP vs Confidence] Pearson Correlation: 0.5142 (p=0.1284)\n",
            "Figure(600x500)\n",
            "\n",
            "🛠 Real-world Context: EMG classification can support gesture-based interfaces in prosthetics or rehabilitation systems, and insights from SHAP improve trust in deployed models.\n",
            "Figure(1200x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./data/ \\\n",
        "  --task cross_people \\\n",
        "  --test_envs 1 \\\n",
        "  --dataset emg \\\n",
        "  --algorithm diversify \\\n",
        "  --latent_domain_num 2 \\\n",
        "  --alpha1 0.1 \\\n",
        "  --alpha 10.0 \\\n",
        "  --lam 0.0 \\\n",
        "  --local_epoch 10 \\\n",
        "  --max_epoch 15 \\\n",
        "  --lr 0.01 \\\n",
        "  --output ./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01 \\\n",
        "  --enable_shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybEVz8Ptj3ZD",
        "outputId": "a54fbfec-7093-4417-ac6d-14f0defa53a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:10.0\n",
            "alpha1:0.1\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:emg\n",
            "data_dir:./data/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:2\n",
            "local_epoch:10\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:15\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[1]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01\n",
            "weight_decay:0.0005\n",
            "enable_shap:True\n",
            "resume:None\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'emg': [0]}\n",
            "select_channel:{'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list:{'emg': 1000}\n",
            "act_people:{'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "num_classes:6\n",
            "input_shape:(8, 1, 200)\n",
            "grid_size:10\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8846430182    \n",
            "1                0.4123840332    \n",
            "2                0.6171653271    \n",
            "3                0.3764030635    \n",
            "4                0.9838128686    \n",
            "5                0.4860121906    \n",
            "6                0.6120703816    \n",
            "7                1.0629581213    \n",
            "8                0.3420240879    \n",
            "9                0.3654412329    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0499072075     1.0486174822     0.0012897112    \n",
            "1                0.8936307430     0.8932097554     0.0004210004    \n",
            "2                1.2768545151     1.2765276432     0.0003269166    \n",
            "3                1.0638037920     1.0634373426     0.0003664145    \n",
            "4                0.9385271072     0.9381116033     0.0004154811    \n",
            "5                1.1271146536     1.1268435717     0.0002710268    \n",
            "6                1.2339220047     1.2335380316     0.0003839491    \n",
            "7                2.2793459892     2.2789850235     0.0003609039    \n",
            "8                1.3636265993     1.3632889986     0.0003375827    \n",
            "9                0.9475824237     0.9473512769     0.0002311745    \n",
            "Counter({np.int64(0): 2352, np.int64(1): 1704})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.9071120620     0.1530423611     1.0601544380     0.8394970414     0.8242843040     0.7965821389     2.5015001297    \n",
            "1                0.4602829516     0.0938491747     0.5541321039     0.8491124260     0.8292201382     0.8120176406     5.0307910442    \n",
            "2                1.2159512043     0.3753017485     1.5912529230     0.8424556213     0.7996051333     0.8395810364     7.3120212555    \n",
            "3                0.9902100563     0.2488907576     1.2391008139     0.8365384615     0.8055281343     0.8026460860     9.6329329014    \n",
            "4                1.5152767897     0.4502246082     1.9655014277     0.7778599606     0.7482724580     0.7789415656     12.1121068001   \n",
            "5                0.8795347810     0.1399251968     1.0194599628     0.8530571992     0.8163869694     0.8142227122     14.6463525295   \n",
            "6                0.3394018412     0.3316158950     0.6710177660     0.8831360947     0.8292201382     0.8340683572     17.1750667095   \n",
            "7                0.4395068884     0.2117160112     0.6512228847     0.8286489152     0.8005923001     0.8219404631     19.4810490608   \n",
            "8                0.3465292752     0.3200278580     0.6665571332     0.8572485207     0.8025666338     0.7872105843     21.8032319546   \n",
            "9                0.5326618552     0.5876652598     1.1203271151     0.7453155819     0.7166831194     0.6714443219     24.1230025291   \n",
            "\n",
            "======== ROUND 1 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.2701067924    \n",
            "1                0.4216512740    \n",
            "2                0.3309559822    \n",
            "3                0.7202114463    \n",
            "4                0.8600301743    \n",
            "5                0.6905670762    \n",
            "6                0.6859095693    \n",
            "7                1.0962805748    \n",
            "8                0.6777515411    \n",
            "9                0.7494497299    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4632594585     1.0560513735     0.4072080553    \n",
            "1                1.0983163118     0.8139734268     0.2843428850    \n",
            "2                1.2549240589     1.1201976538     0.1347264647    \n",
            "3                0.9869813919     0.9040665030     0.0829148665    \n",
            "4                1.2767447233     1.1568707228     0.1198739484    \n",
            "5                0.8004634976     0.7504047751     0.0500587113    \n",
            "6                1.5888702869     1.2480206490     0.3408496380    \n",
            "7                1.1262925863     1.0390390158     0.0872535408    \n",
            "8                1.8429434299     1.6416112185     0.2013322115    \n",
            "9                1.2551842928     1.2434014082     0.0117829256    \n",
            "Counter({np.int64(0): 2207, np.int64(1): 1849})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "10               0.6725009084     0.3764210641     1.0489219427     0.7800788955     0.6920039487     0.7480705623     2.3207490444    \n",
            "11               0.5430507064     0.3199160397     0.8629667759     0.8641518738     0.8065153011     0.7778390298     4.6519775391    \n",
            "12               0.3014619052     0.2520745099     0.5535364151     0.9011341223     0.8341559724     0.8120176406     7.0097985268    \n",
            "13               0.5508163571     0.2129004151     0.7637167573     0.8182938856     0.7759131293     0.7315325248     9.3882923126    \n",
            "14               1.2184292078     0.3021604717     1.5205897093     0.8823964497     0.7976307996     0.8164277839     12.0896813869   \n",
            "15               1.1529437304     0.6626801491     1.8156238794     0.7773668639     0.7285291214     0.7271223815     14.4524948597   \n",
            "16               0.5880652666     0.3397040069     0.9277693033     0.8715483235     0.7907206318     0.7877618523     16.8000490665   \n",
            "17               0.9534872174     0.2259774804     1.1794646978     0.7956114398     0.7275419546     0.6874310915     19.1550898552   \n",
            "18               0.2621722221     0.2096695900     0.4718418121     0.9011341223     0.8094768016     0.7971334068     21.4747328758   \n",
            "19               0.2938413024     0.0922120512     0.3860533535     0.9090236686     0.8292201382     0.8500551268     24.1976802349   \n",
            "\n",
            "======== ROUND 2 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7414863110    \n",
            "1                0.4091318846    \n",
            "2                0.9745694995    \n",
            "3                0.4853703976    \n",
            "4                0.3713736236    \n",
            "5                0.5061742663    \n",
            "6                0.1755568385    \n",
            "7                0.4593176842    \n",
            "8                0.6674804688    \n",
            "9                0.3580988944    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8817182779     1.0857187510     0.7959995270    \n",
            "1                1.1723307371     1.1442993879     0.0280313622    \n",
            "2                1.0514765978     0.9229769111     0.1284997016    \n",
            "3                1.1314716339     1.0526068211     0.0788647905    \n",
            "4                1.0617069006     0.9768545628     0.0848523304    \n",
            "5                0.9221816063     0.9049944282     0.0171872042    \n",
            "6                1.0316548347     1.0173369646     0.0143179297    \n",
            "7                1.0167796612     1.0076040030     0.0091756852    \n",
            "8                1.3730241060     1.3216594458     0.0513646491    \n",
            "9                1.0903633833     1.0517755747     0.0385878608    \n",
            "Counter({np.int64(0): 2200, np.int64(1): 1856})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "20               0.4450617731     0.3842200339     0.8292818069     0.8890532544     0.8193484699     0.7971334068     2.3707890511    \n",
            "21               0.2555824816     0.0799185708     0.3355010450     0.9250493097     0.8351431392     0.8241455347     4.8015203476    \n",
            "22               0.6628383994     0.8374769688     1.5003154278     0.7226331361     0.6347482725     0.6400220507     7.4847390652    \n",
            "23               0.2349389941     0.3896982372     0.6246372461     0.7573964497     0.6959526160     0.6532524807     9.8080296516    \n",
            "24               0.7749075294     0.4521796703     1.2270872593     0.8895463511     0.7946692991     0.7778390298     12.1667563915   \n",
            "25               0.1611269563     0.4273943007     0.5885212421     0.9252958580     0.8311944719     0.8302094818     14.5000305176   \n",
            "26               0.1673129648     0.3665324748     0.5338454247     0.9324457594     0.8400789733     0.8246968026     16.8678472042   \n",
            "27               0.6917281151     0.1895387620     0.8812668920     0.9058185404     0.7917077986     0.7872105843     19.5828003883   \n",
            "28               0.5897303820     0.1875068694     0.7772372365     0.9336785010     0.8134254689     0.7794928335     21.9247159958   \n",
            "29               0.1340735108     0.1900839359     0.3241574466     0.9373767258     0.8292201382     0.8103638368     24.3510215282   \n",
            "\n",
            "======== ROUND 3 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5562281609    \n",
            "1                0.7625988126    \n",
            "2                0.2669244707    \n",
            "3                0.2455037385    \n",
            "4                0.1971369982    \n",
            "5                0.8467562199    \n",
            "6                0.6344718933    \n",
            "7                0.1023689285    \n",
            "8                0.3152773678    \n",
            "9                0.3608242571    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1771053076     1.1744543314     0.0026509168    \n",
            "1                1.4353249073     1.3801940680     0.0551308580    \n",
            "2                1.8332812786     1.7904843092     0.0427969284    \n",
            "3                1.8273806572     1.4895757437     0.3378048837    \n",
            "4                1.0318530798     0.8586273193     0.1732257158    \n",
            "5                1.8466193676     1.0971869230     0.7494325042    \n",
            "6                0.8739894032     0.7983036637     0.0756857544    \n",
            "7                1.1827197075     0.8732598424     0.3094598651    \n",
            "8                0.8283202052     0.8241667747     0.0041534449    \n",
            "9                1.4881147146     1.4868717194     0.0012430429    \n",
            "Counter({np.int64(0): 2276, np.int64(1): 1780})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "30               1.3380283117     0.4702086449     1.8082369566     0.6568047337     0.5844027641     0.5578831312     2.5688927174    \n",
            "31               0.2552622855     0.2745472789     0.5298095942     0.8912721893     0.7847976308     0.7778390298     4.9303102493    \n",
            "32               0.0928440616     0.4010451138     0.4938891828     0.8491124260     0.7403751234     0.7265711136     7.3003990650    \n",
            "33               0.1058477983     0.1712006778     0.2770484686     0.9516765286     0.8460019743     0.8065049614     9.6821460724    \n",
            "34               0.2644875944     0.2522675097     0.5167551041     0.9435404339     0.8390918065     0.8164277839     12.4234495163   \n",
            "35               0.6140697598     0.2266049981     0.8406747580     0.9341715976     0.8183613031     0.7894156560     14.9640626907   \n",
            "36               0.6173297763     0.3346448243     0.9519746304     0.9400887574     0.8173741362     0.7816979052     17.3321974277   \n",
            "37               0.4174499810     0.8774619699     1.2949119806     0.9077909270     0.7917077986     0.7475192944     19.7294704914   \n",
            "38               0.2959396541     0.4795157909     0.7754554749     0.8799309665     0.7443237907     0.7392502756     22.0936901569   \n",
            "39               0.1542942971     0.1423321217     0.2966264188     0.9132149901     0.7788746298     0.7613009923     24.9105057716   \n",
            "\n",
            "======== ROUND 4 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.2435017973    \n",
            "1                0.7483136058    \n",
            "2                0.3836405277    \n",
            "3                0.4609043300    \n",
            "4                1.0933161974    \n",
            "5                0.3558700383    \n",
            "6                0.1218620241    \n",
            "7                0.3715787828    \n",
            "8                0.2329000086    \n",
            "9                0.3990495503    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2599025965     1.2240366936     0.0358659253    \n",
            "1                1.2702761889     1.2656978369     0.0045783464    \n",
            "2                1.3982837200     1.2042483091     0.1940353513    \n",
            "3                1.1373211145     1.0919637680     0.0453573726    \n",
            "4                0.7830522060     0.7663832307     0.0166689847    \n",
            "5                2.0400736332     1.3918242455     0.6482494473    \n",
            "6                1.1097503901     1.0027450323     0.1070053577    \n",
            "7                0.7915209532     0.6937856674     0.0977353081    \n",
            "8                2.2393548489     1.7401614189     0.4991934597    \n",
            "9                1.1136704683     0.8734954000     0.2401750833    \n",
            "Counter({np.int64(0): 2295, np.int64(1): 1761})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "40               0.7890708447     0.1580068916     0.9470777512     0.9356508876     0.8341559724     0.8081587652     2.3888616562    \n",
            "41               0.2404758483     0.1100096777     0.3504855335     0.9509368836     0.8242843040     0.7828004410     4.7396001816    \n",
            "42               0.5255157351     0.0967055783     0.6222212911     0.9423076923     0.8272458045     0.8026460860     7.6830630302    \n",
            "43               0.2728337646     0.6069045067     0.8797382712     0.8843688363     0.7808489635     0.7436604190     10.1671032906   \n",
            "44               0.4614710808     0.3184138536     0.7798849344     0.8417159763     0.7403751234     0.7105843440     12.4883050919   \n",
            "45               0.0759600773     0.3522934020     0.4282534719     0.9339250493     0.8262586377     0.7861080485     14.8805913925   \n",
            "46               0.4142614901     0.6158556342     1.0301171541     0.9294871795     0.7976307996     0.7772877619     17.2393443584   \n",
            "47               0.5760317445     0.3478835523     0.9239152670     0.9114891519     0.7699901283     0.7480705623     20.1371588707   \n",
            "48               0.2558712959     0.0886633769     0.3445346653     0.9302268245     0.8104639684     0.7778390298     22.5325515270   \n",
            "49               0.1032603160     0.3113809228     0.4146412313     0.9304733728     0.8203356367     0.7833517089     24.8956100941   \n",
            "\n",
            "======== ROUND 5 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4193317592    \n",
            "1                0.2099401355    \n",
            "2                0.1656114608    \n",
            "3                0.4668723345    \n",
            "4                0.3999737799    \n",
            "5                0.5397297144    \n",
            "6                0.2539355755    \n",
            "7                0.3120800853    \n",
            "8                0.2825700045    \n",
            "9                0.9215582013    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3445534706     1.2378178835     0.1067356169    \n",
            "1                1.7041097879     1.6773098707     0.0267999172    \n",
            "2                1.1292755604     1.1121734381     0.0171020683    \n",
            "3                0.8882318735     0.8218820095     0.0663498640    \n",
            "4                0.7880983353     0.7831481099     0.0049502100    \n",
            "5                1.0975664854     0.9784007072     0.1191657260    \n",
            "6                1.5239804983     1.4955376387     0.0284428746    \n",
            "7                1.3506997824     1.3196200132     0.0310798045    \n",
            "8                1.3691895008     1.3352375031     0.0339520089    \n",
            "9                1.0494657755     1.0182322264     0.0312335622    \n",
            "Counter({np.int64(0): 2293, np.int64(1): 1763})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "50               0.5379256010     0.0859747157     0.6239002943     0.9371301775     0.8272458045     0.7789415656     2.8535857201    \n",
            "51               0.2685683668     0.2006955743     0.4692639410     0.9215976331     0.7867719645     0.7436604190     5.1906552315    \n",
            "52               1.4989522696     0.7599036694     2.2588558197     0.7840236686     0.6515301086     0.6008820287     7.5326852798    \n",
            "53               0.4182057083     0.6060723662     1.0242780447     0.9119822485     0.8065153011     0.7684674752     9.8960151672    \n",
            "54               0.1822076887     0.2046935707     0.3869012594     0.9267751479     0.7976307996     0.7756339581     12.3542826176   \n",
            "55               0.4259686768     0.8557984233     1.2817671299     0.8651380671     0.7226061204     0.7050716648     15.2326717377   \n",
            "56               0.1917648911     0.1360141635     0.3277790546     0.9420611440     0.7917077986     0.7646085998     17.6747410297   \n",
            "57               0.4548950195     0.3175343573     0.7724293470     0.9425542406     0.8055281343     0.7651598677     20.0278861523   \n",
            "58               0.6955798268     0.6460523009     1.3416321278     0.8910256410     0.7295162883     0.6945975744     22.4017629623   \n",
            "59               1.0477191210     0.7584819198     1.8062009811     0.9240631164     0.7976307996     0.7635060639     24.7869172096   \n",
            "\n",
            "======== ROUND 6 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5108665824    \n",
            "1                0.1126843914    \n",
            "2                0.1796440035    \n",
            "3                0.4614936113    \n",
            "4                0.1887363195    \n",
            "5                0.4182114601    \n",
            "6                0.4931162894    \n",
            "7                0.1254846305    \n",
            "8                0.7131426334    \n",
            "9                0.0807336494    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3919140100     1.2630456686     0.1288682967    \n",
            "1                0.9649701715     0.9305108190     0.0344593488    \n",
            "2                1.6867893934     1.2300609350     0.4567284882    \n",
            "3                1.2911536694     1.2696390152     0.0215146244    \n",
            "4                1.3790966272     1.3755888939     0.0035077184    \n",
            "5                1.4292103052     1.4249445200     0.0042657708    \n",
            "6                0.9975420237     0.9814257622     0.0161162652    \n",
            "7                1.0397400856     1.0348855257     0.0048545212    \n",
            "8                1.2055183649     1.2026790380     0.0028393182    \n",
            "9                1.0212744474     0.8213220239     0.1999524236    \n",
            "Counter({np.int64(0): 2298, np.int64(1): 1758})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "60               0.8800313473     0.2340927273     1.1141240597     0.9319526627     0.7660414610     0.7624035281     2.3682339191    \n",
            "61               0.5659434795     0.4678176939     1.0337611437     0.8589743590     0.7137216190     0.7028665932     4.7494871616    \n",
            "62               0.4572014511     0.7074043751     1.1646058559     0.9109960552     0.7601184600     0.7673649394     7.3380758762    \n",
            "63               0.2843428552     0.2197571993     0.5041000843     0.9077909270     0.7581441264     0.7342888644     9.8687191010    \n",
            "64               0.2554535568     0.5092984438     0.7647520304     0.9149408284     0.7778874630     0.7541345094     12.1839342117   \n",
            "65               0.3599217832     0.4430554807     0.8029772639     0.8974358974     0.7611056269     0.7431091510     14.5802540779   \n",
            "66               0.7324007154     1.0798106194     1.8122112751     0.8234714004     0.6762092794     0.6703417861     16.9886691570   \n",
            "67               0.3130305409     0.5154424310     0.8284729719     0.9097633136     0.7620927937     0.7298787211     19.6514229774   \n",
            "68               0.7306003571     0.3655258715     1.0961261988     0.8969428008     0.7512339585     0.7155457552     22.1813991070   \n",
            "69               0.3880076110     0.9098379016     1.2978454828     0.7502465483     0.6209279368     0.5909592062     24.5063045025   \n",
            "\n",
            "======== ROUND 7 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8713979721    \n",
            "1                0.5504053235    \n",
            "2                0.1554810107    \n",
            "3                0.2299747020    \n",
            "4                0.1457040310    \n",
            "5                0.0975833610    \n",
            "6                0.4591769874    \n",
            "7                0.4911739826    \n",
            "8                0.1493337005    \n",
            "9                0.1833117157    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1872106791     1.1839207411     0.0032899436    \n",
            "1                0.9853876829     0.8991387486     0.0862489268    \n",
            "2                1.8932350874     1.8515549898     0.0416801311    \n",
            "3                0.9443073273     0.7859692574     0.1583380699    \n",
            "4                1.0739232302     1.0688129663     0.0051103118    \n",
            "5                1.1601687670     1.1375602484     0.0226085540    \n",
            "6                1.6209436655     1.6152423620     0.0057012495    \n",
            "7                1.4590771198     1.1720439196     0.2870331705    \n",
            "8                1.1158708334     1.0680334568     0.0478374176    \n",
            "9                1.6135357618     1.6025265455     0.0110092266    \n",
            "Counter({np.int64(0): 2311, np.int64(1): 1745})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "70               0.3534034789     0.2275861353     0.5809895992     0.9124753452     0.7759131293     0.7337375965     2.8037106991    \n",
            "71               0.9088681340     0.4028411210     1.3117092848     0.9026134122     0.7532082922     0.7293274531     5.3059809208    \n",
            "72               0.4490691125     0.5762169957     1.0252860785     0.9469921105     0.7996051333     0.7701212789     7.6398720741    \n",
            "73               0.4470150173     0.6856343150     1.1326493025     0.7771203156     0.6732477789     0.6389195149     10.0306913853   \n",
            "74               0.6608274579     0.1647226214     0.8255500793     0.9001479290     0.7581441264     0.7171995590     12.4024615288   \n",
            "75               0.2483121604     0.5876426101     0.8359547853     0.8510848126     0.7245804541     0.7028665932     15.2350454330   \n",
            "76               0.1230797693     0.3633079827     0.4863877594     0.9487179487     0.7917077986     0.7695700110     17.7377109528   \n",
            "77               0.3221192658     0.1709572524     0.4930765033     0.9391025641     0.7788746298     0.7618522602     20.1216526031   \n",
            "78               0.5337923169     0.3674773872     0.9012696743     0.8708086785     0.7087857848     0.6885336273     22.4823117256   \n",
            "79               1.3867071867     0.7650237083     2.1517310143     0.5458579882     0.4698914116     0.4746416759     24.8350200653   \n",
            "\n",
            "======== ROUND 8 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6426362395    \n",
            "1                0.4130718708    \n",
            "2                0.1847572178    \n",
            "3                0.4125816822    \n",
            "4                0.1903820783    \n",
            "5                0.1393965036    \n",
            "6                0.2097888738    \n",
            "7                0.3375588953    \n",
            "8                0.1914475709    \n",
            "9                0.0903116539    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1893504858     1.0387362242     0.1506142914    \n",
            "1                0.6864145994     0.6852128506     0.0012017615    \n",
            "2                1.3815473318     1.2970894575     0.0844578743    \n",
            "3                1.3262724876     1.3066822290     0.0195902009    \n",
            "4                1.2122677565     1.1363140345     0.0759537295    \n",
            "5                1.0832369328     1.0818079710     0.0014289803    \n",
            "6                1.0868327618     1.0796369314     0.0071958010    \n",
            "7                0.9883333445     0.9824216366     0.0059117260    \n",
            "8                1.2675288916     1.1367579699     0.1307709068    \n",
            "9                1.8676993847     0.8889183402     0.9787809849    \n",
            "Counter({np.int64(0): 2275, np.int64(1): 1781})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "80               0.2300316542     0.3488480747     0.5788797140     0.8656311637     0.7186574531     0.6962513782     2.3576250076    \n",
            "81               0.3008757532     0.8216018677     1.1224776506     0.8922583826     0.7433366239     0.7238147740     4.7512750626    \n",
            "82               0.4423739016     0.4070319235     0.8494058251     0.7460552268     0.6179664363     0.6003307607     7.0782697201    \n",
            "83               0.2633121312     0.2835037112     0.5468158722     0.9378698225     0.7996051333     0.7513781698     9.9601652622    \n",
            "84               0.4711724520     0.7913886905     1.2625610828     0.8350591716     0.6969397828     0.7061742007     12.4126267433   \n",
            "85               0.4951489270     0.4500575960     0.9452065229     0.8782051282     0.7206317868     0.7072767365     14.7477049828   \n",
            "86               0.4255438745     0.3292704523     0.7548143268     0.8971893491     0.7680157947     0.7238147740     17.0958378315   \n",
            "87               0.6234664321     0.3391620219     0.9626284838     0.9142011834     0.7719644620     0.7375964719     19.4252538681   \n",
            "88               0.5006521344     0.3145994842     0.8152515888     0.9479783037     0.7917077986     0.7414553473     22.3108077049   \n",
            "89               0.6846122146     0.4729097784     1.1575219631     0.8745069034     0.7324777887     0.6846747519     24.7495977879   \n",
            "\n",
            "======== ROUND 9 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.1495487839    \n",
            "1                0.1239007935    \n",
            "2                0.0566159375    \n",
            "3                0.1230592132    \n",
            "4                0.0290694889    \n",
            "5                0.2338376641    \n",
            "6                0.3182626665    \n",
            "7                0.9280526042    \n",
            "8                0.3107112646    \n",
            "9                0.3300766349    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0108861923     0.9905243516     0.0203618836    \n",
            "1                1.2509195805     1.2399003506     0.0110191777    \n",
            "2                1.1243056059     1.0858968496     0.0384087823    \n",
            "3                1.5084897280     1.0337208509     0.4747688770    \n",
            "4                0.9738945961     0.9263300896     0.0475644954    \n",
            "5                1.5771428347     1.5556740761     0.0214687232    \n",
            "6                1.6628974676     1.2094646692     0.4534327686    \n",
            "7                2.0512549877     2.0435736179     0.0076813255    \n",
            "8                1.0717132092     1.0155330896     0.0561801307    \n",
            "9                1.2211649418     1.1981059313     0.0230590701    \n",
            "Counter({np.int64(0): 2280, np.int64(1): 1776})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "90               0.6276975274     0.9163436294     1.5440411568     0.7914201183     0.6673247779     0.6775082690     2.3456802368    \n",
            "91               0.5066201091     0.2911584377     0.7977785468     0.9292406312     0.7887462981     0.7629547960     5.2427787781    \n",
            "92               0.6350280643     0.4932742119     1.1283023357     0.8836291913     0.7334649556     0.7100330761     7.6166374683    \n",
            "93               0.2813384831     0.3264868259     0.6078252792     0.8745069034     0.7403751234     0.7078280044     10.0113642216   \n",
            "94               0.3463819921     0.6137804985     0.9601625204     0.9171597633     0.7551826259     0.7353914002     12.3578174114   \n",
            "95               0.3087497950     0.6880207658     0.9967705607     0.8473865878     0.7147087858     0.6708930540     14.6799020767   \n",
            "96               0.7277352214     0.3794330657     1.1071683168     0.8634122288     0.7156959526     0.6923925028     17.5964233875   \n",
            "97               0.7099044919     0.9095880985     1.6194925308     0.7376725838     0.5952615992     0.5545755237     19.9838247299   \n",
            "98               0.3069172204     0.5794281363     0.8863453865     0.9193786982     0.7709772952     0.7409040794     22.3680725098   \n",
            "99               0.2515418828     0.5407012105     0.7922431231     0.9593195266     0.8153998026     0.7640573319     24.7053186893   \n",
            "\n",
            "======== ROUND 10 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5453514457    \n",
            "1                0.1485768408    \n",
            "2                0.2737807333    \n",
            "3                0.1384942085    \n",
            "4                0.4125322998    \n",
            "5                0.1242898479    \n",
            "6                0.4879685938    \n",
            "7                0.1785558313    \n",
            "8                0.8529241681    \n",
            "9                0.2629669011    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.6723369360     1.5374039412     0.1349329948    \n",
            "1                1.2250972986     0.9174104333     0.3076868951    \n",
            "2                1.9403077364     1.9076361656     0.0326715186    \n",
            "3                1.3672533035     1.1122500896     0.2550031543    \n",
            "4                0.9369528294     0.9330567718     0.0038960751    \n",
            "5                1.0727732182     0.9108586907     0.1619145572    \n",
            "6                1.3062964678     0.9606973529     0.3455990851    \n",
            "7                1.0093318224     0.9080150723     0.1013167500    \n",
            "8                1.2973270416     1.2935799360     0.0037471077    \n",
            "9                1.2514491081     1.2394067049     0.0120424032    \n",
            "Counter({np.int64(0): 2335, np.int64(1): 1721})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "100              0.2523173094     0.7279672623     0.9802845716     0.9334319527     0.8124383021     0.7910694598     2.4205765724    \n",
            "101              0.3686830103     0.5782396197     0.9469226599     0.9188856016     0.7512339585     0.7585446527     4.7969589233    \n",
            "102              0.3168278635     0.6189190745     0.9357469082     0.8838757396     0.7186574531     0.7227122381     7.1747701168    \n",
            "103              0.5882825255     0.7809228301     1.3692053556     0.9031065089     0.7581441264     0.7458654906     9.5182201862    \n",
            "104              0.5048266053     0.6004145145     1.1052410603     0.8266765286     0.6900296150     0.6830209482     12.3901267052   \n",
            "105              0.5272375941     0.6331033111     1.1603409052     0.9033530572     0.7374136229     0.7243660419     14.7614958286   \n",
            "106              0.5147424340     0.8591478467     1.3738902807     0.8520710059     0.7058242843     0.7149944873     17.2126793861   \n",
            "107              0.6422548890     1.2507511377     1.8930060863     0.8621794872     0.7166831194     0.6973539140     19.5763149261   \n",
            "108              0.4830122888     0.5552657843     1.0382781029     0.9415680473     0.7680157947     0.7640573319     21.9373567104   \n",
            "109              0.4858205318     0.5521187782     1.0379393101     0.8049802761     0.6525172754     0.6653803749     24.7440373898   \n",
            "\n",
            "======== ROUND 11 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.1356477141    \n",
            "1                0.1793799847    \n",
            "2                0.5509665608    \n",
            "3                0.1691510528    \n",
            "4                0.0616862774    \n",
            "5                0.1740784496    \n",
            "6                0.1791899651    \n",
            "7                0.2064987570    \n",
            "8                0.0950514302    \n",
            "9                0.7300521731    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1827433109     0.9937732220     0.1889700890    \n",
            "1                1.0562646389     1.0460022688     0.0102623329    \n",
            "2                1.1701318026     1.1051729918     0.0649587810    \n",
            "3                1.5071519613     1.3739285469     0.1332234293    \n",
            "4                1.0667369366     1.0651038885     0.0016329897    \n",
            "5                1.2769247293     1.2483724356     0.0285522398    \n",
            "6                1.1447911263     1.1365423203     0.0082488637    \n",
            "7                1.2132005692     1.2044771910     0.0087233400    \n",
            "8                2.2395610809     2.1324925423     0.1070685461    \n",
            "9                1.1278322935     1.1045349836     0.0232973304    \n",
            "Counter({np.int64(0): 2344, np.int64(1): 1712})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "110              0.6059692502     0.5072779655     1.1132471561     0.8888067061     0.7334649556     0.7116868798     2.3328659534    \n",
            "111              0.5730459094     0.5575241446     1.1305700541     0.8572485207     0.7048371175     0.7177508269     4.6599063873    \n",
            "112              0.3728748262     0.4585316181     0.8314064741     0.8979289941     0.7492596249     0.7420066152     7.3797676563    \n",
            "113              0.4075883925     0.8109428287     1.2185312510     0.6787475345     0.5804540967     0.5418963616     9.7292885780    \n",
            "114              0.2982068956     0.4393047094     0.7375116348     0.8338264300     0.6969397828     0.6764057332     12.0514900684   \n",
            "115              0.6300740838     0.4723693430     1.1024434566     0.8392504931     0.6841066140     0.6863285557     14.3846702576   \n",
            "116              0.4892173707     0.4550206363     0.9442380071     0.9499506903     0.7788746298     0.7717750827     16.7216136456   \n",
            "117              0.4060092270     0.5499264598     0.9559357166     0.9514299803     0.7650542942     0.7635060639     19.4805786610   \n",
            "118              0.6290772557     0.6800517440     1.3091289997     0.9164201183     0.7462981244     0.7337375965     21.8401408195   \n",
            "119              0.3338336945     0.5156569481     0.8494906425     0.9203648915     0.7423494571     0.7298787211     24.2156403065   \n",
            "\n",
            "======== ROUND 12 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.3552421331    \n",
            "1                0.2026425749    \n",
            "2                0.2413564920    \n",
            "3                0.0573246665    \n",
            "4                0.2866814435    \n",
            "5                0.6722018719    \n",
            "6                0.2210599035    \n",
            "7                0.2359258085    \n",
            "8                0.6118981242    \n",
            "9                0.0695552006    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2318422794     1.1561429501     0.0756993145    \n",
            "1                1.3517030478     1.0663541555     0.2853489220    \n",
            "2                1.1351010799     1.0147916079     0.1203095242    \n",
            "3                1.0203158855     1.0053793192     0.0149365170    \n",
            "4                1.4779549837     1.0941356421     0.3838193715    \n",
            "5                1.3907022476     1.1016581059     0.2890440822    \n",
            "6                0.8456634879     0.7870764732     0.0585870147    \n",
            "7                0.8818493485     0.8653054833     0.0165438876    \n",
            "8                2.0680887699     2.0460817814     0.0220070537    \n",
            "9                1.3104197979     1.1702553034     0.1401645094    \n",
            "Counter({np.int64(0): 2350, np.int64(1): 1706})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "120              0.6407617927     0.6947494149     1.3355112076     0.8333333333     0.6623889437     0.6835722161     2.8616065979    \n",
            "121              0.2445598245     0.3985793889     0.6431392431     0.9112426036     0.7551826259     0.7436604190     5.2048530579    \n",
            "122              0.7042315602     0.4749573171     1.1791888475     0.9023668639     0.7571569595     0.7414553473     7.5463664532    \n",
            "123              0.9719981551     0.8128814697     1.7848796844     0.7196745562     0.5883514314     0.5518191841     9.8969752789    \n",
            "124              0.7368056774     0.7017560601     1.4385616779     0.8276627219     0.6663376111     0.6400220507     12.2485184669   \n",
            "125              0.6176796556     0.4860670567     1.1037466526     0.8602071006     0.6929911155     0.6945975744     15.0425260067   \n",
            "126              0.5136476159     0.7811371684     1.2947847843     0.7798323471     0.6446199408     0.5948180816     17.4694578648   \n",
            "127              0.4568516314     0.3447034657     0.8015550971     0.9371301775     0.7828232971     0.7613009923     19.8528430462   \n",
            "128              0.1275287420     0.4576954842     0.5852242112     0.9457593688     0.7966436328     0.7436604190     22.1941869259   \n",
            "129              0.2798139155     0.4963382185     0.7761521339     0.9144477318     0.7374136229     0.7282249173     24.5256586075   \n",
            "\n",
            "======== ROUND 13 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.2749931216    \n",
            "1                0.1897182316    \n",
            "2                0.1779852062    \n",
            "3                0.1200258061    \n",
            "4                0.5073102117    \n",
            "5                0.0258899499    \n",
            "6                0.1558283269    \n",
            "7                0.9085917473    \n",
            "8                0.3180356324    \n",
            "9                0.2688753903    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0144423246     1.0124937296     0.0019486417    \n",
            "1                1.0634380579     1.0227066278     0.0407314561    \n",
            "2                1.3942432404     1.3859342337     0.0083089834    \n",
            "3                1.3686246872     0.8861668706     0.4824578762    \n",
            "4                2.0061590672     1.8832577467     0.1229012981    \n",
            "5                1.1251617670     1.1204465628     0.0047152527    \n",
            "6                1.0264089108     0.9788419604     0.0475669764    \n",
            "7                1.5800514221     1.2024363279     0.3776150644    \n",
            "8                1.1526837349     0.9293835759     0.2233001143    \n",
            "9                0.9751706719     0.8425405025     0.1326301694    \n",
            "Counter({np.int64(0): 2383, np.int64(1): 1673})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "130              0.6075211167     0.5350598097     1.1425809860     0.8843688363     0.7512339585     0.7282249173     2.3335564137    \n",
            "131              0.5893751979     0.5587911010     1.1481662989     0.8291420118     0.6722606120     0.6631753032     4.6856679916    \n",
            "132              0.3507448733     0.2719784081     0.6227232814     0.9297337278     0.7808489635     0.7618522602     7.0960290432    \n",
            "133              0.5766515136     0.7896789908     1.3663305044     0.7482741617     0.5942744324     0.5804851158     9.9949460030    \n",
            "134              0.2421679497     0.3922920525     0.6344599724     0.9294871795     0.7828232971     0.7850055127     12.3319878578   \n",
            "135              0.5451175570     0.6214519143     1.1665694714     0.9285009862     0.7393879566     0.7530319735     14.7494630814   \n",
            "136              0.3398274183     0.3566993773     0.6965267658     0.8816568047     0.6999012833     0.7078280044     17.2023682594   \n",
            "137              0.5552006960     0.4886058867     1.0438065529     0.8296351085     0.6386969398     0.6659316428     19.6557798386   \n",
            "138              0.4662233293     0.5422804952     1.0085037947     0.8949704142     0.6910167818     0.7061742007     22.5629510880   \n",
            "139              0.4248350859     0.5560004115     0.9808354974     0.9489644970     0.7680157947     0.7414553473     24.9939906597   \n",
            "\n",
            "======== ROUND 14 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.2762452662    \n",
            "1                0.0587610118    \n",
            "2                0.1612640023    \n",
            "3                0.3996964693    \n",
            "4                0.1851470917    \n",
            "5                0.1675429791    \n",
            "6                0.2591931224    \n",
            "7                0.1169319078    \n",
            "8                2.0209217072    \n",
            "9                0.5090382695    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0472995043     0.9994055629     0.0478939563    \n",
            "1                1.8498160839     1.8439559937     0.0058601131    \n",
            "2                1.3543518782     1.3028895855     0.0514622591    \n",
            "3                1.0417250395     1.0055031776     0.0362218656    \n",
            "4                1.4219837189     1.3550280333     0.0669556633    \n",
            "5                1.4973906279     1.3707090616     0.1266815513    \n",
            "6                1.4531590939     1.3785924911     0.0745666549    \n",
            "7                1.1884403229     1.1117312908     0.0767090693    \n",
            "8                1.4450453520     1.4198484421     0.0251968578    \n",
            "9                1.2774132490     1.1875941753     0.0898191258    \n",
            "Counter({np.int64(0): 2376, np.int64(1): 1680})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "140              0.3963128328     0.6203381419     1.0166509151     0.8518244576     0.6831194472     0.6857772878     2.4742853642    \n",
            "141              0.4925827980     0.3271157444     0.8196985722     0.8838757396     0.7285291214     0.7144432194     5.1148054600    \n",
            "142              0.4051189125     0.5200404525     0.9251593351     0.9114891519     0.7245804541     0.7183020948     7.4440708160    \n",
            "143              0.5348072648     0.5214285254     1.0562357903     0.8730276134     0.6732477789     0.6907386990     9.8004775047    \n",
            "144              0.3281181753     0.8664464355     1.1945645809     0.8959566075     0.7226061204     0.6957001103     12.1554207802   \n",
            "145              0.3807220757     0.3952127993     0.7759348750     0.9265285996     0.7492596249     0.7216097023     14.6130890846   \n",
            "146              0.2274940163     0.5366428494     0.7641368508     0.8769723866     0.6989141165     0.6990077178     17.3180434704   \n",
            "147              0.3136292994     0.4097875655     0.7234168649     0.9324457594     0.7532082922     0.7001102536     19.6330537796   \n",
            "148              0.1360035837     0.4707038105     0.6067073941     0.9013806706     0.7196446199     0.7056229327     22.0085079670   \n",
            "149              0.4047877491     0.6369706988     1.0417584181     0.9573471400     0.7611056269     0.7502756340     24.3643562794   \n",
            "\n",
            "🎯 Final Target Accuracy: 0.8065\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "[WARN] Adjusting flat_inputs from 1600 to 9600\n",
            "/content/extddivesify/diversify/shap_utils.py:47: FutureWarning:\n",
            "\n",
            "The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "\n",
            "[SHAP] Accuracy Drop: 0.2000\n",
            "[SHAP] Flip Rate: 0.2000\n",
            "[SHAP] Confidence Δ: 0.2637\n",
            "[SHAP] AOPC: 0.1650\n",
            "[SHAP] Entropy: 1.6377\n",
            "[SHAP] Coherence: 0.0224\n",
            "[SHAP] Jaccard: 0.0000\n",
            "[SHAP] Kendall’s Tau: 0.0102\n",
            "[SHAP] Cosine Sim: -0.0363\n",
            "Signal shape before reshape: (8, 1, 200)\n",
            "SHAP value shape before reshape: (8, 1, 200, 6)\n",
            "{'text/html': '<html>\\n<head><meta charset=\"utf-8\" /></head>\\n<body>\\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: \\'local\\'};</script>\\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d92029c4-cb00-4fb2-9385-e5556f23c9c4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d92029c4-cb00-4fb2-9385-e5556f23c9c4\")) {                    Plotly.newPlot(                        \"d92029c4-cb00-4fb2-9385-e5556f23c9c4\",                        [{\"hovertemplate\":\"Time=%{x}\\\\u003cbr\\\\u003eChannel=%{y}\\\\u003cbr\\\\u003eSignal=%{z}\\\\u003cbr\\\\u003eSHAP Importance=%{marker.color}\\\\u003cextra\\\\u003e\\\\u003c\\\\u002fextra\\\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[-0.002367062959820032,-0.007444805543248852,0.0013424410402270344,0.001008138186686362,0.0016681659714474033,0.003418618958676234,0.0027438741720591984,0.0018472734567088385,-0.0013074138745044668,-0.01726088548700015,-0.017437814346825082,0.009038295325202247,-0.010038934803257385,-0.009934015969823426,-0.024457866908051074,-0.0207679618227606,-0.043343725614249706,-0.035204408379892506,0.003789876102625082,-0.019478172063827515,0.022610786370933056,0.009159991595273217,0.030593165662139654,0.009914851126571497,-0.00001020554918795824,-0.009075361886061728,-0.003458511355953912,-0.0069118999332810445,-0.010775161097020222,-0.00834755809046328,-0.007838966092094779,-0.0020183726834754148,-0.012255398052123686,-0.0013087467717317243,-0.0031915666380276284,-0.008177276778345307,-0.009495565745358666,-0.0038209734484553337,-0.004718358119134791,-0.009658110788829314,-0.005746189194420974,-0.0024317303129161396,-0.0016782150293389957,0.008802390308119357,-0.00026383417813728255,0.007717393183459838,-0.0024110091229279837,0.02910384101172288,0.017345817269718584,0.009095833947261175,0.017717247227362048,0.017491315336277086,-0.0045132087349581225,0.007779296157726397,0.0031747668205449977,0.01337193565753599,0.010899079342683157,-0.00038132196641527116,0.013518061023205519,-0.003624160618831714,-0.005851613202442725,-0.007613252693166335,-0.005259773068246432,-0.004681046411860734,-0.00028398152062436566,0.0024870180059224367,-0.004285488408640958,0.010784684379662698,-0.005423100461484864,-0.007422374561429024,-0.0031981990905478597,-0.026293466488520306,0.0021075038239359856,-0.0007300619424010316,-0.004503174684941769,0.011817911795030037,0.005317199897641937,0.0046557008754462,0.0014429766063888867,-0.004580530958871047,-0.0018187313495824735,-0.01116930382947127,-0.013907852912476907,-0.014461449347436428,-0.025914942768091958,-0.0022690535406582057,-0.009214993799105287,-0.0014702740202968319,0.001417081415032347,0.0034527981964250407,-0.0011606008435289066,0.004455942194908857,-0.010496141389012337,-0.008318187921152761,0.002827897279833754,0.0003235382415975134,0.009903034311719239,-0.0027942374193420014,-0.016106637000727158,-0.016035128811684746,-0.01732936607246908,-0.009721353684047548,0.002741354808676988,-0.0003343540399024884,0.006870851250520597,0.0029282185908717415,3.2471337666114173e-6,0.02033424067000548,0.005432585021480918,-0.009399399238949021,-0.014429972507059574,0.011594531048710147,0.01953814811228464,0.016587245278060436,0.05539785139262676,0.031254276943703495,-0.00030745065305382013,-0.004190797335468233,-0.0787570453248918,-0.027560394257307053,-0.032872713481386505,-0.02811551218231519,0.023017950356006622,0.0034574524809916816,0.029904517616766196,0.04838754376396537,-0.006465419001566867,-0.008051137517516812,0.016294484492391348,0.005112667723248403,0.020442011145253975,0.018308818029860657,-0.012117348766575256,0.0031051192975913486,-0.0014078885724302381,-0.016253982515384752,-0.010435662542780241,-0.009937677377214035,-0.003662368554311494,-0.027896523708477616,-0.016958552024637658,0.0488644844541947,0.04049347069424888,0.00673479388206033,0.0348342681924502,-0.02326716963822643,-0.010570541101818284,-0.007421274882896493,-0.013826140435412526,-0.01530484389513731,-0.006976221377650897,-0.0005221065366640687,0.009654161481497189,-0.0018820027374507238,-0.0005361281024912993,-0.0034775297002245984,-0.008974256071572503,-0.008098486772117516,-0.0016267971174481015,-0.0036071011563763022,0.0013702543607602518,0.0007148076547309756,-0.004505543814351161,-0.006776815396733582,-0.0067051047226414084,-0.0034196528528506556,-0.007543027827826639,-0.003738673850117872,-0.004993137474230025,0.02292467560619116,0.01335363076456512,0.11654144680748384,0.02921479210878412,0.015610411452750364,0.11172077525407076,0.029221872178216774,0.02372225447713087,0.017137008835561574,-0.0230823849948744,-0.03520655943430029,-0.053291204695900284,-0.06175876843432585,-0.1052600834518671,-0.0705714263021946,-0.024342901694277923,0.006475913338363171,0.0835172967830052,0.04062804440036416,0.0031382006903489432,0.011251671394954125,-0.012431764664749304,-0.013012681466837725,0.0018271840332696836,-0.0020839762097845473,0.00033654655756739277,0.0012596047454280779,-0.0027866649276499325,0.0008577260790237536,0.0018199842670583166,0.009024540893733501,0.0013631728070322424,0.0030988624372791187,0.0018958120005360495,-0.005200796003919095,-0.005622203064073498,-0.010060322820208967,-0.004340576388737342,-0.008018416022726646,-0.0036466645348506668,-0.0012168781249783933,-0.0029656346887350082,-0.003747075272258371,0.005812512089808782,0.0003737750424382587,0.002185940044000745,-0.002736311173066497,-0.0018592348593908052,-0.005268301775989433,-0.00016066090514262518,0.004180279017115633,0.0021089700361092887,0.004492476064721511,0.003845735297848781,0.0075129975254337,0.032047147396951914,0.015456675202585757,-0.0015486093082775672,-0.004009742056950927,0.0010728313548800845,-0.0009749177455281218,-0.00004133748977134625,0.002443984461327394,-0.0016416572616435587,0.001217801560414955,-0.0008375613251700997,-0.005557424447033554,-0.005643968373381843,-0.003066413182144364,0.001291666509738813,-0.0006889859068905935,-0.0017928808956639841,-0.003416942660502779,-0.008205217212283364,-0.0010123007765893515,-0.002641363845517238,-0.002790389427294334,-0.005688602977897972,1.0934891179203987e-6,-0.0008397345469954113,-0.0030468483843530216,0.004428231623023748,-0.002232222779033085,0.0008073958451859653,0.002107305588045468,-0.00038051692536100745,0.008116720399508873,0.002966245975888645,-0.008130353080559871,-0.006559866247698665,-0.009731472013906265,0.0019601852206202843,0.000010166380282801887,0.003976136019142966,0.005096096312627196,0.0022745219563754895,0.003689449008864661,0.003010956628713757,-0.0001672752065739284,-0.0018253721452007692,0.0003897847200278193,0.0009729208347077171,0.004293778561986983,0.003921130999515299,0.0055671674975504475,0.002402945169402907,0.004666196982725523,0.004456999265433599,0.0017443670415862773,0.0016177757497644052,0.0028290462602550783,-9.410238514343897e-6,0.00014569555060006678,-0.0018560869163290288,-0.013618998850385347,-0.00965944491326809,-0.0007280577750255665,-6.875876958171526e-7,-0.0017819001125947882,-0.001559418470909198,0.00004273395946559807,0.003364102491104859,-0.0015731385501567274,-0.0053093044164900976,-0.00223684060620144,-0.003947420123343666,0.0005198954895604402,0.004184443212579936,0.0010896791160727541,0.003613335662521422,0.003176815875728304,-0.00038115563802421093,-0.004808064899407327,-0.0025400437880307436,-0.0030470700867226697,-0.0021855934658863894,-0.002334647278379028,-0.008830414696907004,-0.015894990880042315,-0.009650410308192173,0.0004431668106311311,0.011407679674448445,0.004253960757826765,0.0033909309034546218,-0.004100755070491384,-0.0069145904465888934,-0.010951442954440912,-0.007509370703094949,0.0019595740207781396,-0.0022235803577738502,0.00003220298094674945,-0.003279097843915224,0.0021697941701859236,-0.0001746974109361569,-0.0018122675998408038,-0.0003440040066683044,0.0053299524006433785,-0.010936951342349252,-0.0016267554213603337,-0.0015238734000983338,-0.0025421936843486037,0.003794672666117549,0.015613563746834794,0.014220568002201617,-0.0013974492903798819,0.003646882289710144,0.0027244147786404938,0.00853247040261825,0.018381715131302673,0.014662018006977936,0.007004411813492577,-0.007686514135760565,-0.0038474354660138488,-0.007542017536858718,-0.01030475681181997,-0.010368915856815875,-0.009968553359309832,-0.0034934805468462096,-0.005929386223821591,-0.0012884308428814013,-0.004224402364343405,-0.002827114309184253,0.0015067199128679931,-0.005388842623991271,-0.007404320562879245,-0.013648563995957375,-0.0078890203537109,-0.008953325198187182,-0.0060882847174070776,-0.009479095538457235,-0.0020584101245428124,-0.007481660276728992,-0.0005551371335362395,-0.0003540547622833401,-0.00410909306568404,-0.0030590954896373055,0.001790112757589668,-0.0009209486694696049,-0.013893469013661766,-0.007542196079157293,-0.00038654060335829854,-0.0015850020475530375,0.002499349027251204,0.022996287948141497,0.021761814287553232,-0.003610054807116588,-0.0026746795847429894,-0.0013842372378955285,0.00025344574654203217,-0.0002025598441832699,-0.0016778845116884138,-0.003487383364699781,0.000353391922544688,-0.002332619158551097,-0.0015905075706541538,-0.0010145568473186966,0.0011464706622064114,0.0007926422646657253,-0.0007634178812926015,-0.0006820797219309801,-0.00014750997070223093,0.000714586543229719,0.00020170059481946131,0.00004717615471842388,0.00023010766987378398,0.00003980836784952165,-0.0006297796268578774,-0.00023093224687424177,0.00006264825181763929,0.00023217180569190532,-0.0020994675190498433,-0.0018392123359565933,-0.0020658069503648826,0.00020766777258055905,0.0007139591737844361,0.00020006573322461918,0.0013050463542943664,0.00498933857306838,0.004870029302158703,0.005181295447982848,0.0023535670091708503,0.00013901421334594488,-0.0010257478764591117,-0.0012467218718181055,-0.0008424149127677083,0.004177352762781084,0.0018608149451514084,0.00042016099905595183,0.0011824219836853445,0.0006202926451805979,-0.00003512254140029351,-0.003896458074450493,-0.003370224420602123,-0.002946073616233965,-0.0025501084746792912,0.0006465313587492952,0.00029399975513418514,-0.009036448628952106,-0.007372084054319809,-0.010551936167757958,-0.005168628646060824,-0.005732108188870673,-0.00043597172286051017,-0.0014666859060525894,0.0033704627227659025,0.00012233113132727644,-0.0008666606348318359,-0.005577144678682089,-0.004311573303615053,-0.0036039428669027984,-0.0016378486955848832,-0.0012855786674966414,-0.0018060820342119162,-0.0036321199198331064,-0.002090288035105914,-0.0016079230311637123,-0.0025143319508060813,0.006752143303553264,0.00022517970986276245,0.003238059007950748,0.0027688133995980024,-0.0018968828953802586,-0.0028988017002120614,-0.005172077352957179,-0.00755900441436097,-0.00612013740465045,-0.006352085755982746,-0.004095092580731337,-0.0027539683530145944,-0.0030585587761985757,-0.003373362230680262,0.00008510817618419726,-0.0020572876383084804,-0.0021408911367567876,-0.003911378714595533,-0.0030812634140602313,-0.004423416336067021,-0.0036920130563278994,-0.003504303575027734,0.0009137412610774239,-0.001405293548790117,-0.0014186991611495614,-0.0035288671109204492,-0.0038023806118872017,-0.0036322713713161647,-0.004685796913690865,-0.002240384133377423,-0.003532292234012857,-0.0034513106147642247,-0.0033210346494646124,-0.004335455897186573,-0.0008509122126270086,-0.0013880061815143563,-0.0048041221210345,-0.0018522657337598503,-0.0021248882791648307,-0.004428466587948303,-0.002546369379463916,-0.0015312297837226652,-0.003020182795201739,-0.0041388221240292,-0.005794956833900263,-0.010230791153541455,-0.010853000615801042,-0.008759420141965771,-0.0044905692581475405,-0.012564995209686458,-0.013499423939113816,-0.014499888406135142,-0.01251883692263315,-0.006924851089327906,-0.008187610869451115,-0.005376801282788317,-0.004075952126489331,-0.0071852441954736905,-0.008062055644889673,-0.00927203296062847,-0.005530160347310205,0.001187918862948815,0.010261524376498224,0.011695978386948505,0.006531977618578821,0.009324334804356718,0.013466061170523366,0.0022787711495766416,-0.01067517309760054,-0.016008831250170868,-0.004291381609315674,-0.004461387948443492,-0.002302738333431383,-0.0013270814476224284,-0.001911418090458028,-0.003857660112165225,-0.002702413301449269,-0.0007829180782816062,-0.003701732319314033,-0.003760564849168683,-0.0021082614936555424,-0.002071168962478017,-0.009299443375008801,-0.005955193686531857,0.0014543276047334075,0.006676535898198684,0.01341642469924409,0.00608434450502197,0.0023299691189701357,-0.002630375112251689,-0.0001239315994704763,0.0026258331684706113,-0.0002767854991058509,-0.0009751883432424316,-0.0007969814469106495,0.0015087271167431027,0.0009920839220285416,0.0006137868719330678,-0.000641608402171793,-0.003463247630861588,-0.002171921058713148,-0.0028945981418170654,-0.0016869024839252234,-0.0020918009686283767,-0.002056869949835042,-0.00207090430194512,-0.0011169890737316261,-0.000675791564087073,0.0007317056758135246,0.0034611269171970585,0.0041433693283276325,0.004002363673256089,0.002566931211428406,0.0005744490820992117,0.0009496076921398829,0.0024142418211946883,0.0043118387887564795,0.0033745826998104653,0.0039711555776496725,0.002682491613086313,0.004493660274116944,0.0023373535368591547,0.0009131280142658701,0.004393770776611443,0.008704899058405621,-0.005650324475330611,-0.0013479143381118774,-0.004537433114213248,-0.003635061795648653,-0.0046883348065118,-0.006005560707611342,-0.002220388346662124,0.0010134380233163636,-0.0026977308637773,-0.0018586547230370343,-0.0013633549921602632,0.0014603430172428489,0.0035296492278575897,0.007225586896917473,0.006888675581042965,0.007781070433945085,0.0011243863991694525,-0.0010554222390055656,0.0003279363348459204,0.0008836949430891158,0.0021675189588374146,0.0012053591975321372,0.002140192751539871,0.00010017002932727337,0.0005519582203608783,0.0009954601161249836,0.0007289990389836021,0.000594393346545985,0.0001041207397065591,0.0170954226438577,0.008123880465670178,-0.002456850403783998,-0.01088558203385522,-0.0011471543281610745,-0.011215637437999249,-0.0036583743058145046,0.0010942000274856885,0.008958098750251034,-0.010614828788675368,-0.01679311040788889,-0.011882067968448004,-0.020233951819439728,-0.0020098784007132053,0.008381276124661477,0.012512546207290143,0.004679076373577118,0.0037834771210327744,0.015445911015073458,0.017942384040604036,0.021687142240504425,0.0025787485452989736,0.00980302745786806,0.007148068534055104,0.021877617575228214,0.011450828479913374,0.010425295447930694,0.011805312319969138,0.02158695723240574,0.019075244548730552,0.0146061802127709,0.016780201035241287,0.016778123176967103,0.013920096137250463,0.009049586486071348,0.00728495492755125,0.002673119718868596,-0.0009804350944856803,-0.0009338514064438641,0.0011147452363123496,-0.0051953727573466795,-0.006548288035749768,0.0006893109724236032,0.003912513648780684,-0.0012371107392633955,0.0006020469591021538,0.007709411904215813,0.03242164683373024,-0.0012293207303931315,-0.005931664685097833,-0.012897344888187945,-0.010107206374717256,0.0010234975100805361,-0.0010547057026997209,0.0007757543741414944,-0.004127616867966329,-0.007045212298786889,-0.005201486443790297,0.0007211055684213837,0.005062419959964852,-0.012443978504355377,-0.007847910310374573,-0.00406438570159177,-0.010211081864933172,-0.008299301067988077,-0.01563415303826332,-0.007923675322672352,-0.010535368754062802,-0.00318842123184974,-0.004090996041971569,-0.0036464177925760546,-0.007529478908206026,-0.005981632246403024,-0.006256081610141943,-0.0026031118613900617,-0.005368552636355162,-0.000565168719428281,-0.0006986375665292144,-0.0036180949876628197,-0.0047110528297101455,-0.001135107750693957,-0.003397360909730196,-0.0013853958419834573,-0.00513299504139771,-0.006430495297536254,0.015022131109920641,0.007781807721282045,0.006659325406265755,-0.0001672425278229639,-0.004814618111898501,0.0025330255642378083,0.013238680160914859,0.005082822074958433,0.00026245194021612406,-0.0012723241622249286,0.001057669470659069,0.012533244036603719,0.015038803995897373,0.01910656034791221,0.02729630710867544,0.002738622851514568,-0.014110840080926815,-0.0012031301157549024,-0.009014131966978312,-0.00022375180075565973,-0.010444411096007874,-0.020474562343830865,0.014281875143448511,0.010523402908196052,0.004595172513897221,-0.00011113348106543224,-0.017283313286801178,0.006220993256041159,0.0018657111019516985,-0.037422842967013516,0.003182970807150317,-0.01972394345890886,-0.00044066815947492916,0.00009191989859876533,0.004101530791861781,-0.0026916214264929295,0.00006726031036426623,-0.0010768621771906812,0.00014227995416149497,-0.00021760071346458668,-0.004515204190587004,-0.020272251218557358,-0.014239113933096329,0.0016003473817060392,0.017052242377152044,0.015347329123566547,0.009972582571208477,0.013776406539060796,0.021047558596668143,0.014864020030169437,0.010115591576322913,0.008769577330288788,0.0029989245425288877,0.0001911565947618025,0.00640748886022872,0.010038545083565017,0.0036388536585339657,0.012546899418036142,-0.0012247749837115407,0.001307457695171858,-0.0011278154658308874,0.005217490640158455,-0.0030612251721322536,0.001675121292161445,-0.0011449419495572026,-0.0008926120159837107,0.0011627480465297897,-0.0026725057881170264,-0.0017120462725870311,-0.004696995970637848,-0.004115770988088722,-0.006175391764069597,-0.005025617118614416,-0.007136163651011884,-0.0055316466217239695,0.0029758140250730016,-0.0022230478061828762,-0.0033991438685916364,0.006210275227203965,-0.0017231820966117084,-0.005038026991921167,-0.0038285646587610245,-0.005163818209742506,0.006898211315274239,0.0036346071089307466,0.0036588386089230576,0.010242330531279245,0.013441558578051627,0.011256900732405484,0.01090011396445334,-0.00238822820635202,-0.009985417981321612,-0.008202423031131426,-0.00032653876890738803,-0.009970808401703835,-0.005590657451345275,-0.0012108594722424944,0.001806273590773344,-0.01501188287511468,0.010851166521509489,-0.0002720312913879752,-0.004721799322093527,-0.0024851836690989635,-0.01114354922901839,-0.012491680914536119,-0.020912309604076047,-0.007932838595782718,0.0013277876132633537,-0.0015996689059344742,-0.00146467403586333,-0.0005268456346432989,-0.0012130069856842358,-0.00009155711692680295,-0.00023119870214335,-0.00037115006368064013,0.03634439230275651,0.040591023939972125,0.023138788528740406,0.009801435517147183,0.015919006274392206,-0.01782854941363136,0.09296566558380921,0.020421454993387062,0.040932897167901196,0.02621076659609874,0.048883904392520584,0.08713022681574027,0.07459709490649402,0.03211595810716972,0.008233498937139908,-0.015474502307673296,-0.04526000618352555,-0.08075045887380838,0.00454885745421052,-0.07576079123343031,-0.054025132209062576,-0.03389120319237312,-0.010294524331887564,-0.09354779248436292,0.00459580138946573,-0.034987068424622216,-0.07529008497173588,-0.06214776666214069,-0.030062343770017225,-0.06957108248025179,-0.07040229765698314,-0.03424331937761357,-0.022767157681907218,-0.088643065886572,-0.023161243967479095,-0.05900274546972165,-0.011569643237938484,-0.02339358173776418,0.00512037022660176,-0.021960186461607616,-0.013522657876213392,-0.017098973427588742,0.013629459232712785,-0.03942353713015715,-0.013337707030586898,0.008616688195616007,-0.1105147935450077,0.001887249993160367,0.009633485693484545,0.006780686555430293,0.008502094618355235,-0.022393585182726383,-0.020303061309581,-0.011237358674407005,0.0026224805042147636,-0.011253372222806016,-0.001967035544415315,0.00927335008357962,0.002466833684593439,0.014907956200962266,0.07389519348119696,0.031959477035949625,-0.018870465457439423,-0.0159036535769701,-0.05298935777197281,0.017472447905068595,-0.06284243427217007,-0.0001889861305244267,-0.000700449978467077,-0.010884066849636534,0.004099136703492452,-0.0034664603881537914,0.003419888516267141,0.00901293750697126,0.008562249519551793,0.007717468698198597,0.006656151614151895,-0.0011203376343473792,-0.005738542997278273,0.0012148510625896354,-0.0015464835353971769,0.0016142092645168304,-0.0005505351291503757,0.005856519254545371,-0.009310139265532294,0.005450812983326614,0.001656019672130545,0.0019168017897754908,-0.0004830380300215135,0.0035188604767123857,0.003464061359409243,0.00037590057278672856,-0.005234790888304512,0.0010689313445861142,0.007849334195877114,-0.011767311526151994,-0.01715037878602743,-0.016821802147508908,0.011050959505761663,0.0048083082074299455,-0.01182023905372868,-0.003310911046961943,0.00047788023948669434,0.009852476466524726,-0.01076744954722623,0.009357841530193886,0.017260598950088024,0.08629863585035007,0.04569552435229222,0.0708029456436634,0.2362241099278132,-0.003416729625314474,0.15751982806250453,-0.13016454425329962,-0.30194220691919327,-0.08510118474562962,-0.12113464313248794,-0.006688894238322973,0.0024638866695264974,-0.008414932798283795,-0.022899847322454054,-0.02033492169963817,0.003886208403855562,-0.009475574595853686,0.0025478207195798555,0.007704956301798423,0.09073901114364465,0.053695725121845804,0.013198851491324604,0.014193672221153975,-0.018981957187255222,-0.030216154487182696,-0.0017606266774237156,-0.02715932244124512,-0.013375314263006052,-0.027728607490037877,-0.005474922092010577,-0.038361122055600085,-0.009432808651278416,-0.013226813171058893,0.0023424201644957066,-0.015725751252224047,-0.023031783911089104,-0.00890754023566842,0.015223146416246891,-0.029361652520795662,-0.027106009268512327,-0.019386300817131996,0.0101686348595346,-0.010863137741883596,-0.015821400408943493,-0.04769252450205386,-0.03336051463459929,-0.05446785781532526,-0.05304182693362236,-0.05982475262135267,-0.03911485654922823,-0.04377751192077994,-0.002707428919772307,-0.03406153727943698,-0.011392506149907907,-0.04100859879205624,-0.009190317903024455,-0.01263349448951582,-0.000659589694502453,-0.017649738389688235,0.044494421842197575,-0.003545961497972409,0.009803500433918089,-0.008078565277780095,-0.035528639409070216,0.0009451679264505705,-0.01845404567817847,-0.005097116421287258,0.008267887441130975,-0.0060569617586831255,0.0021744298671061793,0.013094619537393251,0.0101353836847314,-0.01065423188265413,0.0036905637631813684,0.015453808785726627,0.013590856920927763,0.01000464690150693,0.02647806874786814,-0.004908497251259784,-0.0013953933181862037,-0.003387030024896376,-0.005155263002961874,0.006335337976148973,-0.011442644172348082,0.006212215560177962,-0.007109902333468199,0.01791217671901298,-0.0010022455050299566,0.0024911980144679546,0.01735120840991537,0.012822047108784318,0.0016115684799539547,0.004378105552556614,-0.008819894903960327,-0.010776761434196184,0.012765265554965785,0.01571625146122339,0.014262125516931215,0.017768377903848886,0.011769623418028155,0.009391203716707727,0.00879117613658309,-0.00012316684296820313,-0.003933511402768393,0.00018288859670671323,0.0016383795167106048,-0.001941886458856364,-0.0008149615799387296,0.011496326071210206,0.0010787885403260589,0.004342830119033654,0.0025180329588086656,0.00012146716471761465,0.0005528211283187071,0.006079578461746375,-0.004450489320637037,-0.009875925335412225,-0.005245434275517861,-0.0006495475924263397,-0.011197837302461267,-0.01595793462668856,-0.012180979829281569,-0.013599221284190813,0.0009472067467868328,-0.005413596518337727,-0.008327786500255266,0.0036818386676410833,-0.006461086372534434,-0.005894126370549202,-0.004030462742472689,0.00394403887912631,0.00011690874816849828,0.003778440256913503,0.002165286374899248,-0.00017036707140505314,-0.002511254589383801,0.0002378623466938734,-0.011527328907201687,0.013519052571306625,-0.02460540396471818,0.0057515454245731235,-0.008188640039103726,-0.0219715212782224,-0.008479804266244173,0.017809287295676768,-0.0027456840810676417,0.005958158755674958,-0.0011392068893959124,0.0029188538125405708,0.014046998927369714,0.0005383673124015331,0.012352380125472942,0.0057908530967930956,0.0263200622672836,0.10203647454424451,0.039778804406523705,0.04450150202804556,0.030325395986437798,0.019737657780448597,0.025653453776612878,-0.016386455856263638,-0.012539470180248221,0.018665578216314316,0.01270910287469936,0.007482497642437617,0.019675880204886198,-0.0035912972913744548,0.006445139956971009,-0.004054718650877476,0.011429628628926972,0.0044692568480968475,0.0022999549206967154,0.003166079283497917,0.02323112233231465,-0.008605424159516891,-0.0015428004165490468,0.05505667222314514,0.0459529043485721,-0.0018228424402574699,-0.009119569362762073,-0.005389665680316587,-0.0039019546238705516,-0.004859416784408192,-0.01642423029989004,-0.01135404008285453,-0.009948737026813129,-0.006803595654976864,-0.006891453405842185,-0.01870162133127451,-0.015726459057380755,-0.007322635462818046,-0.003941790511210759,-0.004829532331010948,0.004699342728902896,-0.0046381625191619,0.002288332131380836,0.011189875580991307,0.0013323592938832007,-0.009039441123604774,-0.0023443112149834633,-0.18091770339136323,-0.11335199947158496,-0.11649440197894971,-0.043302477026979126,-0.021684871908898156,0.03080362764497598,0.11792198071877162,0.06600478446731965,0.033693889155983925,0.03512265634102126,0.01665495956937472,0.005238008219748735,0.040401723235845566,0.024288517733414967,0.10476282658055425,0.11486102516452472,0.014868555629315475,0.030313929232458275,0.03149696936209997,0.015869093127548695,0.002901091861228148,0.012772748013958335,0.004787757051720594,0.0012686972816785176,-0.008628462945731977,-0.0028109015159619353,-0.008367929573675307,-0.0029621002807592354,0.007311639395387222,-0.0029127314725580313,-0.0039094105595722795,0.0048083938503017025,0.004891097021754831,-0.002081876310209433,-0.031712981251378856,-0.026162867638049647,0.014258739228049913,-0.013224745945384106,-0.027035050404568512,-0.007253225582341353,-0.028574949285636347,-0.01882524136453867,-0.013590353618686398,-0.0025569231305174376,-0.008890243635202447,0.005111833103001118,0.007591500723113616,0.0038582331811388335,0.00253318816733857,-0.005582181736826897,-0.003745635971426964,-0.0017114072882880766,0.00514722587831784,-0.0027679960330715403,0.008302388940743791,-0.0011356886243447661,0.0012082015552247565,-0.007894341096592447,-0.010069327506547173,-0.01825367030445098,-0.015263393715334436,-0.007412656443193555,0.001308818891023596,0.004922506244232257,0.00792464412127932,0.0038783931134579084,0.0002509399103776862,-0.004986405450229843,0.019278214468310278,0.020599790518948186,0.04702270585888376,0.0820396759857734,0.041356061274806656,0.004301149747334421,-0.01730915065854788,-0.03030682502624889,-0.038096468430012465,-0.03709732989470164,-0.03386200234914819,-0.013675874720017115,-0.0022709620340416827,-0.0033432831502674767,0.008965067255000273,-0.001905877764026324,-0.01206669801225265,0.0022827873666149876,-0.002502031585512062,-0.0012868025708788384,0.00025460918065315735,-0.00011305790394544601,-0.0008983063162304461,0.0002965608873637393,0.0012451352765007566,-0.0030738602217752486,-0.002194446191424504,0.0003619493800215423,0.0010747245323727839,0.0034376504093719027,0.00476817258944114,0.004180239518367064,0.006227979475321869,0.0017615694475049775,0.0012478692612300317,-0.0010356046259403229,-0.005674254546950881,-0.0008254030399257317,0.00032699056221948314,-0.00547545647714287,-0.001793513773009181,-0.012688483790649721,-0.004307682140885542,-0.014111393364146352,-0.009107417427003384,-0.017211942623058956,-0.007512010080972686,-0.007177916627066831,-0.005201828137311774,-0.0012077944993507117,-0.0002960741985589266,-0.0011861769792934258,-0.004920426678533356,0.0002667595787594716,-0.004897072785145913,-0.005349105844895045,-0.005772861385291132,-0.005990673234919086,-0.006252344561895977,-0.005590518703684211,-0.0011737570942689974,-0.0019271749661129434,-0.00012170325377762008,-0.0002700621262192726,-0.0024596885098920516,-0.003784428001381457,-0.007110609740872557,-0.005516647448530421,-0.005411718778001766,-0.00516079185763374,-0.004099035926628858,0.03935019594306747,0.009751240412394205,0.00038460745903042454,0.005201562773436308,-0.00048541064218928415,-0.00004119495861232281,-0.008649980649352074,0.0019418282608967274,-0.001213474393201371,-0.002007959374168422,-0.0044877475035415655,-0.008992393074246744,-0.004261121270246804,-0.006977159762755036,-0.000752629290218465,-0.00012647793240224323,0.0003499620264240851,0.0014686453893470268,0.0012402913416735828,0.002148700848920271,-0.00047829644002680044,0.04417097382247448,0.048894061551739774,0.05319133781207105,0.031157982690880697,0.050538544077426195,0.04211130210508903,0.014413639282186827,0.007595458533614874,0.021282806410454214,0.03286183842768272,0.04328317567706108,0.03027365585633864,0.025477059204907466,0.02306493263070782,0.01791453454643488,0.020434638931571197,0.011145568957241872,0.012351047926737616,0.01846415619365871,0.0038394154204676547,-0.006572448182851076,-0.02068666508421302,-0.0022984731282728412,0.012793853258093199,0.03560043110822638,0.03128131199628115,0.020347611357768376,-0.0007659820839762688,-0.0007659562203722695,0.04288982165356477,0.04233922312657038,0.03034012345597148,0.041783296347906194,-0.0036717820912599564,-0.005570378930618365,-0.01672816144612928,-0.01363461569417268,0.002741773845627904,0.02411718014627695,0.052926575765013695,0.045334606625450156,0.01952780146772663,-0.06242226522105435,-0.04027586275090774,-0.08651920097569625,-0.06851025049885114,-0.01931674002359311,-0.059076789456109204,-0.02861793835957845,-0.0804760071914643,-0.04478213656693697,-0.03251692062864701,-0.033336918180187546,-0.01781690982170403,-0.00881949874262015,0.006825593222553532,-0.006969040390686132,-0.016738293304418523,0.014107296553750833,-0.0009677831549197435,-0.0035901585943065584,-0.012124709047687551,-0.008265488470594088,-0.002279653213918209,0.0019419219655295212,0.011035189808656773,0.0074382453846434755,0.006955836918981125,0.016690296004526317,0.0006319326736653844,0.0078042180587848025,0.014397018821910024,0.011166743468493223,0.0190533931987981,-0.014467921302032968,-0.004354927766447266,0.0020283040745804706,-0.0029696532680342593,0.0020678285897398987,-0.004870518964404861,-0.003303192575306942,-0.0006106502648132542,-0.0009348487268046787,-0.004925289831589907,-0.005893283213178317,-0.005441460836057861,-0.005453144432976842,-0.005179605897865258,-0.003330751865481337,-0.003980350370208423,-0.005896758715001245,-0.003556974271001915,-0.003940050897654146,-0.004981576542680462,-0.0034932144141445556,-0.003492797453266879,-0.005072873590203623,-0.0007636465015821159,-0.000719203157738472,0.0011796588611711438,0.0011342533398419619,0.001325299177551642,0.00036481426407893497,0.002374321843187014,-0.01835574706395467,0.002339110399285952,0.007394932288055618,0.01151328516183033,-0.022988874310006697,-0.029823836715271074,-0.0076943677850067616,-0.0007366960247357687,0.004947362836295118,0.018044885092725355,0.006064072154307117,0.004841349475706617,-0.0008893432871749004,0.01999301075314482,0.01610296320480605,0.016992744834472735,0.012384675171536704,-0.007700372952967882,-0.0018447244656272233,-0.004901035368675366,-0.0011257775089082618,-0.003980072836081187,0.0034872143005486578,0.003671744460007176,-0.002538581242940078,0.0015504434434963816,-0.00009585055037556837,-0.0015383780943617846,-0.0002912919541510443,0.0019064157677348703,-0.001307421317809106,-0.002460705110327884,-0.0030273726927892617,-0.002199138437087337,-0.003115164853322009,-0.006970567434715728,-0.010272541975912949,-0.005412111582700163,-0.0192141066363547,-0.011451201358189186,-0.011013589178522428,-0.01905618398450315,-0.00699519575573504,-0.030478834679039817,-0.014666986962159475,-0.03106887157385548,-0.007778591476380825,-0.012053935322910547,-0.005764126467208068,-0.020650750026106834,-0.01572356590380271,-0.0010314672254025936,-0.020574116768936317,-0.12840114533901215,-0.09880329668521881,0.0011969759458831202,6.701971869915724e-6,-0.0020286330497280383,-0.0038921483792364597,-0.0052477277543706196,-0.0041958747315220535,-0.0035251874554281435,-0.002968592831166461,-0.0026548272580839694,-0.000959917805933704,-0.00008375354809686542,0.000929321045987308,0.00047280010767281055,0.002256934492227932,0.0016341814771294594,0.0009622375461428115,0.001296268305547225,0.0036381337752876184,0.006230268588600059,0.004036115909305711,0.012516873888671398,-0.005115320673212409,-0.013014642094882825,-0.006384410584966342,0.00449109523712347,0.004707193545376261,0.012457797982885191,0.012016110044593612,0.010400097138093164,0.0018526756515105565,0.009558986406773329,0.006594982308646043,0.008728495876615247,0.007137453299947083,0.013939053906748692,0.0357548613101244,0.01945130752089123,-0.007238096014286081,0.008880392493059238,-0.017344730091281235,0.0005649482967176785,0.005048769370963176,0.0005885629313221822,0.0062005820994575815,0.005323326040524989,0.015698155543456476,0.005992904305458069,0.006837715821651121,-0.017308473664646346,-0.016811762160311144,-0.021140101831406355,-0.02605036459863186,-0.021483949463193614,-0.019827358308248222,-0.023271194115901988,-0.008344990172190592,-0.006909779311778645,-0.0005295678662757078,0.017473989612578105,-0.00977412952731053,-0.021608426235616207,-0.020748862841476996,-0.014291197750329351,-0.0096458027837798,-0.01981748764713605,0.012765423181311538,0.0017870815160373847,0.011445628168682257,0.01290134402612845,0.012434548232704401,0.014018044574186206,0.011534037534147501,-0.021192615230878193,-0.014037171533952156,0.002874297361510495,-0.0023677604040130973,0.012484901972735921,-0.0037999205912152925,0.0008397414349019527,8.175071949760119e-7,0.05155432395016154,-0.02282972913235426,-0.04580446037774285,-0.04049083395026779,0.0015578950600077708,-0.010051351506263018,0.00002387988691528638,0.013400226132944226,-0.0009367102757096291,-0.01715284027159214,-0.027192252998550732,-0.011399590565512577,-0.005503303778823465,-0.0037544541410170496,-0.009474713200082382,-0.001532906685800602,0.00014550345561777553,-0.0023573619594875104,0.004299114555275689,1.980476857473453e-6,0.019318945822305977,0.05315372611706456,0.04797135340049863,0.05556975103293856,0.04559902986511588,0.019917058913658064,-0.04419313935795799,-0.014998833532445133,-0.025524497575437028,-0.03181292396038771,-0.03480628928324828,-0.04027325659990311,-0.04130801217009624,-0.027203870005905628,-0.01941390646000703,-0.005614942599398394,-0.0035436157680427036,-0.001575543016466933,-0.0008430306916125119,0.001484094459253053,0.002596388182913264,0.0017455213237553835,0.008067087018086264,0.013791368653376898,0.03276018534476558,0.028152726435412962,0.024891545103552442,0.015246774458015958,-0.0026826787022097656,-0.006090369696418445,-0.02282631831864516,-0.01453713069592292,-0.02064763940870762,-0.009910158104806518,-0.008202565368264914,-0.010921625992826497,-0.010510334745049477,-0.005347149741282919,-0.009223651412564019,-0.008425684956212839,0.00027564121410250664,-0.011445931314180294,-0.006916243738184373,0.031720707969119154,0.061948510740573205,0.006466039999698599,0.004087577457539737,0.0029849086422473192,-0.0019982374215032905,0.02752001319701473,0.0400291969999671,0.04020359180867672,0.013748045079410076,0.0030116619697461524,-0.01635925332084298,-0.009210710413753986,-0.041351874669392906,-0.03384322812780738,-0.02125800889916718,-0.0029903584509156644,0.0031864045498271785,-0.012994306161999702,0.009707684007783731,0.025161814875900745,-0.00821599771734327,0.02017373312264681,-0.0004449777625268325,0.0004376138870914777,0.003362004063092172,0.0024990327074192464,0.001673247653040259,0.0009789566878074159,0.0006239462842737945,-0.0016418828866638553],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y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z\":[0.6117647,0.6117647,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.54901963,0.54901963,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5686275,0.56078434,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.38039216,0.38039216,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.53333336,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5176471,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.54509807,0.54509807,0.29803923,0.29803923,0.29803923,0.29803923,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.30588236,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.5647059,0.22745098,0.50980395,0.50980395,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.43529412,0.43529412,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5254902,0.49803922,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.4509804,0.4509804,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.56078434,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.45490196,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.49803922,0.49803922,0.5882353,0.5882353,0.5882353,0.5882353,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4392157,0.50980395,0.50980395,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.49019608,0.49019608,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.5137255,0.49019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5019608,0.5019608,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.4745098,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4745098,0.4745098,0.52156866,0.52156866,0.52156866,0.52156866,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4862745,0.5921569,0.5921569,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.54901963,0.5921569,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.49019608,0.49019608,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.40784314,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47843137,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.47058824,0.47058824,0.5019608,0.5019608,0.5019608,0.5019608,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.5568628,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.41568628,0.827451,0.827451,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.29803923,0.37254903,0.37254903,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.34509805,0.7529412,0.5137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.6745098,0.6745098,0.6745098,0.6745098,0.6745098,0.6745098,0.6745098,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.34509805,0.34509805,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.42745098,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.8117647,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.40392157,0.5058824,0.5686275,0.5686275,0.5686275,0.5686275,0.5686275,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.34901962,0.34901962,0.42745098,0.42745098,0.42745098,0.42745098,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.37254903,0.6156863,0.6156863,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.4745098,0.4745098,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.654902,0.5019608,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.56078434,0.63529414,0.63529414,0.63529414,0.63529414,0.63529414,0.63529414,0.63529414,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.42745098,0.3529412,0.3529412,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5921569,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.65882355,0.21176471,0.21176471,0.21176471,0.21176471,0.21176471,0.21176471,0.21176471,0.21176471,0.21176471,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.49803922,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.44313726,0.44313726,0.49803922,0.49803922,0.49803922,0.49803922,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.6313726,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.59607846,0.43529412,0.43529412,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5137255,0.5137255,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.7921569,0.23921569,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.44705883,0.44705883,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.6392157,0.62352943,0.654902,0.654902,0.654902,0.654902,0.654902,0.654902,0.654902,0.654902,0.654902,0.654902,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.54509807,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.49019608,0.49019608,0.43137255,0.43137255,0.43137255,0.43137255,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.42352942,0.58431375,0.34509805,0.34509805,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.24313726,0.24313726,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5803922,0.4745098,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.4,0.4,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.6156863,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.38431373,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6039216,0.6745098,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.6392157,0.6392157,0.5294118,0.5294118,0.5294118,0.5294118,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.2509804,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.40784314],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"Channel\"}},\"zaxis\":{\"title\":{\"text\":\"Signal\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"SHAP Importance\"}},\"colorscale\":[[0.0,\"#000004\"],[0.1111111111111111,\"#1b0c41\"],[0.2222222222222222,\"#4a0c6b\"],[0.3333333333333333,\"#781c6d\"],[0.4444444444444444,\"#a52c60\"],[0.5555555555555556,\"#cf4446\"],[0.6666666666666666,\"#ed6925\"],[0.7777777777777778,\"#fb9b06\"],[0.8888888888888888,\"#f7d13d\"],[1.0,\"#fcffa4\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"4D EMG SHAP Visualization\"}},                        {\"responsive\": true}                    ).then(function(){\\n                            \\nvar gd = document.getElementById(\\'d92029c4-cb00-4fb2-9385-e5556f23c9c4\\');\\nvar x = new MutationObserver(function (mutations, observer) {{\\n        var display = window.getComputedStyle(gd).display;\\n        if (!display || display === \\'none\\') {{\\n            console.log([gd, \\'removed!\\']);\\n            Plotly.purge(gd);\\n            observer.disconnect();\\n        }}\\n}});\\n\\n// Listen for the removal of the full notebook cells\\nvar notebookContainer = gd.closest(\\'#notebook-container\\');\\nif (notebookContainer) {{\\n    x.observe(notebookContainer, {childList: true});\\n}}\\n\\n// Listen for the clearing of the current output cell\\nvar outputEl = gd.closest(\\'.output\\');\\nif (outputEl) {{\\n    x.observe(outputEl, {childList: true});\\n}}\\n\\n                        })                };                            </script>        </div>\\n</body>\\n</html>'}\n",
            "[INFO] Saved fallback HTML plot: 4D_EMG_SHAP_Visualization.html\n",
            "[INFO] Saved 4D SHAP surface plot to: shap_4d_surface.html\n",
            "[SHAP4D] Channel Variance: 0.0000\n",
            "[SHAP4D] Temporal Entropy: 2.2068\n",
            "[SHAP4D] Mutual Info: 0.2434\n",
            "[SHAP4D] PCA Alignment: 0.0000\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "Figure(640x480)\n",
            "[INFO] Saved SHAP heatmap to: shap_temporal_heatmap.png\n",
            "\n",
            "📊 Training baseline model for SHAP comparison...\n",
            "[INFO] Saved SHAP heatmap to: shap_heatmap_baseline.png\n",
            "\n",
            "🔍 Running ablation: shuffling SHAP-important segments...\n",
            "[Ablation] Accuracy post SHAP shuffle: 1.0000\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n",
            "[SHAP Ablation] KL Divergence (Original vs Post-Ablation): 0.0000\n",
            "/content/extddivesify/diversify/train.py:260: MatplotlibDeprecationWarning:\n",
            "\n",
            "The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "\n",
            "Figure(800x500)\n",
            "Figure(1000x500)\n",
            "[SHAP vs Confidence] Pearson Correlation: 0.6904 (p=0.02711)\n",
            "Figure(600x500)\n",
            "\n",
            "🛠 Real-world Context: EMG classification can support gesture-based interfaces in prosthetics or rehabilitation systems, and insights from SHAP improve trust in deployed models.\n",
            "Figure(1200x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./data/ \\\n",
        "  --task cross_people \\\n",
        "  --test_envs 2 \\\n",
        "  --dataset emg \\\n",
        "  --algorithm diversify \\\n",
        "  --latent_domain_num 20 \\\n",
        "  --alpha1 0.5 \\\n",
        "  --alpha 1.0 \\\n",
        "  --lam 0.0 \\\n",
        "  --local_epoch 1 \\\n",
        "  --max_epoch 150 \\\n",
        "  --lr 0.01 \\\n",
        "  --output ./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01 \\\n",
        "  --enable_shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oChTuGPuj3ci",
        "outputId": "c36ca199-e6a3-47c7-f3ec-ef5ddf66bc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:1.0\n",
            "alpha1:0.5\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:emg\n",
            "data_dir:./data/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:20\n",
            "local_epoch:1\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:150\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[2]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01\n",
            "weight_decay:0.0005\n",
            "enable_shap:True\n",
            "resume:None\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'emg': [0]}\n",
            "select_channel:{'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list:{'emg': 1000}\n",
            "act_people:{'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "num_classes:6\n",
            "input_shape:(8, 1, 200)\n",
            "grid_size:10\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5315846205    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4928314686     0.5541893840     0.9386420846    \n",
            "Counter({np.int64(13): 4169})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.8351063132     0.0017557649     0.8368620872     0.4144878868     0.4097888676     0.4790669856     1.7332653999    \n",
            "\n",
            "======== ROUND 1 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2852004766    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.9529386759     0.8680340052     0.0849046484    \n",
            "Counter({np.int64(5): 4169})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "1                0.1863303185     0.0041552689     0.1904855818     0.6893739506     0.6689059501     0.6034688995     1.7365479469    \n",
            "\n",
            "======== ROUND 2 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.1966127306    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3171032667     1.1789963245     0.1381068975    \n",
            "Counter({np.int64(1): 4169})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "2                0.4136161506     0.0078356080     0.4214517474     0.6711441593     0.6573896353     0.6297846890     2.0951464176    \n",
            "\n",
            "======== ROUND 3 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6332643032    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.6661176682     0.4760117531     0.1901058853    \n",
            "Counter({np.int64(5): 1570, np.int64(3): 879, np.int64(1): 624, np.int64(0): 208, np.int64(9): 168, np.int64(12): 107, np.int64(2): 105, np.int64(11): 92, np.int64(13): 52, np.int64(15): 50, np.int64(6): 48, np.int64(17): 45, np.int64(10): 36, np.int64(8): 36, np.int64(18): 34, np.int64(7): 33, np.int64(14): 33, np.int64(19): 22, np.int64(4): 16, np.int64(16): 11})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "3                1.8479254246     1.1363140345     2.9842395782     0.4024946030     0.3742802303     0.3247607656     1.7363550663    \n",
            "\n",
            "======== ROUND 4 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                4.0196094513    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0155348778     1.4420386553     1.5734963417    \n",
            "Counter({np.int64(5): 971, np.int64(1): 878, np.int64(3): 476, np.int64(13): 279, np.int64(16): 250, np.int64(0): 224, np.int64(15): 204, np.int64(8): 147, np.int64(14): 137, np.int64(12): 122, np.int64(19): 93, np.int64(11): 79, np.int64(9): 65, np.int64(2): 55, np.int64(18): 55, np.int64(17): 40, np.int64(4): 37, np.int64(7): 24, np.int64(6): 22, np.int64(10): 11})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "4                0.3002581298     1.1018166542     1.4020748138     0.7366274886     0.7111324376     0.6680622010     1.7802634239    \n",
            "\n",
            "======== ROUND 5 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7841879129    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2584340572     0.7498008609     0.5086331367    \n",
            "Counter({np.int64(5): 817, np.int64(1): 546, np.int64(0): 397, np.int64(3): 372, np.int64(14): 272, np.int64(16): 271, np.int64(18): 211, np.int64(11): 175, np.int64(8): 160, np.int64(9): 151, np.int64(13): 124, np.int64(15): 122, np.int64(12): 121, np.int64(2): 101, np.int64(19): 100, np.int64(7): 61, np.int64(17): 50, np.int64(4): 47, np.int64(10): 40, np.int64(6): 31})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "5                0.9376872182     2.7371518612     3.6748390198     0.5471336052     0.5134357006     0.5107655502     1.9765434265    \n",
            "\n",
            "======== ROUND 6 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.1119407415    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5824372768     0.8941684365     1.6882688999    \n",
            "Counter({np.int64(5): 732, np.int64(0): 470, np.int64(18): 392, np.int64(3): 367, np.int64(9): 353, np.int64(14): 312, np.int64(1): 301, np.int64(8): 270, np.int64(16): 191, np.int64(11): 183, np.int64(10): 119, np.int64(2): 100, np.int64(7): 100, np.int64(15): 95, np.int64(19): 43, np.int64(12): 41, np.int64(4): 34, np.int64(6): 26, np.int64(13): 26, np.int64(17): 14})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "6                0.9643924236     2.4577388763     3.4221313000     0.6644279204     0.6372360845     0.5927033493     1.7421977520    \n",
            "\n",
            "======== ROUND 7 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.9834420681    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.7642040253     1.6505746841     3.1136293411    \n",
            "Counter({np.int64(5): 610, np.int64(0): 448, np.int64(1): 321, np.int64(3): 309, np.int64(16): 272, np.int64(8): 241, np.int64(14): 240, np.int64(18): 216, np.int64(2): 207, np.int64(13): 193, np.int64(9): 186, np.int64(15): 162, np.int64(19): 133, np.int64(11): 122, np.int64(7): 119, np.int64(4): 106, np.int64(12): 102, np.int64(6): 63, np.int64(17): 60, np.int64(10): 59})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "7                0.4933128357     2.3879458904     2.8812587261     0.6296473975     0.6410748560     0.6136363636     1.7115342617    \n",
            "\n",
            "======== ROUND 8 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2950119972    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.8256330490     3.2779858112     3.5476472378    \n",
            "Counter({np.int64(5): 694, np.int64(18): 312, np.int64(3): 302, np.int64(16): 263, np.int64(0): 241, np.int64(1): 233, np.int64(14): 228, np.int64(8): 218, np.int64(4): 192, np.int64(13): 181, np.int64(12): 163, np.int64(11): 152, np.int64(6): 149, np.int64(19): 143, np.int64(9): 142, np.int64(15): 139, np.int64(10): 127, np.int64(2): 113, np.int64(17): 102, np.int64(7): 75})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "8                0.3588810563     1.6676180363     2.0264990330     0.7121611897     0.6957773512     0.7230861244     1.7906131744    \n",
            "\n",
            "======== ROUND 9 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1075620651    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0450537205     1.6159894466     0.4290642440    \n",
            "Counter({np.int64(5): 485, np.int64(3): 352, np.int64(18): 320, np.int64(16): 306, np.int64(6): 289, np.int64(1): 226, np.int64(0): 216, np.int64(14): 195, np.int64(11): 192, np.int64(8): 185, np.int64(9): 176, np.int64(12): 172, np.int64(15): 167, np.int64(4): 163, np.int64(10): 149, np.int64(13): 132, np.int64(19): 131, np.int64(2): 126, np.int64(17): 120, np.int64(7): 67})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "9                0.3610825241     1.4900815487     1.8511641026     0.5886303670     0.5700575816     0.4970095694     1.7689049244    \n",
            "\n",
            "======== ROUND 10 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.8710962534    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2208836079     0.9419715405     1.2789120674    \n",
            "Counter({np.int64(0): 765, np.int64(14): 420, np.int64(16): 411, np.int64(8): 324, np.int64(4): 297, np.int64(18): 293, np.int64(3): 260, np.int64(15): 256, np.int64(12): 212, np.int64(19): 141, np.int64(9): 138, np.int64(11): 136, np.int64(7): 115, np.int64(6): 110, np.int64(13): 67, np.int64(5): 66, np.int64(2): 59, np.int64(10): 49, np.int64(17): 31, np.int64(1): 19})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "10               0.1898493022     1.8199869394     2.0098361969     0.7157591749     0.6650671785     0.6501196172     1.8671958447    \n",
            "\n",
            "======== ROUND 11 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4045510292    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9861812592     1.5110249519     0.4751563072    \n",
            "Counter({np.int64(0): 662, np.int64(14): 370, np.int64(3): 365, np.int64(8): 354, np.int64(16): 337, np.int64(4): 328, np.int64(18): 299, np.int64(15): 251, np.int64(12): 227, np.int64(19): 207, np.int64(11): 160, np.int64(9): 126, np.int64(6): 125, np.int64(13): 73, np.int64(7): 71, np.int64(17): 54, np.int64(2): 46, np.int64(10): 46, np.int64(5): 45, np.int64(1): 23})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "11               1.4240961075     3.3939561844     4.8180522919     0.5821539938     0.5892514395     0.5299043062     1.7460811138    \n",
            "\n",
            "======== ROUND 12 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9646331072    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.3919849396     0.7932113409     1.5987737179    \n",
            "Counter({np.int64(16): 786, np.int64(4): 545, np.int64(19): 362, np.int64(3): 345, np.int64(13): 262, np.int64(12): 257, np.int64(15): 252, np.int64(8): 242, np.int64(6): 227, np.int64(11): 165, np.int64(18): 139, np.int64(17): 124, np.int64(14): 123, np.int64(7): 90, np.int64(5): 72, np.int64(1): 67, np.int64(9): 45, np.int64(0): 40, np.int64(10): 16, np.int64(2): 10})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "12               0.6700845361     1.0424207449     1.7125053406     0.7042456225     0.7236084453     0.7123205742     1.7518117428    \n",
            "\n",
            "======== ROUND 13 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6230955124    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.8959918022     1.0222201347     1.8737716675    \n",
            "Counter({np.int64(16): 695, np.int64(3): 394, np.int64(8): 363, np.int64(4): 340, np.int64(18): 253, np.int64(15): 252, np.int64(19): 234, np.int64(6): 216, np.int64(13): 215, np.int64(14): 180, np.int64(12): 177, np.int64(5): 165, np.int64(11): 142, np.int64(17): 130, np.int64(7): 120, np.int64(9): 87, np.int64(10): 66, np.int64(1): 65, np.int64(0): 45, np.int64(2): 30})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "13               1.3087538481     1.7641501427     3.0729041100     0.8153034301     0.7783109405     0.7278708134     2.2127923965    \n",
            "\n",
            "======== ROUND 14 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3465151787    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                5.4014739990     2.2323107719     3.1691632271    \n",
            "Counter({np.int64(16): 846, np.int64(4): 392, np.int64(19): 304, np.int64(14): 281, np.int64(8): 277, np.int64(13): 210, np.int64(3): 198, np.int64(12): 196, np.int64(18): 185, np.int64(15): 178, np.int64(6): 166, np.int64(17): 146, np.int64(9): 143, np.int64(5): 135, np.int64(0): 115, np.int64(11): 105, np.int64(10): 90, np.int64(1): 79, np.int64(7): 74, np.int64(2): 49})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "14               0.8200627565     3.0035612583     3.8236241341     0.7095226673     0.6737044146     0.6435406699     1.7444996834    \n",
            "\n",
            "======== ROUND 15 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5716786385    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.9137620926     1.6033495665     3.3104124069    \n",
            "Counter({np.int64(16): 548, np.int64(8): 502, np.int64(14): 360, np.int64(19): 316, np.int64(4): 305, np.int64(18): 302, np.int64(3): 291, np.int64(15): 275, np.int64(9): 219, np.int64(12): 156, np.int64(6): 154, np.int64(7): 147, np.int64(11): 104, np.int64(13): 102, np.int64(5): 100, np.int64(10): 87, np.int64(17): 77, np.int64(0): 62, np.int64(1): 35, np.int64(2): 27})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "15               1.5736038685     2.6814579964     4.2550621033     0.6958503238     0.6794625720     0.6411483254     1.8278777599    \n",
            "\n",
            "======== ROUND 16 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9394327998    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.8147377968     1.1737638712     1.6409740448    \n",
            "Counter({np.int64(16): 525, np.int64(8): 340, np.int64(19): 337, np.int64(4): 312, np.int64(3): 281, np.int64(14): 279, np.int64(15): 255, np.int64(18): 217, np.int64(7): 214, np.int64(6): 201, np.int64(11): 176, np.int64(9): 176, np.int64(12): 150, np.int64(10): 132, np.int64(5): 130, np.int64(13): 123, np.int64(0): 109, np.int64(17): 95, np.int64(1): 61, np.int64(2): 56})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "16               1.4106971025     2.5773572922     3.9880542755     0.5953466059     0.5806142035     0.5448564593     1.8590464592    \n",
            "\n",
            "======== ROUND 17 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4240696430    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.5185291767     1.0433900356     2.4751391411    \n",
            "Counter({np.int64(16): 519, np.int64(5): 330, np.int64(13): 251, np.int64(17): 250, np.int64(10): 242, np.int64(11): 239, np.int64(12): 236, np.int64(6): 223, np.int64(19): 211, np.int64(3): 208, np.int64(18): 164, np.int64(4): 159, np.int64(2): 158, np.int64(0): 158, np.int64(15): 152, np.int64(7): 152, np.int64(1): 141, np.int64(9): 131, np.int64(8): 126, np.int64(14): 119})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "17               0.5075166225     2.1424782276     2.6499948502     0.8133845047     0.7802303263     0.7332535885     1.7604408264    \n",
            "\n",
            "======== ROUND 18 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5782163143    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.4504027367     1.3524394035     1.0979634523    \n",
            "Counter({np.int64(16): 749, np.int64(1): 358, np.int64(17): 348, np.int64(5): 249, np.int64(13): 245, np.int64(12): 225, np.int64(19): 212, np.int64(6): 206, np.int64(11): 193, np.int64(0): 188, np.int64(2): 186, np.int64(10): 186, np.int64(3): 133, np.int64(4): 131, np.int64(15): 129, np.int64(7): 112, np.int64(18): 97, np.int64(9): 79, np.int64(8): 76, np.int64(14): 67})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "18               1.3142595291     3.3753657341     4.6896252632     0.6785799952     0.6823416507     0.6895933014     1.7551348209    \n",
            "\n",
            "======== ROUND 19 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.6305849552    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                5.8331222534     1.9863878489     3.8467342854    \n",
            "Counter({np.int64(16): 641, np.int64(12): 273, np.int64(19): 258, np.int64(15): 233, np.int64(0): 230, np.int64(3): 225, np.int64(6): 215, np.int64(5): 211, np.int64(1): 210, np.int64(17): 210, np.int64(11): 191, np.int64(7): 175, np.int64(13): 171, np.int64(4): 170, np.int64(8): 142, np.int64(2): 136, np.int64(18): 134, np.int64(14): 131, np.int64(9): 114, np.int64(10): 99})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "19               0.9695092440     2.2774233818     3.2469325066     0.8181818182     0.7811900192     0.7840909091     1.7574374676    \n",
            "\n",
            "======== ROUND 20 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4436206818    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3899577856     0.6912994981     0.6986582875    \n",
            "Counter({np.int64(16): 380, np.int64(0): 334, np.int64(12): 261, np.int64(5): 258, np.int64(6): 254, np.int64(1): 221, np.int64(8): 214, np.int64(11): 208, np.int64(19): 203, np.int64(4): 202, np.int64(3): 196, np.int64(17): 189, np.int64(14): 189, np.int64(15): 184, np.int64(7): 164, np.int64(2): 155, np.int64(18): 151, np.int64(10): 138, np.int64(13): 137, np.int64(9): 131})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "20               0.7125928402     1.8075883389     2.5201811790     0.6622691293     0.6170825336     0.5980861244     1.7781267166    \n",
            "\n",
            "======== ROUND 21 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9440699816    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1664137840     1.0918387175     1.0745750666    \n",
            "Counter({np.int64(0): 605, np.int64(9): 366, np.int64(18): 327, np.int64(8): 295, np.int64(3): 272, np.int64(14): 270, np.int64(11): 217, np.int64(4): 197, np.int64(6): 192, np.int64(7): 185, np.int64(15): 184, np.int64(12): 176, np.int64(5): 169, np.int64(19): 148, np.int64(10): 134, np.int64(17): 108, np.int64(1): 98, np.int64(16): 95, np.int64(13): 70, np.int64(2): 61})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "21               0.7366628051     1.8841471672     2.6208100319     0.6298872631     0.6190019194     0.5604066986     2.1510574818    \n",
            "\n",
            "======== ROUND 22 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                4.5285434723    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8625236750     0.8438100815     1.0187135935    \n",
            "Counter({np.int64(0): 547, np.int64(18): 326, np.int64(8): 323, np.int64(9): 321, np.int64(11): 271, np.int64(15): 269, np.int64(3): 255, np.int64(4): 221, np.int64(7): 218, np.int64(12): 217, np.int64(6): 195, np.int64(14): 175, np.int64(5): 171, np.int64(10): 152, np.int64(19): 150, np.int64(1): 96, np.int64(17): 84, np.int64(13): 78, np.int64(16): 57, np.int64(2): 43})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "22               0.2558929324     2.1567370892     2.4126300812     0.7675701607     0.7197696737     0.6860047847     1.7427771091    \n",
            "\n",
            "======== ROUND 23 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9273934960    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.5866980553     1.6162078381     1.9704903364    \n",
            "Counter({np.int64(0): 940, np.int64(9): 439, np.int64(18): 318, np.int64(11): 263, np.int64(3): 235, np.int64(5): 227, np.int64(6): 212, np.int64(7): 186, np.int64(10): 185, np.int64(8): 177, np.int64(14): 169, np.int64(15): 146, np.int64(12): 145, np.int64(16): 141, np.int64(2): 99, np.int64(4): 97, np.int64(17): 79, np.int64(19): 54, np.int64(1): 38, np.int64(13): 19})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "23               0.8103873134     1.6067702770     2.4171576500     0.8488846246     0.8166986564     0.7488038278     1.7395155430    \n",
            "\n",
            "======== ROUND 24 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9775456190    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.9893352985     1.1681222916     2.8212130070    \n",
            "Counter({np.int64(0): 737, np.int64(9): 313, np.int64(18): 304, np.int64(3): 275, np.int64(7): 263, np.int64(11): 232, np.int64(15): 224, np.int64(16): 220, np.int64(8): 219, np.int64(6): 195, np.int64(12): 187, np.int64(5): 151, np.int64(10): 129, np.int64(19): 122, np.int64(1): 117, np.int64(14): 116, np.int64(17): 112, np.int64(2): 95, np.int64(4): 86, np.int64(13): 72})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "24               1.2183494568     1.7944967747     3.0128462315     0.7903573999     0.7322456814     0.7021531100     1.8302717209    \n",
            "\n",
            "======== ROUND 25 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.1663017273    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7790830135     0.7812633514     0.9978196025    \n",
            "Counter({np.int64(0): 809, np.int64(9): 366, np.int64(18): 330, np.int64(3): 264, np.int64(8): 207, np.int64(14): 204, np.int64(10): 199, np.int64(11): 198, np.int64(15): 195, np.int64(7): 176, np.int64(16): 174, np.int64(6): 138, np.int64(12): 134, np.int64(5): 133, np.int64(4): 125, np.int64(2): 121, np.int64(17): 110, np.int64(1): 104, np.int64(13): 92, np.int64(19): 90})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "25               0.3645083904     1.3559565544     1.7204649448     0.7970736388     0.7562380038     0.7159090909     1.7399525642    \n",
            "\n",
            "======== ROUND 26 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8770737052    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1963751316     0.6925918460     0.5037833452    \n",
            "Counter({np.int64(0): 510, np.int64(8): 373, np.int64(15): 367, np.int64(3): 325, np.int64(4): 265, np.int64(7): 264, np.int64(18): 248, np.int64(14): 245, np.int64(9): 229, np.int64(11): 206, np.int64(19): 192, np.int64(12): 183, np.int64(10): 135, np.int64(6): 130, np.int64(1): 108, np.int64(17): 87, np.int64(5): 86, np.int64(13): 82, np.int64(16): 69, np.int64(2): 65})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "26               0.9124085307     1.5948026180     2.5072112083     0.7975533701     0.7581573896     0.7296650718     1.7619211674    \n",
            "\n",
            "======== ROUND 27 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6982033253    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.7689514160     1.1472795010     1.6216720343    \n",
            "Counter({np.int64(0): 518, np.int64(15): 402, np.int64(8): 356, np.int64(3): 336, np.int64(14): 292, np.int64(7): 279, np.int64(4): 272, np.int64(18): 267, np.int64(11): 238, np.int64(9): 204, np.int64(19): 169, np.int64(6): 141, np.int64(10): 133, np.int64(12): 131, np.int64(1): 103, np.int64(13): 98, np.int64(17): 86, np.int64(5): 71, np.int64(16): 39, np.int64(2): 34})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "27               0.8927757740     1.3043315411     2.1971073151     0.7702086831     0.7120921305     0.6830143541     1.7463729382    \n",
            "\n",
            "======== ROUND 28 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.2642283440    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.2420096397     1.0419914722     2.2000181675    \n",
            "Counter({np.int64(0): 822, np.int64(9): 368, np.int64(3): 347, np.int64(18): 343, np.int64(15): 317, np.int64(11): 278, np.int64(8): 276, np.int64(7): 259, np.int64(14): 165, np.int64(10): 158, np.int64(6): 142, np.int64(4): 131, np.int64(16): 120, np.int64(12): 96, np.int64(1): 81, np.int64(5): 71, np.int64(19): 60, np.int64(2): 54, np.int64(17): 48, np.int64(13): 33})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "28               0.5782303810     2.2080383301     2.7862687111     0.7781242504     0.7466410749     0.6979665072     1.7374494076    \n",
            "\n",
            "======== ROUND 29 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.3014006615    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.1886799335     0.9812145233     2.2074654102    \n",
            "Counter({np.int64(0): 931, np.int64(14): 312, np.int64(9): 258, np.int64(10): 216, np.int64(3): 214, np.int64(6): 203, np.int64(11): 193, np.int64(18): 190, np.int64(8): 186, np.int64(4): 185, np.int64(16): 172, np.int64(7): 163, np.int64(1): 156, np.int64(15): 140, np.int64(13): 116, np.int64(2): 115, np.int64(5): 114, np.int64(12): 110, np.int64(17): 104, np.int64(19): 91})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "29               0.5510154366     1.5847983360     2.1358137131     0.8253777884     0.7773512476     0.7248803828     2.1870324612    \n",
            "\n",
            "======== ROUND 30 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.4652681351    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8660418987     0.7440678477     1.1219741106    \n",
            "Counter({np.int64(0): 571, np.int64(4): 333, np.int64(8): 323, np.int64(15): 298, np.int64(14): 281, np.int64(7): 278, np.int64(3): 271, np.int64(19): 236, np.int64(11): 222, np.int64(6): 208, np.int64(12): 185, np.int64(9): 150, np.int64(13): 138, np.int64(10): 136, np.int64(18): 128, np.int64(1): 102, np.int64(17): 98, np.int64(5): 92, np.int64(2): 64, np.int64(16): 55})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "30               0.6543250680     1.8727461100     2.5270712376     0.6807387863     0.6497120921     0.5801435407     1.7565662861    \n",
            "\n",
            "======== ROUND 31 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.0281445980    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.8371670246     1.6748410463     1.1623259783    \n",
            "Counter({np.int64(0): 517, np.int64(15): 342, np.int64(19): 331, np.int64(4): 287, np.int64(7): 281, np.int64(6): 264, np.int64(11): 260, np.int64(12): 247, np.int64(3): 211, np.int64(14): 189, np.int64(10): 166, np.int64(8): 164, np.int64(5): 157, np.int64(13): 155, np.int64(17): 141, np.int64(1): 118, np.int64(18): 110, np.int64(2): 95, np.int64(9): 89, np.int64(16): 45})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "31               0.9393746853     2.2568092346     3.1961839199     0.8313744303     0.7927063340     0.7517942584     1.7195737362    \n",
            "\n",
            "======== ROUND 32 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.6054506302    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.7859711647     1.7136024237     5.0723686218    \n",
            "Counter({np.int64(0): 829, np.int64(18): 368, np.int64(10): 305, np.int64(11): 304, np.int64(9): 299, np.int64(16): 241, np.int64(3): 204, np.int64(2): 192, np.int64(6): 159, np.int64(5): 154, np.int64(15): 151, np.int64(17): 148, np.int64(7): 142, np.int64(12): 128, np.int64(8): 126, np.int64(14): 104, np.int64(19): 88, np.int64(1): 85, np.int64(4): 80, np.int64(13): 62})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "32               0.7449034452     2.0375146866     2.7824182510     0.6370832334     0.5719769674     0.5412679426     1.8408091068    \n",
            "\n",
            "======== ROUND 33 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9725508690    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6409256458     1.4184427261     1.2224829197    \n",
            "Counter({np.int64(0): 600, np.int64(10): 238, np.int64(17): 221, np.int64(9): 214, np.int64(11): 213, np.int64(1): 203, np.int64(8): 195, np.int64(2): 193, np.int64(16): 192, np.int64(3): 192, np.int64(19): 191, np.int64(14): 188, np.int64(5): 184, np.int64(18): 178, np.int64(6): 174, np.int64(4): 173, np.int64(12): 170, np.int64(13): 157, np.int64(7): 150, np.int64(15): 143})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "33               0.1638382673     1.7559752464     1.9198135138     0.7685296234     0.7264875240     0.7003588517     1.7747709751    \n",
            "\n",
            "======== ROUND 34 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.0107204914    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.7563328743     1.7116265297     2.0447063446    \n",
            "Counter({np.int64(9): 536, np.int64(18): 352, np.int64(11): 340, np.int64(0): 313, np.int64(10): 286, np.int64(6): 243, np.int64(16): 208, np.int64(5): 190, np.int64(3): 189, np.int64(7): 174, np.int64(17): 171, np.int64(8): 167, np.int64(12): 164, np.int64(15): 150, np.int64(2): 142, np.int64(19): 126, np.int64(14): 116, np.int64(1): 112, np.int64(4): 99, np.int64(13): 91})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "34               0.3317762911     1.5367552042     1.8685314655     0.6699448309     0.6343570058     0.5956937799     1.7270560265    \n",
            "\n",
            "======== ROUND 35 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9610369205    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.7076764107     1.7079330683     0.9997433424    \n",
            "Counter({np.int64(9): 769, np.int64(11): 363, np.int64(18): 353, np.int64(3): 324, np.int64(8): 308, np.int64(15): 271, np.int64(10): 213, np.int64(6): 211, np.int64(7): 200, np.int64(4): 182, np.int64(12): 162, np.int64(5): 147, np.int64(19): 147, np.int64(17): 101, np.int64(13): 91, np.int64(16): 72, np.int64(0): 71, np.int64(14): 67, np.int64(1): 59, np.int64(2): 58})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "35               0.7175218463     1.7035796642     2.4211015701     0.8205804749     0.7677543186     0.6997607656     1.8009588718    \n",
            "\n",
            "======== ROUND 36 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9147682190    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.6644110680     1.4761027098     3.1883084774    \n",
            "Counter({np.int64(9): 694, np.int64(11): 411, np.int64(6): 292, np.int64(10): 286, np.int64(18): 273, np.int64(3): 217, np.int64(5): 211, np.int64(7): 189, np.int64(8): 188, np.int64(15): 180, np.int64(12): 164, np.int64(16): 153, np.int64(17): 153, np.int64(19): 132, np.int64(0): 128, np.int64(2): 121, np.int64(4): 107, np.int64(1): 98, np.int64(13): 91, np.int64(14): 81})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "36               0.3367263079     1.9228339195     2.2595601082     0.8083473255     0.7667946257     0.7332535885     1.7408573627    \n",
            "\n",
            "======== ROUND 37 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3202328682    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.6140291691     1.0223217010     2.5917074680    \n",
            "Counter({np.int64(9): 715, np.int64(11): 429, np.int64(18): 367, np.int64(10): 325, np.int64(6): 264, np.int64(3): 248, np.int64(7): 221, np.int64(16): 202, np.int64(12): 193, np.int64(15): 184, np.int64(5): 177, np.int64(8): 165, np.int64(17): 117, np.int64(0): 110, np.int64(19): 106, np.int64(2): 93, np.int64(4): 92, np.int64(1): 60, np.int64(13): 58, np.int64(14): 43})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "37               0.0863998756     2.1860127449     2.2724125385     0.8071479971     0.7495201536     0.7069377990     2.1843674183    \n",
            "\n",
            "======== ROUND 38 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0269722939    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.6797127724     1.2669174671     3.4127953053    \n",
            "Counter({np.int64(9): 473, np.int64(11): 350, np.int64(10): 297, np.int64(7): 286, np.int64(3): 271, np.int64(6): 246, np.int64(18): 238, np.int64(12): 230, np.int64(15): 229, np.int64(0): 222, np.int64(5): 186, np.int64(19): 166, np.int64(8): 163, np.int64(16): 162, np.int64(17): 144, np.int64(4): 134, np.int64(2): 121, np.int64(13): 100, np.int64(1): 89, np.int64(14): 62})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "38               0.3962926865     1.5414320230     1.9377247095     0.7860398177     0.7360844530     0.6943779904     1.7463388443    \n",
            "\n",
            "======== ROUND 39 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7250181437    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9238619804     0.9909471869     0.9329147339    \n",
            "Counter({np.int64(9): 595, np.int64(11): 344, np.int64(7): 286, np.int64(6): 270, np.int64(3): 254, np.int64(12): 234, np.int64(18): 234, np.int64(10): 221, np.int64(5): 214, np.int64(15): 213, np.int64(0): 189, np.int64(17): 170, np.int64(16): 169, np.int64(19): 167, np.int64(2): 152, np.int64(8): 112, np.int64(13): 108, np.int64(1): 92, np.int64(4): 83, np.int64(14): 62})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "39               1.2693966627     2.4350111485     3.7044076920     0.6373230991     0.5873320537     0.5328947368     1.7386693954    \n",
            "\n",
            "======== ROUND 40 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5809783936    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.1937551498     1.2939559221     2.8997993469    \n",
            "Counter({np.int64(7): 391, np.int64(9): 368, np.int64(19): 325, np.int64(15): 315, np.int64(11): 274, np.int64(0): 266, np.int64(12): 259, np.int64(3): 232, np.int64(6): 216, np.int64(5): 183, np.int64(13): 179, np.int64(10): 176, np.int64(17): 168, np.int64(18): 147, np.int64(8): 134, np.int64(4): 129, np.int64(2): 110, np.int64(1): 110, np.int64(16): 97, np.int64(14): 90})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "40               0.7372105122     1.9370198250     2.6742303371     0.7913168626     0.7639155470     0.7147129187     1.8607475758    \n",
            "\n",
            "======== ROUND 41 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7340556979    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.0673666000     1.1364185810     2.9309480190    \n",
            "Counter({np.int64(16): 600, np.int64(7): 307, np.int64(2): 277, np.int64(11): 270, np.int64(10): 263, np.int64(6): 258, np.int64(17): 251, np.int64(12): 240, np.int64(5): 208, np.int64(19): 204, np.int64(0): 183, np.int64(13): 179, np.int64(18): 177, np.int64(3): 144, np.int64(15): 141, np.int64(1): 138, np.int64(9): 116, np.int64(4): 85, np.int64(14): 65, np.int64(8): 63})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "41               0.5767390132     2.2161228657     2.7928619385     0.7210362197     0.6804222649     0.6668660287     1.7516365051    \n",
            "\n",
            "======== ROUND 42 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.7618670464    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6409621239     0.8773452044     1.7636170387    \n",
            "Counter({np.int64(16): 454, np.int64(0): 283, np.int64(7): 273, np.int64(13): 262, np.int64(19): 259, np.int64(11): 236, np.int64(12): 228, np.int64(17): 200, np.int64(6): 196, np.int64(4): 193, np.int64(14): 191, np.int64(9): 169, np.int64(5): 169, np.int64(10): 167, np.int64(3): 162, np.int64(18): 158, np.int64(15): 153, np.int64(2): 147, np.int64(8): 141, np.int64(1): 128})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "42               0.6671306491     2.0270247459     2.6941554546     0.8687934757     0.8003838772     0.7350478469     1.7452507019    \n",
            "\n",
            "======== ROUND 43 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.2747004032    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.4181566238     1.6124325991     0.8057240248    \n",
            "Counter({np.int64(0): 514, np.int64(13): 417, np.int64(19): 340, np.int64(12): 301, np.int64(1): 290, np.int64(4): 251, np.int64(17): 246, np.int64(5): 210, np.int64(7): 197, np.int64(14): 187, np.int64(6): 184, np.int64(11): 167, np.int64(15): 143, np.int64(2): 143, np.int64(10): 118, np.int64(8): 111, np.int64(3): 103, np.int64(9): 92, np.int64(16): 80, np.int64(18): 75})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "43               0.6055948138     1.9973212481     2.6029160023     0.8496042216     0.8051823417     0.7356459330     1.7544767857    \n",
            "\n",
            "======== ROUND 44 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.4555735588    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.6933799982     0.8482129574     0.8451670408    \n",
            "Counter({np.int64(1): 1052, np.int64(13): 510, np.int64(17): 318, np.int64(12): 287, np.int64(19): 278, np.int64(7): 225, np.int64(5): 203, np.int64(6): 175, np.int64(2): 170, np.int64(4): 166, np.int64(14): 154, np.int64(0): 125, np.int64(11): 115, np.int64(15): 108, np.int64(10): 74, np.int64(3): 59, np.int64(8): 55, np.int64(16): 32, np.int64(18): 32, np.int64(9): 31})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "44               0.7525538802     2.0527875423     2.8053414822     0.8201007436     0.7744721689     0.7188995215     1.7532107830    \n",
            "\n",
            "======== ROUND 45 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1009950638    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2737679482     1.3060468435     0.9677211046    \n",
            "Counter({np.int64(1): 774, np.int64(19): 358, np.int64(13): 330, np.int64(4): 305, np.int64(12): 238, np.int64(7): 227, np.int64(14): 213, np.int64(17): 213, np.int64(15): 187, np.int64(0): 186, np.int64(5): 183, np.int64(6): 178, np.int64(11): 146, np.int64(2): 141, np.int64(8): 135, np.int64(10): 107, np.int64(3): 94, np.int64(9): 60, np.int64(16): 48, np.int64(18): 46})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "45               0.1728558093     1.9604818821     2.1333377361     0.8568001919     0.8272552783     0.7434210526     2.1287236214    \n",
            "\n",
            "======== ROUND 46 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1146173477    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0694546700     0.8452957869     1.2241587639    \n",
            "Counter({np.int64(1): 919, np.int64(13): 344, np.int64(4): 302, np.int64(17): 257, np.int64(12): 245, np.int64(19): 229, np.int64(5): 218, np.int64(7): 188, np.int64(0): 180, np.int64(2): 180, np.int64(14): 179, np.int64(15): 165, np.int64(6): 164, np.int64(11): 119, np.int64(10): 115, np.int64(8): 104, np.int64(3): 86, np.int64(9): 68, np.int64(16): 67, np.int64(18): 40})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "46               0.5978528261     2.2118206024     2.8096733093     0.7987526985     0.7476007678     0.6800239234     1.7437467575    \n",
            "\n",
            "======== ROUND 47 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.8777577877    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0526409149     1.2556983232     0.7969424725    \n",
            "Counter({np.int64(1): 795, np.int64(13): 410, np.int64(4): 388, np.int64(19): 323, np.int64(17): 230, np.int64(7): 225, np.int64(12): 225, np.int64(5): 203, np.int64(15): 182, np.int64(6): 173, np.int64(14): 160, np.int64(2): 143, np.int64(11): 139, np.int64(10): 122, np.int64(0): 110, np.int64(8): 107, np.int64(3): 99, np.int64(18): 47, np.int64(16): 47, np.int64(9): 41})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "47               1.6514728069     2.4874753952     4.1389484406     0.7929959223     0.7428023033     0.6686602871     1.7324020863    \n",
            "\n",
            "======== ROUND 48 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2846173048    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5798988342     1.0437933207     0.5361055732    \n",
            "Counter({np.int64(1): 817, np.int64(13): 323, np.int64(19): 320, np.int64(4): 283, np.int64(5): 226, np.int64(17): 213, np.int64(12): 210, np.int64(7): 209, np.int64(6): 188, np.int64(15): 169, np.int64(2): 168, np.int64(10): 155, np.int64(11): 154, np.int64(14): 137, np.int64(8): 129, np.int64(0): 121, np.int64(3): 112, np.int64(16): 96, np.int64(18): 84, np.int64(9): 55})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "48               0.6692420244     2.0838193893     2.7530612946     0.8870232670     0.8176583493     0.7571770335     2.2187650204    \n",
            "\n",
            "======== ROUND 49 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                5.0656795502    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.9581856728     1.6611200571     1.2970657349    \n",
            "Counter({np.int64(1): 494, np.int64(0): 377, np.int64(5): 237, np.int64(12): 230, np.int64(6): 226, np.int64(17): 219, np.int64(10): 205, np.int64(11): 197, np.int64(2): 195, np.int64(14): 189, np.int64(13): 185, np.int64(18): 184, np.int64(7): 179, np.int64(16): 179, np.int64(19): 175, np.int64(9): 149, np.int64(8): 148, np.int64(3): 143, np.int64(15): 139, np.int64(4): 119})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "49               2.0557017326     1.6234921217     3.6791939735     0.8244183257     0.7744721689     0.7105263158     1.7726643085    \n",
            "\n",
            "======== ROUND 50 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.4369423389    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.7489428520     1.1103670597     1.6385759115    \n",
            "Counter({np.int64(0): 823, np.int64(9): 298, np.int64(16): 246, np.int64(1): 212, np.int64(11): 210, np.int64(14): 202, np.int64(18): 192, np.int64(2): 187, np.int64(5): 184, np.int64(10): 176, np.int64(8): 165, np.int64(7): 161, np.int64(17): 150, np.int64(3): 149, np.int64(13): 148, np.int64(4): 147, np.int64(6): 140, np.int64(12): 137, np.int64(15): 125, np.int64(19): 117})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "50               1.4146749973     2.3853390217     3.8000140190     0.8522427441     0.7946257198     0.7553827751     1.7748286724    \n",
            "\n",
            "======== ROUND 51 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.3733234406    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.9155769348     0.4471381009     2.4684388638    \n",
            "Counter({np.int64(0): 561, np.int64(18): 359, np.int64(11): 336, np.int64(7): 328, np.int64(16): 320, np.int64(3): 246, np.int64(10): 234, np.int64(9): 221, np.int64(6): 206, np.int64(12): 203, np.int64(15): 185, np.int64(8): 147, np.int64(19): 143, np.int64(5): 137, np.int64(17): 135, np.int64(2): 116, np.int64(14): 84, np.int64(4): 71, np.int64(13): 70, np.int64(1): 67})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "51               0.3666353822     1.8119460344     2.1785814762     0.8755097146     0.8071017274     0.7314593301     1.8426778316    \n",
            "\n",
            "======== ROUND 52 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3736838102    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1291189194     0.9300618768     1.1990569830    \n",
            "Counter({np.int64(0): 613, np.int64(1): 354, np.int64(12): 294, np.int64(2): 271, np.int64(17): 257, np.int64(5): 224, np.int64(13): 213, np.int64(19): 199, np.int64(6): 193, np.int64(16): 193, np.int64(7): 191, np.int64(10): 168, np.int64(11): 168, np.int64(14): 158, np.int64(4): 140, np.int64(9): 135, np.int64(15): 118, np.int64(3): 107, np.int64(18): 94, np.int64(8): 79})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "52               0.8866625428     2.9980189800     3.8846814632     0.8308946990     0.7783109405     0.7015550239     1.7466127872    \n",
            "\n",
            "======== ROUND 53 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3912907839    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.0638380051     1.0982887745     2.9655489922    \n",
            "Counter({np.int64(0): 858, np.int64(16): 329, np.int64(10): 253, np.int64(7): 242, np.int64(6): 222, np.int64(12): 217, np.int64(5): 211, np.int64(2): 211, np.int64(11): 204, np.int64(18): 185, np.int64(9): 184, np.int64(17): 178, np.int64(1): 140, np.int64(3): 139, np.int64(19): 126, np.int64(15): 107, np.int64(13): 102, np.int64(8): 94, np.int64(14): 85, np.int64(4): 82})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "53               0.7526074648     1.4041383266     2.1567459106     0.8505636843     0.7629558541     0.7218899522     1.8170223236    \n",
            "\n",
            "======== ROUND 54 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9106996059    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8678208590     0.8348841667     1.0329366922    \n",
            "Counter({np.int64(1): 905, np.int64(13): 385, np.int64(12): 343, np.int64(19): 271, np.int64(17): 257, np.int64(7): 254, np.int64(6): 245, np.int64(2): 233, np.int64(5): 223, np.int64(0): 183, np.int64(4): 157, np.int64(14): 129, np.int64(10): 126, np.int64(11): 115, np.int64(16): 98, np.int64(15): 97, np.int64(9): 44, np.int64(3): 42, np.int64(8): 33, np.int64(18): 29})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "54               0.3311918080     1.4633128643     1.7945046425     0.9018949388     0.8358925144     0.7679425837     1.7629604340    \n",
            "\n",
            "======== ROUND 55 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4925003052    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.8750896454     2.2677471638     4.6073427200    \n",
            "Counter({np.int64(1): 905, np.int64(13): 282, np.int64(0): 246, np.int64(14): 238, np.int64(2): 227, np.int64(4): 222, np.int64(12): 220, np.int64(17): 209, np.int64(5): 207, np.int64(6): 192, np.int64(19): 190, np.int64(7): 179, np.int64(10): 150, np.int64(16): 131, np.int64(11): 127, np.int64(15): 118, np.int64(9): 106, np.int64(3): 82, np.int64(8): 72, np.int64(18): 66})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "55               0.4588087797     1.7762284279     2.2350373268     0.8073878628     0.7802303263     0.6943779904     1.7426600456    \n",
            "\n",
            "======== ROUND 56 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5081942081    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8060557842     1.1364587545     0.6695969701    \n",
            "Counter({np.int64(1): 815, np.int64(12): 271, np.int64(13): 269, np.int64(19): 258, np.int64(6): 244, np.int64(7): 231, np.int64(17): 227, np.int64(2): 211, np.int64(5): 208, np.int64(4): 197, np.int64(14): 184, np.int64(11): 181, np.int64(10): 154, np.int64(0): 146, np.int64(15): 134, np.int64(16): 101, np.int64(3): 97, np.int64(9): 89, np.int64(18): 76, np.int64(8): 76})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "56               0.7382819653     2.7817976475     3.5200796127     0.8059486687     0.7629558541     0.6943779904     2.1845071316    \n",
            "\n",
            "======== ROUND 57 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.7971127033    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6672165394     1.0053057671     1.6619107723    \n",
            "Counter({np.int64(0): 672, np.int64(9): 273, np.int64(16): 239, np.int64(12): 229, np.int64(10): 222, np.int64(19): 210, np.int64(7): 198, np.int64(14): 195, np.int64(8): 192, np.int64(5): 183, np.int64(15): 176, np.int64(6): 172, np.int64(18): 171, np.int64(4): 163, np.int64(11): 163, np.int64(17): 161, np.int64(1): 152, np.int64(2): 143, np.int64(3): 132, np.int64(13): 123})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "57               0.5705724359     2.1412532330     2.7118256092     0.8697529384     0.8128598848     0.7200956938     1.7627756596    \n",
            "\n",
            "======== ROUND 58 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6224939823    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5925841331     1.2206946611     1.3718894720    \n",
            "Counter({np.int64(0): 528, np.int64(19): 259, np.int64(14): 251, np.int64(8): 249, np.int64(9): 238, np.int64(15): 237, np.int64(4): 234, np.int64(7): 229, np.int64(12): 228, np.int64(11): 219, np.int64(3): 178, np.int64(18): 169, np.int64(10): 169, np.int64(6): 169, np.int64(1): 152, np.int64(17): 149, np.int64(13): 147, np.int64(16): 138, np.int64(5): 123, np.int64(2): 103})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "58               0.4154084325     2.8950281143     3.3104364872     0.6075797553     0.5777351248     0.5376794258     1.7714641094    \n",
            "\n",
            "======== ROUND 59 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7434083223    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                5.3352165222     0.8645040393     4.4707126617    \n",
            "Counter({np.int64(0): 601, np.int64(9): 278, np.int64(19): 252, np.int64(12): 244, np.int64(8): 226, np.int64(11): 220, np.int64(1): 220, np.int64(15): 211, np.int64(7): 211, np.int64(18): 184, np.int64(6): 178, np.int64(17): 172, np.int64(10): 170, np.int64(4): 164, np.int64(14): 164, np.int64(16): 153, np.int64(3): 149, np.int64(13): 137, np.int64(5): 128, np.int64(2): 107})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "59               0.4497896731     1.7954025269     2.2451922894     0.6848165028     0.6199616123     0.5221291866     1.8090317249    \n",
            "\n",
            "======== ROUND 60 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.2853360176    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.2136585712     1.6972725391     1.5163860321    \n",
            "Counter({np.int64(0): 525, np.int64(8): 309, np.int64(19): 279, np.int64(12): 269, np.int64(15): 248, np.int64(11): 233, np.int64(9): 227, np.int64(14): 216, np.int64(18): 215, np.int64(3): 212, np.int64(4): 204, np.int64(6): 191, np.int64(7): 184, np.int64(17): 182, np.int64(10): 153, np.int64(5): 132, np.int64(13): 116, np.int64(16): 110, np.int64(1): 103, np.int64(2): 61})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "60               0.1663314700     1.5969709158     1.7633023262     0.8179419525     0.7504798464     0.6985645933     1.7378976345    \n",
            "\n",
            "======== ROUND 61 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                4.8137807846    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.7826938629     1.5177500248     1.2649438381    \n",
            "Counter({np.int64(0): 448, np.int64(19): 302, np.int64(15): 286, np.int64(12): 273, np.int64(11): 233, np.int64(1): 232, np.int64(3): 221, np.int64(7): 213, np.int64(8): 205, np.int64(4): 195, np.int64(6): 194, np.int64(17): 185, np.int64(9): 175, np.int64(14): 174, np.int64(18): 168, np.int64(10): 161, np.int64(13): 143, np.int64(5): 128, np.int64(16): 126, np.int64(2): 107})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "61               1.3167661428     2.1652834415     3.4820494652     0.7121611897     0.6698656430     0.6088516746     1.7744731903    \n",
            "\n",
            "======== ROUND 62 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8367853165    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2091355324     1.2714879513     0.9376475215    \n",
            "Counter({np.int64(1): 666, np.int64(12): 294, np.int64(2): 268, np.int64(0): 262, np.int64(19): 239, np.int64(16): 232, np.int64(7): 231, np.int64(17): 221, np.int64(15): 183, np.int64(5): 182, np.int64(13): 181, np.int64(10): 179, np.int64(11): 168, np.int64(9): 139, np.int64(3): 135, np.int64(6): 133, np.int64(4): 125, np.int64(8): 119, np.int64(18): 119, np.int64(14): 93})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "62               1.0546811819     2.0615751743     3.1162562370     0.7917965939     0.7514395393     0.6955741627     1.7717645168    \n",
            "\n",
            "======== ROUND 63 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1643650532    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9997886419     1.0184035301     0.9813851118    \n",
            "Counter({np.int64(14): 551, np.int64(8): 303, np.int64(15): 292, np.int64(19): 286, np.int64(4): 266, np.int64(7): 266, np.int64(11): 226, np.int64(9): 218, np.int64(3): 213, np.int64(18): 213, np.int64(12): 209, np.int64(0): 173, np.int64(10): 166, np.int64(16): 139, np.int64(6): 126, np.int64(1): 125, np.int64(13): 112, np.int64(5): 103, np.int64(17): 98, np.int64(2): 84})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "63               0.4872346520     2.1951787472     2.6824133396     0.8428879827     0.7831094050     0.7009569378     1.7487969398    \n",
            "\n",
            "======== ROUND 64 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2822914124    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9654539824     0.9152154922     1.0502384901    \n",
            "Counter({np.int64(0): 610, np.int64(19): 302, np.int64(8): 286, np.int64(4): 267, np.int64(15): 264, np.int64(9): 256, np.int64(14): 216, np.int64(7): 216, np.int64(18): 194, np.int64(16): 183, np.int64(10): 173, np.int64(11): 169, np.int64(12): 164, np.int64(3): 153, np.int64(13): 146, np.int64(1): 132, np.int64(5): 120, np.int64(17): 111, np.int64(6): 109, np.int64(2): 98})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "64               0.1300561428     1.5178341866     1.6478903294     0.9052530583     0.8368522073     0.7565789474     2.2005558014    \n",
            "\n",
            "======== ROUND 65 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4973196983    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0452246666     1.0970215797     0.9482030869    \n",
            "Counter({np.int64(9): 778, np.int64(18): 387, np.int64(8): 339, np.int64(15): 261, np.int64(11): 223, np.int64(3): 220, np.int64(19): 217, np.int64(7): 215, np.int64(10): 197, np.int64(16): 185, np.int64(12): 170, np.int64(4): 168, np.int64(14): 125, np.int64(17): 110, np.int64(5): 110, np.int64(6): 108, np.int64(13): 104, np.int64(0): 102, np.int64(1): 83, np.int64(2): 67})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "65               0.7659175396     2.9866862297     3.7526037693     0.8009114896     0.7447216891     0.6722488038     1.7714693546    \n",
            "\n",
            "======== ROUND 66 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.1729441881    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9736242294     1.0342165232     0.9394077659    \n",
            "Counter({np.int64(9): 440, np.int64(15): 395, np.int64(8): 364, np.int64(14): 318, np.int64(4): 306, np.int64(7): 278, np.int64(19): 277, np.int64(3): 269, np.int64(11): 233, np.int64(18): 196, np.int64(12): 191, np.int64(6): 150, np.int64(10): 143, np.int64(5): 125, np.int64(13): 112, np.int64(17): 108, np.int64(16): 97, np.int64(1): 67, np.int64(0): 53, np.int64(2): 47})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "66               0.3545060456     1.7410211563     2.0955271721     0.8716718638     0.8013435701     0.7488038278     1.7965118885    \n",
            "\n",
            "======== ROUND 67 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6658258438    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.5953578949     0.7437666059     2.8515913486    \n",
            "Counter({np.int64(9): 718, np.int64(15): 292, np.int64(7): 275, np.int64(8): 262, np.int64(11): 250, np.int64(3): 228, np.int64(18): 222, np.int64(19): 211, np.int64(10): 187, np.int64(12): 186, np.int64(14): 173, np.int64(16): 168, np.int64(0): 155, np.int64(4): 153, np.int64(17): 136, np.int64(5): 134, np.int64(6): 132, np.int64(13): 104, np.int64(1): 93, np.int64(2): 90})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "67               1.9646326303     2.4671628475     4.4317955971     0.9102902375     0.8205374280     0.7320574163     1.8666672707    \n",
            "\n",
            "======== ROUND 68 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3473811150    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.3695919514     1.4391279221     0.9304640293    \n",
            "Counter({np.int64(0): 635, np.int64(15): 267, np.int64(19): 242, np.int64(7): 224, np.int64(9): 202, np.int64(8): 198, np.int64(17): 198, np.int64(12): 197, np.int64(1): 197, np.int64(11): 191, np.int64(3): 180, np.int64(10): 178, np.int64(4): 178, np.int64(16): 168, np.int64(14): 166, np.int64(5): 161, np.int64(2): 152, np.int64(18): 150, np.int64(13): 149, np.int64(6): 136})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "68               0.8290383816     2.7785067558     3.6075451374     0.6910530103     0.6094049904     0.5340909091     1.7397572994    \n",
            "\n",
            "======== ROUND 69 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0214648247    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1669862270     0.6629631519     1.5040230751    \n",
            "Counter({np.int64(0): 681, np.int64(15): 253, np.int64(18): 242, np.int64(11): 219, np.int64(10): 213, np.int64(9): 208, np.int64(7): 206, np.int64(12): 205, np.int64(8): 203, np.int64(16): 199, np.int64(19): 195, np.int64(14): 174, np.int64(3): 171, np.int64(5): 166, np.int64(17): 164, np.int64(6): 162, np.int64(2): 151, np.int64(4): 150, np.int64(1): 115, np.int64(13): 92})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "69               0.7423626184     2.3496344090     3.0919971466     0.8836651475     0.7936660269     0.7117224880     1.7793381214    \n",
            "\n",
            "======== ROUND 70 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5081458092    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1469979286     1.2146927118     0.9323052168    \n",
            "Counter({np.int64(0): 635, np.int64(15): 338, np.int64(8): 306, np.int64(18): 283, np.int64(9): 258, np.int64(11): 233, np.int64(7): 233, np.int64(3): 227, np.int64(19): 208, np.int64(16): 192, np.int64(10): 185, np.int64(14): 179, np.int64(12): 163, np.int64(4): 142, np.int64(17): 127, np.int64(6): 125, np.int64(5): 119, np.int64(2): 86, np.int64(13): 67, np.int64(1): 63})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "70               0.6331271529     2.6831443310     3.3162715435     0.8807867594     0.7965451056     0.7254784689     1.7341632843    \n",
            "\n",
            "======== ROUND 71 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9669934511    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.0108170509     1.5512700081     4.4595470428    \n",
            "Counter({np.int64(0): 648, np.int64(15): 304, np.int64(19): 236, np.int64(7): 219, np.int64(16): 215, np.int64(12): 214, np.int64(9): 211, np.int64(5): 210, np.int64(8): 187, np.int64(4): 181, np.int64(1): 180, np.int64(6): 176, np.int64(10): 175, np.int64(11): 162, np.int64(2): 161, np.int64(17): 150, np.int64(3): 147, np.int64(18): 147, np.int64(14): 136, np.int64(13): 110})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "71               0.0302946605     1.9289050102     1.9591996670     0.8824658191     0.8147792706     0.7063397129     1.7835767269    \n",
            "\n",
            "======== ROUND 72 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0644823313    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5065519810     1.7342698574     0.7722820640    \n",
            "Counter({np.int64(0): 578, np.int64(15): 345, np.int64(7): 255, np.int64(8): 252, np.int64(14): 229, np.int64(19): 227, np.int64(9): 211, np.int64(12): 209, np.int64(4): 201, np.int64(11): 201, np.int64(3): 183, np.int64(10): 182, np.int64(18): 178, np.int64(6): 159, np.int64(1): 157, np.int64(5): 153, np.int64(16): 153, np.int64(13): 106, np.int64(17): 100, np.int64(2): 90})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "72               0.2353697866     2.4626810551     2.6980507374     0.8412089230     0.7773512476     0.7033492823     2.1598076820    \n",
            "\n",
            "======== ROUND 73 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6817739010    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.3550462723     1.2538256645     1.1012206078    \n",
            "Counter({np.int64(0): 528, np.int64(15): 310, np.int64(7): 271, np.int64(11): 254, np.int64(18): 253, np.int64(12): 235, np.int64(6): 234, np.int64(10): 221, np.int64(3): 220, np.int64(8): 219, np.int64(19): 191, np.int64(16): 182, np.int64(9): 180, np.int64(14): 175, np.int64(4): 157, np.int64(5): 155, np.int64(17): 114, np.int64(1): 106, np.int64(13): 83, np.int64(2): 81})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "73               0.7556376457     3.0925076008     3.8481452465     0.8232189974     0.7543186180     0.6830143541     1.7439675331    \n",
            "\n",
            "======== ROUND 74 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4004949331    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.3171095848     0.8458654284     1.4712442160    \n",
            "Counter({np.int64(14): 571, np.int64(15): 510, np.int64(4): 422, np.int64(19): 310, np.int64(8): 277, np.int64(12): 250, np.int64(7): 246, np.int64(11): 220, np.int64(3): 208, np.int64(6): 190, np.int64(5): 171, np.int64(10): 152, np.int64(13): 145, np.int64(17): 126, np.int64(18): 111, np.int64(9): 88, np.int64(16): 56, np.int64(1): 49, np.int64(2): 37, np.int64(0): 30})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "74               1.1667822599     2.1703920364     3.3371744156     0.8599184457     0.7744721689     0.7033492823     1.8006229401    \n",
            "\n",
            "======== ROUND 75 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0896962881    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5120744705     1.0201829672     1.4918913841    \n",
            "Counter({np.int64(8): 984, np.int64(15): 601, np.int64(3): 446, np.int64(11): 267, np.int64(4): 241, np.int64(18): 221, np.int64(19): 206, np.int64(10): 194, np.int64(7): 192, np.int64(12): 187, np.int64(6): 158, np.int64(5): 123, np.int64(17): 89, np.int64(13): 61, np.int64(9): 57, np.int64(16): 52, np.int64(14): 40, np.int64(1): 23, np.int64(2): 20, np.int64(0): 7})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "75               0.0391311496     2.4188065529     2.4579377174     0.9117294315     0.8071017274     0.7362440191     1.8366539478    \n",
            "\n",
            "======== ROUND 76 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5641046166    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1988742352     1.2295458317     0.9693284631    \n",
            "Counter({np.int64(8): 728, np.int64(15): 328, np.int64(4): 294, np.int64(3): 290, np.int64(11): 236, np.int64(18): 232, np.int64(10): 226, np.int64(12): 223, np.int64(14): 218, np.int64(7): 201, np.int64(19): 190, np.int64(6): 173, np.int64(9): 164, np.int64(5): 144, np.int64(17): 143, np.int64(16): 104, np.int64(13): 100, np.int64(1): 66, np.int64(0): 55, np.int64(2): 54})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "76               0.1128461510     1.8681362867     1.9809824228     0.9278004318     0.8310940499     0.7661483254     1.7741429806    \n",
            "\n",
            "======== ROUND 77 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0834904909    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.6146194935     0.9949316382     2.6196877956    \n",
            "Counter({np.int64(14): 720, np.int64(4): 444, np.int64(15): 406, np.int64(19): 255, np.int64(8): 245, np.int64(3): 211, np.int64(12): 204, np.int64(11): 195, np.int64(13): 181, np.int64(10): 180, np.int64(7): 177, np.int64(9): 135, np.int64(18): 131, np.int64(1): 131, np.int64(6): 130, np.int64(17): 129, np.int64(5): 103, np.int64(16): 94, np.int64(2): 54, np.int64(0): 44})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "77               0.8078157306     2.5676052570     3.3754210472     0.8951786999     0.7946257198     0.7248803828     1.7540786266    \n",
            "\n",
            "======== ROUND 78 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1121411324    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.7870006561     1.7356064320     3.0513939857    \n",
            "Counter({np.int64(14): 937, np.int64(1): 287, np.int64(9): 271, np.int64(12): 231, np.int64(17): 224, np.int64(4): 211, np.int64(13): 199, np.int64(16): 191, np.int64(0): 189, np.int64(5): 169, np.int64(19): 164, np.int64(8): 157, np.int64(15): 156, np.int64(2): 150, np.int64(10): 144, np.int64(18): 134, np.int64(11): 103, np.int64(6): 97, np.int64(3): 80, np.int64(7): 75})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "78               0.9865472317     2.1754155159     3.1619627476     0.8568001919     0.7754318618     0.6955741627     1.7713828087    \n",
            "\n",
            "======== ROUND 79 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.8040724993    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5009703636     1.1147463322     1.3862240314    \n",
            "Counter({np.int64(14): 824, np.int64(9): 461, np.int64(18): 300, np.int64(8): 280, np.int64(4): 227, np.int64(12): 208, np.int64(16): 190, np.int64(0): 168, np.int64(15): 166, np.int64(17): 151, np.int64(10): 134, np.int64(3): 133, np.int64(19): 131, np.int64(1): 130, np.int64(11): 128, np.int64(13): 120, np.int64(5): 107, np.int64(7): 106, np.int64(2): 103, np.int64(6): 102})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "79               0.0937002599     1.3188405037     1.4125407934     0.9028544015     0.8042226488     0.7284688995     1.7305037975    \n",
            "\n",
            "======== ROUND 80 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.1016765833    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0097916126     1.3705399036     1.6392517090    \n",
            "Counter({np.int64(14): 516, np.int64(8): 496, np.int64(9): 423, np.int64(4): 268, np.int64(12): 236, np.int64(18): 230, np.int64(15): 204, np.int64(16): 181, np.int64(17): 168, np.int64(3): 165, np.int64(19): 163, np.int64(1): 144, np.int64(0): 144, np.int64(13): 142, np.int64(11): 131, np.int64(5): 123, np.int64(2): 121, np.int64(10): 121, np.int64(6): 101, np.int64(7): 92})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "80               0.2677924633     2.5031328201     2.7709252834     0.8539218038     0.7811900192     0.7147129187     2.1938862801    \n",
            "\n",
            "======== ROUND 81 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0864262581    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.8157033920     0.6266327500     2.1890707016    \n",
            "Counter({np.int64(8): 611, np.int64(14): 476, np.int64(4): 325, np.int64(9): 289, np.int64(15): 262, np.int64(3): 221, np.int64(12): 199, np.int64(18): 188, np.int64(11): 166, np.int64(13): 160, np.int64(1): 160, np.int64(17): 157, np.int64(19): 153, np.int64(5): 135, np.int64(7): 130, np.int64(6): 117, np.int64(16): 113, np.int64(10): 108, np.int64(0): 104, np.int64(2): 95})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "81               0.2148218453     1.9530143738     2.1678361893     0.9033341329     0.7994241843     0.7284688995     1.7388999462    \n",
            "\n",
            "======== ROUND 82 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3378399611    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.4396595955     1.3154917955     3.1241679192    \n",
            "Counter({np.int64(9): 782, np.int64(8): 395, np.int64(18): 380, np.int64(3): 287, np.int64(15): 215, np.int64(14): 200, np.int64(11): 189, np.int64(12): 179, np.int64(4): 167, np.int64(16): 156, np.int64(10): 151, np.int64(0): 140, np.int64(7): 135, np.int64(17): 128, np.int64(6): 124, np.int64(5): 123, np.int64(19): 119, np.int64(13): 102, np.int64(2): 99, np.int64(1): 98})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "82               0.6408745646     1.8953830004     2.5362575054     0.8834252818     0.7984644914     0.6991626794     1.7965936661    \n",
            "\n",
            "======== ROUND 83 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.9806871414    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7448430061     1.2147061825     0.5301367640    \n",
            "Counter({np.int64(4): 589, np.int64(15): 517, np.int64(8): 407, np.int64(3): 328, np.int64(19): 298, np.int64(12): 252, np.int64(11): 193, np.int64(7): 192, np.int64(13): 189, np.int64(9): 157, np.int64(6): 146, np.int64(14): 141, np.int64(18): 136, np.int64(17): 129, np.int64(10): 112, np.int64(5): 105, np.int64(16): 95, np.int64(1): 70, np.int64(2): 60, np.int64(0): 53})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "83               1.1507688761     2.4045577049     3.5553264618     0.9203645958     0.8339731286     0.7278708134     1.8852598667    \n",
            "\n",
            "======== ROUND 84 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6069777012    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2494325638     1.1978545189     1.0515780449    \n",
            "Counter({np.int64(8): 566, np.int64(9): 444, np.int64(18): 327, np.int64(3): 251, np.int64(12): 213, np.int64(16): 211, np.int64(10): 196, np.int64(14): 194, np.int64(5): 188, np.int64(15): 178, np.int64(4): 166, np.int64(17): 164, np.int64(11): 161, np.int64(0): 159, np.int64(19): 149, np.int64(2): 147, np.int64(6): 139, np.int64(13): 123, np.int64(1): 102, np.int64(7): 91})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "84               0.1024332121     2.0094769001     2.1119101048     0.9220436556     0.8474088292     0.7476076555     1.7532830238    \n",
            "\n",
            "======== ROUND 85 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9675846100    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5497303009     1.5715336800     0.9781965613    \n",
            "Counter({np.int64(9): 737, np.int64(18): 349, np.int64(8): 284, np.int64(0): 247, np.int64(3): 238, np.int64(16): 207, np.int64(12): 197, np.int64(14): 189, np.int64(15): 185, np.int64(10): 175, np.int64(5): 163, np.int64(6): 162, np.int64(11): 157, np.int64(4): 138, np.int64(2): 136, np.int64(19): 136, np.int64(17): 133, np.int64(7): 130, np.int64(13): 104, np.int64(1): 102})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "85               1.5672258139     1.9098802805     3.4771060944     0.8651954905     0.7898272553     0.7272727273     1.7695000172    \n",
            "\n",
            "======== ROUND 86 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                4.1885662079    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.2336609364     1.3856019974     1.8480589390    \n",
            "Counter({np.int64(8): 540, np.int64(15): 292, np.int64(4): 288, np.int64(14): 246, np.int64(12): 230, np.int64(19): 207, np.int64(6): 203, np.int64(9): 192, np.int64(16): 192, np.int64(3): 188, np.int64(17): 180, np.int64(0): 179, np.int64(13): 178, np.int64(7): 177, np.int64(5): 165, np.int64(18): 154, np.int64(10): 151, np.int64(1): 150, np.int64(11): 135, np.int64(2): 122})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "86               0.7752553821     2.6713209152     3.4465763569     0.8925401775     0.8090211132     0.7177033493     1.7471246719    \n",
            "\n",
            "======== ROUND 87 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.6726651192    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.9686212540     1.4402204752     1.5284008980    \n",
            "Counter({np.int64(4): 504, np.int64(19): 373, np.int64(13): 367, np.int64(15): 307, np.int64(12): 294, np.int64(17): 254, np.int64(14): 237, np.int64(5): 231, np.int64(1): 223, np.int64(6): 222, np.int64(7): 208, np.int64(2): 186, np.int64(16): 128, np.int64(10): 118, np.int64(3): 113, np.int64(8): 105, np.int64(11): 103, np.int64(0): 86, np.int64(18): 55, np.int64(9): 55})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "87               0.5461480618     1.9627867937     2.5089349747     0.8579995203     0.7696737044     0.6764354067     1.7406833172    \n",
            "\n",
            "======== ROUND 88 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9238613844    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.6219546795     1.2182042599     2.4037504196    \n",
            "Counter({np.int64(14): 782, np.int64(1): 360, np.int64(13): 311, np.int64(2): 283, np.int64(17): 230, np.int64(19): 228, np.int64(16): 200, np.int64(5): 200, np.int64(0): 196, np.int64(12): 194, np.int64(6): 193, np.int64(4): 156, np.int64(10): 154, np.int64(15): 149, np.int64(7): 132, np.int64(9): 105, np.int64(11): 97, np.int64(18): 73, np.int64(3): 67, np.int64(8): 59})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "88               0.7014839649     1.9542508125     2.6557347775     0.9074118494     0.8099808061     0.7392344498     2.1252067089    \n",
            "\n",
            "======== ROUND 89 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.9081509113    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2823472023     1.1931933165     1.0891540051    \n",
            "Counter({np.int64(8): 464, np.int64(14): 289, np.int64(6): 258, np.int64(10): 237, np.int64(15): 235, np.int64(19): 226, np.int64(12): 222, np.int64(4): 217, np.int64(3): 204, np.int64(7): 198, np.int64(1): 186, np.int64(13): 180, np.int64(5): 178, np.int64(11): 177, np.int64(18): 174, np.int64(16): 156, np.int64(9): 153, np.int64(2): 149, np.int64(17): 138, np.int64(0): 128})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "89               2.8547334671     2.0160048008     4.8707380295     0.9102902375     0.8119001919     0.7302631579     1.8281741142    \n",
            "\n",
            "======== ROUND 90 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5437476635    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5364252329     0.6214630008     0.9149622321    \n",
            "Counter({np.int64(14): 793, np.int64(1): 407, np.int64(13): 333, np.int64(17): 273, np.int64(19): 253, np.int64(2): 253, np.int64(12): 231, np.int64(0): 220, np.int64(16): 201, np.int64(5): 201, np.int64(10): 162, np.int64(4): 145, np.int64(15): 142, np.int64(6): 137, np.int64(9): 101, np.int64(7): 86, np.int64(8): 75, np.int64(11): 62, np.int64(18): 56, np.int64(3): 38})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "90               0.2746531069     2.4093041420     2.6839573383     0.9002158791     0.8157389635     0.7350478469     1.7501673698    \n",
            "\n",
            "======== ROUND 91 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8857624531    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0931582451     1.1154834032     1.9776747227    \n",
            "Counter({np.int64(9): 439, np.int64(8): 301, np.int64(14): 247, np.int64(10): 240, np.int64(12): 240, np.int64(15): 239, np.int64(6): 223, np.int64(5): 207, np.int64(18): 205, np.int64(19): 200, np.int64(4): 199, np.int64(7): 187, np.int64(0): 185, np.int64(3): 176, np.int64(16): 176, np.int64(1): 156, np.int64(11): 149, np.int64(17): 146, np.int64(13): 129, np.int64(2): 125})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "91               0.1876445711     2.3531057835     2.5407502651     0.8879827297     0.8186180422     0.7218899522     1.8228292465    \n",
            "\n",
            "======== ROUND 92 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.9829509258    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.9760842323     1.5057615042     1.4703227282    \n",
            "Counter({np.int64(9): 685, np.int64(0): 429, np.int64(16): 311, np.int64(14): 245, np.int64(5): 231, np.int64(10): 213, np.int64(1): 210, np.int64(12): 200, np.int64(2): 199, np.int64(18): 198, np.int64(6): 187, np.int64(17): 173, np.int64(19): 155, np.int64(13): 143, np.int64(8): 134, np.int64(7): 109, np.int64(4): 96, np.int64(15): 91, np.int64(11): 87, np.int64(3): 73})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "92               2.2081744671     1.9579898119     4.1661643982     0.8606380427     0.7811900192     0.7165071770     1.7340190411    \n",
            "\n",
            "======== ROUND 93 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6608710289    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.1376986504     1.2000432014     1.9376555681    \n",
            "Counter({np.int64(9): 523, np.int64(12): 298, np.int64(19): 251, np.int64(6): 242, np.int64(14): 227, np.int64(5): 220, np.int64(17): 220, np.int64(7): 217, np.int64(8): 207, np.int64(15): 199, np.int64(1): 192, np.int64(0): 186, np.int64(13): 173, np.int64(4): 162, np.int64(16): 160, np.int64(2): 157, np.int64(3): 157, np.int64(10): 147, np.int64(18): 129, np.int64(11): 102})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "93               0.3440042734     2.7651929855     3.1091971397     0.9028544015     0.8099808061     0.7380382775     1.7371411324    \n",
            "\n",
            "======== ROUND 94 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4482190609    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.4790778160     1.4189649820     2.0601129532    \n",
            "Counter({np.int64(9): 470, np.int64(14): 397, np.int64(1): 267, np.int64(12): 252, np.int64(19): 229, np.int64(5): 217, np.int64(17): 216, np.int64(6): 208, np.int64(13): 207, np.int64(2): 203, np.int64(7): 201, np.int64(0): 198, np.int64(15): 161, np.int64(8): 159, np.int64(4): 157, np.int64(16): 156, np.int64(10): 139, np.int64(3): 119, np.int64(18): 108, np.int64(11): 105})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "94               0.2058481425     1.7384479046     1.9442960024     0.8755097146     0.7888675624     0.7087320574     1.8118975163    \n",
            "\n",
            "======== ROUND 95 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.2282857895    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1813263893     1.3682612181     0.8130651712    \n",
            "Counter({np.int64(13): 931, np.int64(19): 665, np.int64(12): 324, np.int64(17): 312, np.int64(15): 285, np.int64(4): 257, np.int64(1): 233, np.int64(7): 230, np.int64(6): 189, np.int64(5): 131, np.int64(2): 129, np.int64(11): 113, np.int64(3): 81, np.int64(10): 69, np.int64(14): 62, np.int64(16): 60, np.int64(0): 44, np.int64(8): 22, np.int64(18): 18, np.int64(9): 14})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "95               0.2162478268     1.8656096458     2.0818574429     0.9021348045     0.8099808061     0.7428229665     1.7331030369    \n",
            "\n",
            "======== ROUND 96 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3089858294    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.7833976746     1.2203227282     2.5630750656    \n",
            "Counter({np.int64(13): 691, np.int64(19): 417, np.int64(15): 326, np.int64(7): 303, np.int64(12): 274, np.int64(4): 266, np.int64(1): 240, np.int64(6): 235, np.int64(17): 182, np.int64(5): 181, np.int64(2): 179, np.int64(11): 167, np.int64(3): 157, np.int64(10): 135, np.int64(14): 112, np.int64(16): 102, np.int64(18): 57, np.int64(0): 56, np.int64(8): 55, np.int64(9): 34})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "96               0.2240189463     1.3044227362     1.5284416676     0.9246821780     0.8195777351     0.7511961722     2.0965294838    \n",
            "\n",
            "======== ROUND 97 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                5.4001908302    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0810298920     1.9598486423     1.1211812496    \n",
            "Counter({np.int64(14): 753, np.int64(1): 427, np.int64(13): 327, np.int64(2): 283, np.int64(17): 259, np.int64(19): 241, np.int64(12): 224, np.int64(0): 196, np.int64(5): 178, np.int64(6): 159, np.int64(16): 152, np.int64(4): 138, np.int64(15): 133, np.int64(7): 132, np.int64(10): 132, np.int64(11): 105, np.int64(3): 93, np.int64(9): 84, np.int64(8): 80, np.int64(18): 73})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "97               0.4913729429     2.2794296741     2.7708024979     0.8966178940     0.8051823417     0.7075358852     1.7478268147    \n",
            "\n",
            "======== ROUND 98 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8919184804    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.6868393421     1.0620546341     0.6247846484    \n",
            "Counter({np.int64(14): 740, np.int64(1): 283, np.int64(2): 269, np.int64(12): 222, np.int64(13): 221, np.int64(17): 209, np.int64(0): 197, np.int64(5): 191, np.int64(4): 188, np.int64(19): 188, np.int64(7): 184, np.int64(8): 170, np.int64(16): 169, np.int64(15): 157, np.int64(6): 156, np.int64(10): 150, np.int64(9): 134, np.int64(3): 120, np.int64(18): 111, np.int64(11): 110})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "98               0.3051230013     2.9660291672     3.2711522579     0.8232189974     0.7428023033     0.6866028708     1.7365658283    \n",
            "\n",
            "======== ROUND 99 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7891595364    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0263624191     1.0884939432     1.9378685951    \n",
            "Counter({np.int64(8): 639, np.int64(14): 291, np.int64(9): 240, np.int64(4): 227, np.int64(3): 214, np.int64(12): 208, np.int64(18): 203, np.int64(5): 203, np.int64(10): 201, np.int64(6): 187, np.int64(2): 185, np.int64(7): 182, np.int64(16): 172, np.int64(11): 163, np.int64(15): 153, np.int64(0): 152, np.int64(1): 147, np.int64(17): 142, np.int64(19): 135, np.int64(13): 125})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "99               0.6711786985     2.3241820335     2.9953608513     0.8606380427     0.7610364683     0.6818181818     2.1157064438    \n",
            "\n",
            "======== ROUND 100 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4300818443    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.7752425671     0.8595154285     1.9157271385    \n",
            "Counter({np.int64(13): 695, np.int64(19): 355, np.int64(12): 354, np.int64(5): 299, np.int64(17): 286, np.int64(7): 285, np.int64(2): 282, np.int64(6): 280, np.int64(15): 202, np.int64(1): 189, np.int64(10): 174, np.int64(4): 165, np.int64(16): 137, np.int64(0): 98, np.int64(3): 96, np.int64(11): 93, np.int64(14): 87, np.int64(18): 39, np.int64(8): 33, np.int64(9): 20})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "100              1.0802886486     2.9112873077     3.9915759563     0.9186855361     0.8234165067     0.7206937799     1.7370593548    \n",
            "\n",
            "======== ROUND 101 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.8284214735    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.1861784458     1.1495356560     2.0366427898    \n",
            "Counter({np.int64(14): 699, np.int64(16): 280, np.int64(10): 268, np.int64(0): 256, np.int64(2): 236, np.int64(5): 224, np.int64(1): 215, np.int64(8): 187, np.int64(6): 174, np.int64(9): 173, np.int64(11): 171, np.int64(18): 169, np.int64(7): 162, np.int64(3): 158, np.int64(4): 154, np.int64(13): 141, np.int64(17): 137, np.int64(15): 129, np.int64(12): 127, np.int64(19): 109})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "101              1.7138199806     2.0900928974     3.8039128780     0.9021348045     0.8214971209     0.7021531100     1.7321169376    \n",
            "\n",
            "======== ROUND 102 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.7550184727    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.4182415009     1.1512796879     2.2669618130    \n",
            "Counter({np.int64(14): 652, np.int64(5): 302, np.int64(6): 295, np.int64(2): 289, np.int64(12): 279, np.int64(17): 260, np.int64(1): 246, np.int64(7): 244, np.int64(10): 230, np.int64(13): 209, np.int64(19): 195, np.int64(16): 181, np.int64(0): 153, np.int64(11): 146, np.int64(15): 124, np.int64(4): 94, np.int64(3): 80, np.int64(18): 70, np.int64(9): 61, np.int64(8): 59})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "102              1.1111501455     2.5419034958     3.6530537605     0.8781482370     0.7744721689     0.6866028708     1.8576266766    \n",
            "\n",
            "======== ROUND 103 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5276392698    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.9691588879     1.8058874607     1.1632714272    \n",
            "Counter({np.int64(14): 814, np.int64(10): 217, np.int64(11): 209, np.int64(6): 205, np.int64(7): 202, np.int64(0): 196, np.int64(13): 192, np.int64(16): 191, np.int64(5): 191, np.int64(4): 189, np.int64(15): 188, np.int64(2): 171, np.int64(3): 168, np.int64(1): 163, np.int64(9): 160, np.int64(8): 156, np.int64(12): 154, np.int64(19): 147, np.int64(18): 133, np.int64(17): 123})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "103              0.2883907557     1.6479172707     1.9363080263     0.9323578796     0.8550863724     0.7314593301     1.7460379601    \n",
            "\n",
            "======== ROUND 104 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.0407207012    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2487795353     0.9598855972     0.2888938785    \n",
            "Counter({np.int64(14): 562, np.int64(10): 319, np.int64(8): 261, np.int64(16): 250, np.int64(18): 236, np.int64(6): 210, np.int64(5): 204, np.int64(3): 202, np.int64(15): 200, np.int64(11): 192, np.int64(7): 191, np.int64(9): 190, np.int64(12): 166, np.int64(4): 166, np.int64(2): 151, np.int64(19): 149, np.int64(0): 142, np.int64(17): 130, np.int64(13): 128, np.int64(1): 120})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "104              0.4522398710     2.5044074059     2.9566473961     0.9201247302     0.8099808061     0.7093301435     1.8579912186    \n",
            "\n",
            "======== ROUND 105 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.7599480152    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0484905243     1.0298122168     1.0186781883    \n",
            "Counter({np.int64(9): 438, np.int64(8): 338, np.int64(14): 322, np.int64(3): 256, np.int64(18): 237, np.int64(15): 230, np.int64(16): 220, np.int64(10): 213, np.int64(4): 190, np.int64(5): 184, np.int64(12): 180, np.int64(0): 171, np.int64(7): 169, np.int64(11): 167, np.int64(2): 164, np.int64(6): 157, np.int64(17): 150, np.int64(19): 148, np.int64(13): 118, np.int64(1): 117})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "105              0.1293253005     2.2966012955     2.4259266853     0.9100503718     0.8128598848     0.7362440191     1.7302265167    \n",
            "\n",
            "======== ROUND 106 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7036105394    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2291574478     0.8196906447     1.4094668627    \n",
            "Counter({np.int64(8): 837, np.int64(3): 355, np.int64(4): 315, np.int64(15): 289, np.int64(9): 198, np.int64(18): 191, np.int64(14): 187, np.int64(10): 177, np.int64(11): 175, np.int64(12): 159, np.int64(16): 158, np.int64(6): 154, np.int64(5): 145, np.int64(7): 143, np.int64(2): 140, np.int64(19): 128, np.int64(17): 124, np.int64(13): 118, np.int64(0): 96, np.int64(1): 80})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "106              1.0119751692     1.9825625420     2.9945378304     0.8301751019     0.7332053743     0.6459330144     1.7552762032    \n",
            "\n",
            "======== ROUND 107 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9856283665    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0609254837     1.0612281561     1.9996973276    \n",
            "Counter({np.int64(8): 673, np.int64(3): 307, np.int64(15): 305, np.int64(4): 251, np.int64(14): 213, np.int64(6): 198, np.int64(7): 195, np.int64(10): 189, np.int64(11): 189, np.int64(9): 185, np.int64(12): 175, np.int64(18): 173, np.int64(16): 163, np.int64(19): 161, np.int64(5): 149, np.int64(13): 147, np.int64(2): 135, np.int64(17): 133, np.int64(0): 116, np.int64(1): 112})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "107              0.2486877441     2.2294082642     2.4780960083     0.9162868793     0.8023032630     0.6931818182     2.1851830482    \n",
            "\n",
            "======== ROUND 108 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.1613872051    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.8131437302     1.1652036905     2.6479401588    \n",
            "Counter({np.int64(9): 880, np.int64(8): 243, np.int64(18): 230, np.int64(15): 216, np.int64(3): 209, np.int64(14): 208, np.int64(7): 195, np.int64(0): 188, np.int64(6): 185, np.int64(10): 184, np.int64(12): 182, np.int64(11): 174, np.int64(16): 174, np.int64(19): 145, np.int64(4): 136, np.int64(17): 133, np.int64(2): 130, np.int64(5): 130, np.int64(13): 121, np.int64(1): 106})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "108              0.1930023283     1.2667939663     1.4597963095     0.8894219237     0.8119001919     0.7511961722     1.7497456074    \n",
            "\n",
            "======== ROUND 109 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5993733406    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8910410404     1.0057481527     0.8852929473    \n",
            "Counter({np.int64(14): 679, np.int64(9): 361, np.int64(0): 314, np.int64(1): 300, np.int64(13): 246, np.int64(16): 209, np.int64(12): 202, np.int64(19): 199, np.int64(2): 197, np.int64(17): 195, np.int64(6): 183, np.int64(5): 159, np.int64(10): 156, np.int64(7): 141, np.int64(15): 134, np.int64(4): 114, np.int64(8): 114, np.int64(18): 104, np.int64(11): 90, np.int64(3): 72})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "109              0.5677239299     2.6426999569     3.2104239464     0.9261213720     0.8320537428     0.7356459330     1.7725863457    \n",
            "\n",
            "======== ROUND 110 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5853700638    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8810880184     1.0062108040     0.8748771548    \n",
            "Counter({np.int64(9): 558, np.int64(14): 447, np.int64(1): 288, np.int64(0): 273, np.int64(17): 223, np.int64(2): 221, np.int64(16): 213, np.int64(12): 197, np.int64(10): 188, np.int64(6): 187, np.int64(13): 187, np.int64(5): 170, np.int64(19): 166, np.int64(7): 143, np.int64(8): 140, np.int64(18): 132, np.int64(15): 126, np.int64(4): 114, np.int64(11): 106, np.int64(3): 90})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "110              0.3996526301     2.5007979870     2.9004507065     0.9220436556     0.8128598848     0.7021531100     1.8632800579    \n",
            "\n",
            "======== ROUND 111 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7474443913    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6398692131     1.2652328014     1.3746365309    \n",
            "Counter({np.int64(8): 366, np.int64(9): 330, np.int64(14): 257, np.int64(10): 229, np.int64(3): 224, np.int64(6): 217, np.int64(4): 208, np.int64(15): 206, np.int64(7): 201, np.int64(12): 200, np.int64(18): 198, np.int64(5): 185, np.int64(16): 185, np.int64(17): 179, np.int64(2): 179, np.int64(19): 177, np.int64(0): 168, np.int64(11): 162, np.int64(13): 150, np.int64(1): 148})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "111              0.4875008166     1.9690434933     2.4565443993     0.7682897577     0.6593090211     0.5783492823     1.7930929661    \n",
            "\n",
            "======== ROUND 112 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1062321663    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4352937937     1.2299944162     0.2052994072    \n",
            "Counter({np.int64(1): 706, np.int64(17): 383, np.int64(13): 338, np.int64(2): 337, np.int64(19): 304, np.int64(14): 274, np.int64(12): 257, np.int64(5): 236, np.int64(0): 199, np.int64(6): 196, np.int64(16): 189, np.int64(7): 180, np.int64(10): 117, np.int64(15): 117, np.int64(11): 91, np.int64(4): 68, np.int64(9): 55, np.int64(18): 50, np.int64(3): 41, np.int64(8): 31})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "112              0.3749908209     2.3533952236     2.7283859253     0.9162868793     0.8234165067     0.7171052632     1.7799866199    \n",
            "\n",
            "======== ROUND 113 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.6434857845    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.9484226704     1.0328478813     2.9155747890    \n",
            "Counter({np.int64(14): 566, np.int64(1): 286, np.int64(6): 259, np.int64(5): 251, np.int64(13): 246, np.int64(7): 243, np.int64(19): 229, np.int64(15): 226, np.int64(2): 203, np.int64(12): 196, np.int64(0): 192, np.int64(16): 190, np.int64(4): 186, np.int64(10): 184, np.int64(17): 176, np.int64(11): 151, np.int64(9): 118, np.int64(18): 92, np.int64(8): 88, np.int64(3): 87})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "113              0.6307788491     2.8436810970     3.4744598866     0.9390741185     0.8301343570     0.7248803828     1.7544333935    \n",
            "\n",
            "======== ROUND 114 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2994608879    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.9737386703     1.7254700661     5.2482686043    \n",
            "Counter({np.int64(1): 680, np.int64(2): 283, np.int64(17): 270, np.int64(13): 268, np.int64(0): 260, np.int64(14): 257, np.int64(5): 257, np.int64(19): 255, np.int64(12): 250, np.int64(16): 235, np.int64(7): 215, np.int64(6): 212, np.int64(10): 150, np.int64(9): 127, np.int64(15): 119, np.int64(11): 108, np.int64(4): 68, np.int64(18): 62, np.int64(3): 50, np.int64(8): 43})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "114              1.2053675652     2.1324553490     3.3378229141     0.9002158791     0.7763915547     0.6800239234     1.7643599510    \n",
            "\n",
            "======== ROUND 115 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1416006088    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.4744563103     1.2828069925     2.1916494370    \n",
            "Counter({np.int64(1): 891, np.int64(13): 288, np.int64(17): 281, np.int64(19): 266, np.int64(0): 233, np.int64(2): 220, np.int64(7): 216, np.int64(6): 204, np.int64(12): 203, np.int64(5): 188, np.int64(16): 184, np.int64(10): 180, np.int64(14): 172, np.int64(15): 136, np.int64(11): 122, np.int64(9): 106, np.int64(4): 99, np.int64(18): 65, np.int64(3): 58, np.int64(8): 57})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "115              0.0830285102     2.7535068989     2.8365354538     0.9345166707     0.8282149712     0.7296650718     2.1508476734    \n",
            "\n",
            "======== ROUND 116 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.1665792465    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.5635266304     1.6343950033     1.9291316271    \n",
            "Counter({np.int64(14): 570, np.int64(15): 257, np.int64(19): 257, np.int64(13): 244, np.int64(7): 221, np.int64(12): 206, np.int64(6): 202, np.int64(5): 196, np.int64(8): 196, np.int64(10): 195, np.int64(3): 192, np.int64(4): 189, np.int64(9): 184, np.int64(11): 179, np.int64(2): 170, np.int64(18): 158, np.int64(17): 156, np.int64(16): 154, np.int64(1): 122, np.int64(0): 121})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "116              0.4746558964     2.8842506409     3.3589065075     0.8882225953     0.7984644914     0.7440191388     1.7754428387    \n",
            "\n",
            "======== ROUND 117 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7662490606    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6781744957     1.4736084938     1.2045660019    \n",
            "Counter({np.int64(14): 755, np.int64(13): 271, np.int64(19): 246, np.int64(4): 222, np.int64(15): 214, np.int64(7): 213, np.int64(5): 191, np.int64(2): 177, np.int64(6): 177, np.int64(17): 171, np.int64(16): 169, np.int64(12): 167, np.int64(10): 166, np.int64(9): 163, np.int64(11): 157, np.int64(8): 156, np.int64(1): 151, np.int64(3): 148, np.int64(0): 139, np.int64(18): 116})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "117              0.8490059972     3.0961592197     3.9451651573     0.8970976253     0.7850287908     0.6991626794     1.7651717663    \n",
            "\n",
            "======== ROUND 118 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.3339605331    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0056178570     1.1488569975     0.8567608595    \n",
            "Counter({np.int64(14): 533, np.int64(19): 474, np.int64(13): 470, np.int64(15): 357, np.int64(4): 272, np.int64(17): 238, np.int64(7): 210, np.int64(1): 171, np.int64(12): 170, np.int64(3): 160, np.int64(8): 155, np.int64(11): 149, np.int64(5): 133, np.int64(6): 130, np.int64(10): 120, np.int64(2): 111, np.int64(16): 95, np.int64(9): 82, np.int64(0): 71, np.int64(18): 68})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "118              1.0063952208     2.0644731522     3.0708684921     0.8445670425     0.7456813820     0.6854066986     1.9123139381    \n",
            "\n",
            "======== ROUND 119 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.0031933784    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.6487574577     1.4076597691     3.2410976887    \n",
            "Counter({np.int64(1): 449, np.int64(14): 416, np.int64(13): 293, np.int64(19): 289, np.int64(17): 272, np.int64(7): 256, np.int64(12): 249, np.int64(2): 235, np.int64(15): 219, np.int64(6): 203, np.int64(5): 180, np.int64(11): 148, np.int64(0): 145, np.int64(4): 141, np.int64(10): 134, np.int64(16): 132, np.int64(3): 115, np.int64(9): 111, np.int64(8): 100, np.int64(18): 82})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "119              0.4816709161     2.5540864468     3.0357573032     0.7392660110     0.6209213052     0.5681818182     1.7786579132    \n",
            "\n",
            "======== ROUND 120 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1981973648    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.1533396244     1.7079685926     1.4453710318    \n",
            "Counter({np.int64(1): 615, np.int64(13): 316, np.int64(19): 310, np.int64(14): 294, np.int64(17): 246, np.int64(2): 213, np.int64(15): 208, np.int64(12): 208, np.int64(7): 206, np.int64(5): 200, np.int64(4): 183, np.int64(6): 170, np.int64(16): 152, np.int64(10): 147, np.int64(11): 147, np.int64(0): 143, np.int64(3): 136, np.int64(8): 98, np.int64(9): 96, np.int64(18): 81})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "120              0.1292076260     2.4601740837     2.5893816948     0.9525065963     0.8272552783     0.7266746411     1.7794432640    \n",
            "\n",
            "======== ROUND 121 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7527658939    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.0541515350     1.5115077496     4.5426440239    \n",
            "Counter({np.int64(14): 491, np.int64(1): 371, np.int64(2): 265, np.int64(16): 219, np.int64(5): 212, np.int64(0): 203, np.int64(6): 195, np.int64(8): 195, np.int64(12): 192, np.int64(10): 191, np.int64(3): 185, np.int64(17): 181, np.int64(13): 179, np.int64(7): 171, np.int64(9): 168, np.int64(18): 161, np.int64(19): 156, np.int64(15): 153, np.int64(11): 148, np.int64(4): 133})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "121              0.6551314592     2.1265516281     2.7816829681     0.8894219237     0.7984644914     0.7075358852     1.7358744144    \n",
            "\n",
            "======== ROUND 122 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.5207718611    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.8690347672     1.0094938278     3.8595411777    \n",
            "Counter({np.int64(1): 448, np.int64(14): 426, np.int64(13): 271, np.int64(2): 262, np.int64(17): 236, np.int64(16): 208, np.int64(19): 206, np.int64(0): 190, np.int64(12): 187, np.int64(15): 174, np.int64(5): 174, np.int64(7): 173, np.int64(9): 171, np.int64(6): 157, np.int64(4): 154, np.int64(8): 153, np.int64(10): 149, np.int64(3): 146, np.int64(11): 145, np.int64(18): 139})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "122              0.5946663618     2.7698974609     3.3645639420     0.9114895658     0.8349328215     0.7206937799     1.7670290470    \n",
            "\n",
            "======== ROUND 123 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4681211710    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7706139088     1.2097235918     0.5608902574    \n",
            "Counter({np.int64(9): 525, np.int64(14): 486, np.int64(8): 266, np.int64(18): 240, np.int64(16): 222, np.int64(3): 220, np.int64(2): 217, np.int64(0): 209, np.int64(10): 208, np.int64(5): 183, np.int64(4): 175, np.int64(1): 161, np.int64(12): 145, np.int64(11): 142, np.int64(15): 138, np.int64(6): 134, np.int64(17): 133, np.int64(19): 130, np.int64(7): 127, np.int64(13): 108})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "123              2.1642346382     2.2994470596     4.4636816978     0.8409690573     0.7504798464     0.6411483254     2.1566224098    \n",
            "\n",
            "======== ROUND 124 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2937352657    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.8049063683     1.2155041695     3.5894021988    \n",
            "Counter({np.int64(9): 482, np.int64(3): 287, np.int64(14): 281, np.int64(8): 279, np.int64(15): 223, np.int64(11): 210, np.int64(2): 204, np.int64(18): 201, np.int64(4): 200, np.int64(7): 188, np.int64(19): 178, np.int64(0): 172, np.int64(6): 171, np.int64(16): 166, np.int64(17): 163, np.int64(5): 159, np.int64(10): 157, np.int64(12): 154, np.int64(13): 151, np.int64(1): 143})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "124              1.1972532272     2.5107393265     3.7079925537     0.9066922523     0.7898272553     0.7254784689     1.7465252876    \n",
            "\n",
            "======== ROUND 125 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.9725964069    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                5.7036805153     1.4661698341     4.2375106812    \n",
            "Counter({np.int64(8): 422, np.int64(14): 380, np.int64(3): 321, np.int64(4): 297, np.int64(15): 296, np.int64(11): 211, np.int64(19): 203, np.int64(7): 203, np.int64(13): 182, np.int64(2): 182, np.int64(16): 169, np.int64(18): 161, np.int64(12): 156, np.int64(6): 151, np.int64(17): 150, np.int64(10): 149, np.int64(9): 147, np.int64(5): 145, np.int64(1): 131, np.int64(0): 113})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "125              0.2668748200     2.2591168880     2.5259916782     0.9426721036     0.8358925144     0.7517942584     1.7474193573    \n",
            "\n",
            "======== ROUND 126 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.1389284134    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9756891727     1.2515860796     0.7241031528    \n",
            "Counter({np.int64(14): 454, np.int64(8): 298, np.int64(3): 288, np.int64(15): 246, np.int64(19): 215, np.int64(11): 213, np.int64(18): 204, np.int64(10): 196, np.int64(12): 193, np.int64(7): 191, np.int64(2): 191, np.int64(13): 186, np.int64(4): 178, np.int64(6): 178, np.int64(16): 175, np.int64(5): 174, np.int64(17): 171, np.int64(1): 159, np.int64(9): 141, np.int64(0): 118})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "126              0.2118648589     2.7361860275     2.9480509758     0.9122091629     0.8195777351     0.7093301435     1.8433907032    \n",
            "\n",
            "======== ROUND 127 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.9344754219    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8702781200     0.7452797294     1.1249984503    \n",
            "Counter({np.int64(9): 708, np.int64(0): 362, np.int64(16): 298, np.int64(2): 269, np.int64(18): 269, np.int64(14): 237, np.int64(1): 201, np.int64(5): 197, np.int64(8): 194, np.int64(10): 186, np.int64(17): 180, np.int64(12): 180, np.int64(3): 156, np.int64(6): 137, np.int64(11): 131, np.int64(7): 125, np.int64(19): 102, np.int64(15): 84, np.int64(13): 78, np.int64(4): 75})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "127              0.2341213971     2.4181756973     2.6522970200     0.9325977453     0.8253358925     0.7218899522     1.7555375099    \n",
            "\n",
            "======== ROUND 128 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                3.9249484539    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0853862762     1.2248098850     0.8605763316    \n",
            "Counter({np.int64(9): 907, np.int64(8): 289, np.int64(18): 272, np.int64(14): 229, np.int64(16): 216, np.int64(0): 213, np.int64(10): 208, np.int64(3): 193, np.int64(2): 193, np.int64(12): 156, np.int64(6): 156, np.int64(17): 150, np.int64(11): 140, np.int64(5): 136, np.int64(7): 135, np.int64(1): 127, np.int64(4): 120, np.int64(13): 111, np.int64(19): 110, np.int64(15): 108})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "128              1.9753873348     1.6467337608     3.6221210957     0.9124490285     0.8023032630     0.7129186603     1.7580990791    \n",
            "\n",
            "======== ROUND 129 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0863049030    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8994817734     1.0840742588     0.8154074550    \n",
            "Counter({np.int64(9): 842, np.int64(18): 335, np.int64(8): 296, np.int64(3): 248, np.int64(16): 217, np.int64(10): 213, np.int64(0): 213, np.int64(14): 210, np.int64(2): 167, np.int64(11): 160, np.int64(6): 153, np.int64(7): 142, np.int64(12): 141, np.int64(17): 132, np.int64(5): 131, np.int64(4): 130, np.int64(15): 121, np.int64(13): 114, np.int64(1): 105, np.int64(19): 99})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "129              0.3134609461     2.3187012672     2.6321620941     0.9431518350     0.8253358925     0.7314593301     1.7524678707    \n",
            "\n",
            "======== ROUND 130 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2843227386    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.4134759903     1.5120724440     0.9014034271    \n",
            "Counter({np.int64(9): 748, np.int64(18): 326, np.int64(0): 271, np.int64(8): 259, np.int64(10): 250, np.int64(16): 244, np.int64(3): 241, np.int64(14): 205, np.int64(2): 161, np.int64(11): 148, np.int64(17): 146, np.int64(5): 144, np.int64(6): 144, np.int64(12): 140, np.int64(1): 137, np.int64(15): 129, np.int64(7): 126, np.int64(4): 124, np.int64(13): 119, np.int64(19): 107})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "130              0.3458855748     2.2119021416     2.5577876568     0.9268409691     0.8272552783     0.7171052632     1.7920262814    \n",
            "\n",
            "======== ROUND 131 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.4781656265    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.6697747707     0.8726193905     2.7971553802    \n",
            "Counter({np.int64(14): 369, np.int64(9): 346, np.int64(15): 265, np.int64(3): 248, np.int64(7): 241, np.int64(13): 232, np.int64(19): 230, np.int64(4): 228, np.int64(11): 227, np.int64(17): 213, np.int64(1): 197, np.int64(8): 193, np.int64(10): 191, np.int64(12): 181, np.int64(6): 168, np.int64(2): 155, np.int64(5): 143, np.int64(16): 117, np.int64(0): 113, np.int64(18): 112})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "131              1.3894889355     2.6525001526     4.0419893265     0.9541856560     0.8243761996     0.7356459330     2.1707415581    \n",
            "\n",
            "======== ROUND 132 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.5492811203    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.3908982277     1.5161503553     1.8747477531    \n",
            "Counter({np.int64(9): 820, np.int64(18): 302, np.int64(3): 282, np.int64(8): 252, np.int64(0): 236, np.int64(10): 229, np.int64(16): 203, np.int64(14): 188, np.int64(11): 171, np.int64(15): 166, np.int64(2): 163, np.int64(7): 157, np.int64(17): 145, np.int64(1): 140, np.int64(12): 132, np.int64(5): 129, np.int64(6): 127, np.int64(4): 113, np.int64(13): 111, np.int64(19): 103})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "132              0.5067685843     1.8114247322     2.3181934357     0.9592228352     0.8349328215     0.7308612440     1.7569470406    \n",
            "\n",
            "======== ROUND 133 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.3798673153    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2654466629     0.7827882767     0.4826583862    \n",
            "Counter({np.int64(14): 631, np.int64(1): 310, np.int64(13): 234, np.int64(2): 218, np.int64(9): 216, np.int64(4): 209, np.int64(17): 204, np.int64(15): 196, np.int64(11): 192, np.int64(7): 187, np.int64(8): 187, np.int64(10): 181, np.int64(0): 171, np.int64(19): 161, np.int64(3): 160, np.int64(12): 159, np.int64(5): 153, np.int64(16): 142, np.int64(18): 140, np.int64(6): 118})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "133              0.4690756798     1.8411998749     2.3102755547     0.9385943871     0.8262955854     0.7398325359     1.7622661591    \n",
            "\n",
            "======== ROUND 134 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8783198595    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.6280488968     1.6547553539     4.9732937813    \n",
            "Counter({np.int64(0): 591, np.int64(9): 378, np.int64(14): 297, np.int64(1): 292, np.int64(2): 205, np.int64(16): 202, np.int64(10): 199, np.int64(8): 195, np.int64(17): 185, np.int64(13): 184, np.int64(18): 177, np.int64(11): 161, np.int64(7): 158, np.int64(5): 150, np.int64(12): 140, np.int64(6): 137, np.int64(4): 132, np.int64(3): 132, np.int64(19): 128, np.int64(15): 126})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "134              0.9502601624     2.9153826237     3.8656427860     0.9316382826     0.8282149712     0.7260765550     1.8245048523    \n",
            "\n",
            "======== ROUND 135 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0077756643    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7691686153     1.0167865753     0.7523820400    \n",
            "Counter({np.int64(0): 528, np.int64(9): 431, np.int64(10): 267, np.int64(18): 264, np.int64(16): 230, np.int64(2): 227, np.int64(5): 209, np.int64(3): 206, np.int64(11): 205, np.int64(7): 182, np.int64(6): 181, np.int64(12): 178, np.int64(8): 177, np.int64(17): 161, np.int64(1): 159, np.int64(15): 142, np.int64(14): 126, np.int64(19): 123, np.int64(13): 89, np.int64(4): 84})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "135              0.9572887421     2.1713078022     3.1285965443     0.8812664908     0.8147792706     0.7422248804     1.7821404934    \n",
            "\n",
            "======== ROUND 136 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.0330262184    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                3.0797338486     1.1528303623     1.9269033670    \n",
            "Counter({np.int64(9): 812, np.int64(0): 487, np.int64(18): 424, np.int64(16): 273, np.int64(2): 237, np.int64(10): 214, np.int64(1): 193, np.int64(3): 187, np.int64(8): 177, np.int64(14): 155, np.int64(5): 146, np.int64(17): 131, np.int64(12): 110, np.int64(6): 108, np.int64(7): 107, np.int64(11): 93, np.int64(4): 86, np.int64(13): 82, np.int64(15): 81, np.int64(19): 66})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "136              2.4980616570     1.8966829777     4.3947448730     0.9448308947     0.8349328215     0.7422248804     1.7619543076    \n",
            "\n",
            "======== ROUND 137 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.4998102188    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2352743149     1.4261746407     0.8090996146    \n",
            "Counter({np.int64(9): 637, np.int64(0): 385, np.int64(18): 326, np.int64(8): 241, np.int64(1): 219, np.int64(14): 217, np.int64(2): 214, np.int64(10): 204, np.int64(16): 188, np.int64(3): 186, np.int64(13): 153, np.int64(17): 151, np.int64(5): 148, np.int64(7): 140, np.int64(4): 137, np.int64(12): 135, np.int64(11): 133, np.int64(19): 128, np.int64(6): 127, np.int64(15): 100})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "137              0.0250742976     1.9643673897     1.9894416332     0.8584792516     0.7591170825     0.6710526316     1.7363452911    \n",
            "\n",
            "======== ROUND 138 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3537306786    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                6.9640598297     1.5678188801     5.3962407112    \n",
            "Counter({np.int64(0): 557, np.int64(9): 546, np.int64(1): 282, np.int64(18): 265, np.int64(8): 238, np.int64(14): 224, np.int64(10): 198, np.int64(2): 195, np.int64(16): 186, np.int64(3): 180, np.int64(17): 178, np.int64(13): 153, np.int64(7): 136, np.int64(4): 133, np.int64(12): 132, np.int64(5): 129, np.int64(6): 117, np.int64(19): 113, np.int64(15): 105, np.int64(11): 102})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "138              1.2528786659     2.4565808773     3.7094595432     0.9469896858     0.8435700576     0.7374401914     1.7583684921    \n",
            "\n",
            "======== ROUND 139 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.2769998312    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.9726355076     0.9116811752     1.0609543324    \n",
            "Counter({np.int64(9): 662, np.int64(18): 415, np.int64(10): 276, np.int64(0): 259, np.int64(8): 223, np.int64(3): 222, np.int64(5): 202, np.int64(2): 195, np.int64(6): 191, np.int64(16): 183, np.int64(7): 169, np.int64(11): 163, np.int64(17): 155, np.int64(1): 148, np.int64(12): 147, np.int64(4): 131, np.int64(14): 130, np.int64(15): 106, np.int64(13): 98, np.int64(19): 94})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "139              0.1715720147     2.5562994480     2.7278714180     0.9114895658     0.8128598848     0.6955741627     2.1606068611    \n",
            "\n",
            "======== ROUND 140 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.3070920110    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.5484855175     1.1407626867     1.4077228308    \n",
            "Counter({np.int64(9): 621, np.int64(0): 519, np.int64(18): 292, np.int64(2): 231, np.int64(16): 222, np.int64(1): 218, np.int64(7): 192, np.int64(10): 189, np.int64(17): 185, np.int64(5): 179, np.int64(8): 172, np.int64(14): 165, np.int64(12): 143, np.int64(6): 137, np.int64(3): 127, np.int64(4): 125, np.int64(13): 124, np.int64(19): 121, np.int64(15): 108, np.int64(11): 99})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "140              1.6661783457     2.9909143448     4.6570925713     0.9194051331     0.8157389635     0.6901913876     1.7464368343    \n",
            "\n",
            "======== ROUND 141 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8062033057    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.6757150888     1.1143606901     0.5613543987    \n",
            "Counter({np.int64(0): 756, np.int64(9): 395, np.int64(18): 299, np.int64(2): 275, np.int64(16): 273, np.int64(10): 214, np.int64(1): 211, np.int64(5): 199, np.int64(17): 172, np.int64(7): 169, np.int64(6): 158, np.int64(8): 147, np.int64(12): 144, np.int64(3): 141, np.int64(11): 128, np.int64(14): 125, np.int64(19): 98, np.int64(13): 95, np.int64(4): 85, np.int64(15): 85})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "141              1.0547459126     2.9245119095     3.9792578220     0.8395298633     0.7216890595     0.6100478469     1.7536253929    \n",
            "\n",
            "======== ROUND 142 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.8733109236    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                4.9128370285     1.8310086727     3.0818281174    \n",
            "Counter({np.int64(0): 409, np.int64(1): 357, np.int64(7): 261, np.int64(17): 236, np.int64(14): 216, np.int64(2): 216, np.int64(19): 208, np.int64(11): 204, np.int64(15): 202, np.int64(5): 195, np.int64(12): 192, np.int64(3): 192, np.int64(13): 186, np.int64(6): 181, np.int64(9): 166, np.int64(10): 160, np.int64(4): 154, np.int64(18): 153, np.int64(8): 153, np.int64(16): 128})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "142              0.1487370282     1.9671418667     2.1158788204     0.8764691773     0.7715930902     0.7003588517     2.0389642715    \n",
            "\n",
            "======== ROUND 143 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                8.5466222763    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1536893845     1.2204538584     0.9332354069    \n",
            "Counter({np.int64(0): 442, np.int64(14): 379, np.int64(1): 293, np.int64(13): 248, np.int64(17): 246, np.int64(8): 245, np.int64(9): 211, np.int64(4): 201, np.int64(19): 194, np.int64(3): 183, np.int64(15): 181, np.int64(7): 177, np.int64(2): 173, np.int64(10): 160, np.int64(5): 155, np.int64(11): 151, np.int64(12): 137, np.int64(18): 134, np.int64(16): 133, np.int64(6): 126})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "143              0.0134232631     2.0142414570     2.0276646614     0.9513072679     0.8387715931     0.7200956938     1.7920653820    \n",
            "\n",
            "======== ROUND 144 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.9495316744    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7344230413     1.5594438314     0.1749792546    \n",
            "Counter({np.int64(0): 852, np.int64(1): 358, np.int64(9): 264, np.int64(2): 239, np.int64(14): 235, np.int64(17): 227, np.int64(16): 180, np.int64(5): 172, np.int64(8): 161, np.int64(13): 159, np.int64(10): 157, np.int64(7): 153, np.int64(19): 151, np.int64(12): 143, np.int64(3): 141, np.int64(18): 132, np.int64(11): 124, np.int64(6): 122, np.int64(4): 102, np.int64(15): 97})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "144              0.0704047754     2.5609540939     2.6313588619     0.8966178940     0.8090211132     0.7003588517     1.7674052715    \n",
            "\n",
            "======== ROUND 145 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.3128354549    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0417895317     1.0982345343     0.9435548782    \n",
            "Counter({np.int64(0): 566, np.int64(14): 302, np.int64(1): 296, np.int64(2): 230, np.int64(9): 225, np.int64(17): 220, np.int64(7): 208, np.int64(19): 189, np.int64(13): 189, np.int64(8): 188, np.int64(10): 184, np.int64(5): 177, np.int64(12): 160, np.int64(16): 155, np.int64(11): 152, np.int64(15): 150, np.int64(6): 149, np.int64(18): 148, np.int64(3): 143, np.int64(4): 138})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "145              1.1010086536     2.2630321980     3.3640408516     0.8033101463     0.7120921305     0.6046650718     1.7426958084    \n",
            "\n",
            "======== ROUND 146 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.3860001862    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.2518429756     1.4286231995     0.8232198358    \n",
            "Counter({np.int64(0): 450, np.int64(1): 320, np.int64(7): 258, np.int64(17): 256, np.int64(19): 223, np.int64(14): 220, np.int64(10): 218, np.int64(11): 208, np.int64(6): 200, np.int64(5): 190, np.int64(15): 184, np.int64(13): 183, np.int64(12): 181, np.int64(3): 177, np.int64(2): 174, np.int64(8): 162, np.int64(18): 147, np.int64(9): 145, np.int64(16): 137, np.int64(4): 136})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "146              0.7116926312     2.1468818188     2.8585743904     0.9393139842     0.8186180422     0.7188995215     1.7484447956    \n",
            "\n",
            "======== ROUND 147 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                5.1147866249    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.3644654751     0.8515093923     1.5129561424    \n",
            "Counter({np.int64(0): 736, np.int64(9): 385, np.int64(2): 275, np.int64(16): 273, np.int64(10): 259, np.int64(5): 251, np.int64(18): 242, np.int64(7): 235, np.int64(6): 222, np.int64(11): 186, np.int64(12): 180, np.int64(3): 160, np.int64(8): 147, np.int64(17): 143, np.int64(1): 108, np.int64(19): 105, np.int64(14): 83, np.int64(15): 80, np.int64(13): 59, np.int64(4): 40})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "147              1.3263078928     2.4878642559     3.8141722679     0.9083713121     0.8147792706     0.6985645933     2.1407938004    \n",
            "\n",
            "======== ROUND 148 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8240697384    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6949400902     1.4220703840     1.2728695869    \n",
            "Counter({np.int64(0): 801, np.int64(9): 408, np.int64(16): 295, np.int64(2): 251, np.int64(18): 237, np.int64(10): 221, np.int64(7): 212, np.int64(5): 210, np.int64(6): 192, np.int64(11): 170, np.int64(3): 158, np.int64(12): 158, np.int64(1): 153, np.int64(8): 151, np.int64(17): 144, np.int64(14): 99, np.int64(19): 94, np.int64(15): 85, np.int64(13): 74, np.int64(4): 56})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "148              3.3643996716     2.2845127583     5.6489124298     0.9062125210     0.7994241843     0.7230861244     1.7688136101    \n",
            "\n",
            "======== ROUND 149 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.7795979977    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.6123285294     1.0170279741     1.5953006744    \n",
            "Counter({np.int64(0): 493, np.int64(9): 278, np.int64(3): 265, np.int64(11): 263, np.int64(7): 257, np.int64(18): 248, np.int64(6): 228, np.int64(10): 216, np.int64(5): 186, np.int64(12): 184, np.int64(19): 184, np.int64(8): 179, np.int64(2): 173, np.int64(16): 171, np.int64(17): 166, np.int64(15): 157, np.int64(1): 156, np.int64(13): 124, np.int64(14): 122, np.int64(4): 119})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "149              0.1714524180     2.6121530533     2.7836055756     0.9146078196     0.8138195777     0.7488038278     1.7644901276    \n",
            "\n",
            "🎯 Final Target Accuracy: 0.7315\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "[WARN] Adjusting flat_inputs from 1600 to 9600\n",
            "/content/extddivesify/diversify/shap_utils.py:47: FutureWarning:\n",
            "\n",
            "The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "\n",
            "[SHAP] Accuracy Drop: 0.0000\n",
            "[SHAP] Flip Rate: 0.0000\n",
            "[SHAP] Confidence Δ: 0.0003\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 1.6475\n",
            "[SHAP] Coherence: 0.0240\n",
            "[SHAP] Jaccard: 0.0000\n",
            "[SHAP] Kendall’s Tau: 0.0393\n",
            "[SHAP] Cosine Sim: 0.1311\n",
            "Signal shape before reshape: (8, 1, 200)\n",
            "SHAP value shape before reshape: (8, 1, 200, 6)\n",
            "{'text/html': '<html>\\n<head><meta charset=\"utf-8\" /></head>\\n<body>\\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: \\'local\\'};</script>\\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1e198563-72c9-48c0-891f-e214352cac57\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e198563-72c9-48c0-891f-e214352cac57\")) {                    Plotly.newPlot(                        \"1e198563-72c9-48c0-891f-e214352cac57\",                        [{\"hovertemplate\":\"Time=%{x}\\\\u003cbr\\\\u003eChannel=%{y}\\\\u003cbr\\\\u003eSignal=%{z}\\\\u003cbr\\\\u003eSHAP Importance=%{marker.color}\\\\u003cextra\\\\u003e\\\\u003c\\\\u002fextra\\\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[-0.00005925135459013594,-0.00007475491050475587,-0.00009686408642058571,-0.00008367381330269079,0.000027173664420843124,-5.051144398748875e-6,0.000309932113547499,0.0003022565894449751,0.0006010639481246471,0.0006606033227096001,0.0008551049589489897,0.0010146869462914765,0.0012448177052040894,0.0012676299472029011,-0.002167600051810344,0.003308641014882596,0.0033987072141220174,0.0025240794445077577,0.0017728275512733187,0.0005870173433019469,0.0006430636858567595,-0.0014665295990804832,-0.0009539614742000898,-0.003934673499315977,-0.003823164510928715,-0.006439920204381148,-0.006487075860301654,-0.007188667853673299,-0.008370134979486465,-0.007787839819987615,-0.006473713864882787,-0.004920334710429112,-0.004329240104804437,-0.0021316579853494964,-0.0009874932778378327,0.0021941600328621766,0.001826619729399681,-0.00020281613493959108,-0.00027775570439795655,-0.00021229489357210696,0.00016746910599370798,-4.365341737866402e-6,0.0003789309024189909,0.0006590559690569838,0.0004346101389576991,0.00028836363344453275,-0.00004126635515907159,-0.00034695239931655425,-0.0004996160278096795,-0.0016899774006257455,-0.0006301383837126195,-0.0015981704151878755,-0.0004117856587981805,-0.0001323782877686123,-0.0002464127804463108,-6.570015102624893e-6,0.0016333666474868853,-0.0011650687083601952,-0.00142707188691323,-0.0014713159471284598,-0.0014014475358029206,-0.0008277954378475746,-0.0010228515893686563,-0.0003571106353774667,-0.00042515255821247894,0.00008644989187208314,-0.00022743147565051913,0.00008154093908766906,-0.0003282706408451001,0.0002162197003296266,-0.00035419662405426305,0.00045970511079455417,0.000436410671682097,0.0003516853563875581,0.00036895110194260877,0.0003977965485925476,0.0005388561791429917,0.00038078004339088994,0.0005444511771202087,0.0004382436163723469,0.00035982610036929447,-0.000033893040381371975,0.00020878735813312232,0.0003943404881283641,0.0002742665577291821,0.00031038369828214246,0.00015553156845271587,0.00030009277785817784,0.00034049580911717686,0.0004553770995698869,0.0002678021361740927,0.00017590269756813845,0.00014546767730886737,0.00018179697023394206,-3.193941665813327e-6,-8.687376976013184e-6,-0.00019248489600916704,-0.00016429922349440554,-0.00018659587173412243,-0.00008501326859307785,-0.00011290034793394928,-0.00015999971462103227,-0.00010846497025340796,-0.00007035025434258084,-0.0001085494295693934,-0.00015850500494707376,-0.0000665476933742563,0.000030344337574206293,0.00021385440292457739,0.0005166348710190505,0.0005883660196559504,0.0006996080046519637,0.0007485188543796539,0.001472429644005994,0.001017111314771076,-0.0033183112391270697,-0.004203097312711179,-0.002557113921890656,-0.0029931419218579927,-0.0015753833188985784,-0.0020460820766553902,-0.001124036100615437,-0.0002289969512882332,-0.0005739447466718653,0.0004995083048318824,0.0005756124931698045,0.0010522606802017738,0.0006448589480593606,0.001527807869327565,0.0014688004351531465,0.0011599384403477113,0.0016675990773364902,0.001331645529717207,0.0015851565209838252,0.0012413097235063713,0.001058507749500374,0.000948348354237775,0.00037477092701010406,0.00029337247300039354,0.00007775280391797423,-0.00011058759021883209,-0.000039223135293771826,-0.00008286597828070323,0.00009538921585772187,-0.00017574837935778,-0.00018371853123729429,-0.0002890573038409154,-0.00028138497509644367,-0.00002416300048935227,0.00007501649573290099,0.00014615008452286324,0.0002193954618026813,-0.003756310480336348,-0.00010470809502294287,-2.716114977374673e-8,-0.00014479384602357945,-0.00006561448390129954,0.0000389418516230459,-0.00005659308711377283,0.00006378634619371344,0.000022260125357812893,-0.00008355648242286406,-0.00020833526893208423,-0.0002857534758125742,-0.00039024259118984145,-0.0006379402863482634,0.00022713282669428736,0.0008575480557434881,0.0006028381176292896,0.0006533711954640845,0.0007634155723887185,0.00042962281925914186,0.00007075990045753618,0.00020892227864048132,0.0002800999985386928,0.0003169449046254158,0.0009099679300561547,0.0005620426964014769,0.001023340955725871,0.00045428114632765454,0.000010323555519183477,-0.00020291365217417479,-0.0006189075453827778,-0.0007181881616512934,-0.0006829687821057936,-0.0010415911480473976,-0.00046861065008367103,-0.00014719685229162374,0.0003472285655637582,-0.0030288038154443107,-0.0031549546401947737,-0.0031085769878700376,-0.003954594178746144,-0.0028031106727818647,-0.002576104598119855,-0.0010504251501212518,-0.000682381292184194,-0.0004368147056084126,-0.00018546481927235922,-0.000056400119016567864,-1.1785329358341794e-6,-5.338450137060136e-6,6.927340412706447e-6,7.112132152542472e-6,0.000058962243201676756,0.00004871819207134346,0.0001309599756496027,0.0001224244260811247,0.00026126282561259967,0.0001780114835128188,0.0004872564071168502,0.0005053186129468182,0.0007097058502646784,0.0007532682890693346,0.0008870646124705672,0.0003624451928772032,0.0005417732366671165,0.0005657024254711965,0.0006249787984415889,0.0007582444911046574,0.0005776498777170976,0.0005810341972392052,0.00028387512914681184,0.00032457995500105125,0.0003060147088641922,0.0003176453950194021,-0.00008886765378216903,0.00006495474372059107,0.00020232625926534334,0.00019202249435087046,0.00024274286503593126,0.00014215211073557535,0.00027510822595407564,0.00016053354678054652,0.00017079040117096156,0.00027408168110317394,0.0002673315369368841,0.00019719814493631324,0.000332566295886257,0.0004940793890758263,0.0005914619784258927,0.0006371255925235649,0.0006905576447024941,0.0007393973452659944,0.0007326731962772707,0.0006719412825380763,0.000669572502374649,0.00044598703971132636,0.0005312133580446243,0.00010478514983939628,0.00033721841949348647,0.0004294249617184202,0.0005593235885802036,0.0005120642514763555,0.000614841619002012,0.0006212596199475229,0.0005762360536512764,0.0002861558459699154,0.0005027282168157399,0.0003970846337324474,0.0005382137484654473,0.0005706798983737826,0.000344305946782697,0.00023737563363586864,0.00017295791743284403,0.00010988232679665089,0.00013465661322697997,-0.00021677025749037662,-0.0009738166506091753,-0.000996682676486671,-0.0012127540733975668,-0.0010117431326458852,-0.0021275053053007773,0.0011205143528059125,0.0007907898204090694,0.00047665285334611934,0.0005792596687873205,0.0004987273326454064,0.0007926397859894981,0.0009458080700521047,0.0010574530994441982,0.0007947214956705769,0.0005499939434230328,0.0004623524534205596,0.0004511407654111584,0.00042774935718625784,0.0004818690164635579,0.0003219244147961338,0.00047741306480020285,0.0005159887562816342,0.0006498397948841254,0.0007518571801483631,0.000804650674884518,0.0007165918359532952,-0.00012839577781657377,-0.00015340963727794588,5.165503049890201e-6,0.00010420816640059154,0.00023005062636608878,0.0003031464875675738,0.0003234310473393028,0.0005044596812998255,0.0006444695318350568,0.0007202657531403626,0.0008648893175025781,0.0008994221376876036,0.0008078002914165457,0.0005014889951174458,0.0009403240401297808,0.001096927250425021,0.0013567937227586906,0.0013741743556844692,0.0013360277225729078,0.001398233735623459,0.0017027439510760207,0.00013768507536345473,0.00020030649223675331,-0.00014600394448886314,-0.00005564399422534431,1.919067775209745e-6,0.00020046650509660444,-0.00011416311220576365,0.00039287045365199447,-0.0002838914782235709,-0.00009413274528924376,-0.0001449979899916798,-0.0005389712750911713,-0.00033958343071086955,-0.0005198463525933524,-0.0007914543821243569,-0.0007603614940308034,-0.0009553795389365405,-0.0009109936266516646,-0.0007728459895588458,-0.0006057070907748615,-0.0003684929106384516,-0.00040143685570607585,0.00012725149281322956,-0.0008310924361770352,-0.0010057688147450488,-0.0010681826000412304,-0.001020706365428244,-0.0014249572219947975,-0.0007941879642506441,-0.0010431242020179827,-0.0005298053826360652,-0.0003549146931618452,-0.00016961432023284337,0.00019731030139761666,0.00030139726974690956,0.0004908705983931819,0.000637576888645223,0.0000361877609975636,-0.0004268394356283049,-0.00042234337888658047,-0.00019572679593693465,-0.00018365680565087436,-0.0002265229425878109,-0.00016807514960722378,-0.00023060700126128117,-0.0001942574623778152,-0.00033449635763342184,-0.0002616593847051263,-0.000264662488916656,-0.00023296396830119193,-0.00023433899817367396,0.0003204891108907759,0.0003121407935395837,0.00027877290510029223,0.00021460348216351122,0.00019663853648429117,0.0001230301180233558,0.000275973929092288,0.0001297012980406483,0.0007803973470193645,0.0008247933195283016,0.0007868263758912993,0.000819491494136552,0.0008532117572030984,0.0007528791514535745,0.000787929748184979,0.0006888506334992902,0.0007670695000949005,0.0007933447050163522,0.0008827266186320534,0.0007819851549963156,0.0006662279726394141,0.0008417295951706668,0.0009328107310769459,0.0010278145006547372,0.0007922208848564575,0.0006167740405847629,0.0005652351925770441,0.0004927286936435848,0.00045010505709797144,0.00025380578396531445,0.00018930185857849816,0.00009078870789380744,0.000022086398530518636,4.606542840216814e-6,-0.00004198332802237322,-0.00005839654477313161,-0.00009852904865207772,-0.00010313827078789473,-0.00013184481455634037,-0.00014261453179642558,-0.00021426529080296555,-0.00016237469390034676,-0.0002076367624492074,-0.00023957775920280255,-0.00003258053523798784,-0.00021076237317174673,-0.00014061727173005542,-0.00011967758958538373,-0.0006124708258236448,-0.00015201528246204057,0.00017340900376439095,0.00032845574120680493,0.00016916186238328615,0.0003545614648222302,0.00027553394708471995,0.00011777029430959374,0.00006652703935590883,-0.00008493504719808698,-0.000037493339429299034,-0.0000257792416960001,0.00004849345229255656,-0.0001982414881543567,0.00004163514434670409,0.0001390456066777309,0.0003373292274773121,1.610011774270485e-6,0.00010750207002274692,-0.000640900213814651,-0.0005826591950608417,-0.0009095291219030818,-0.0005873106323027363,-0.0008762575841198365,-0.0007509847249214848,-0.0005152379162609577,-0.0006410414741064111,-0.0006474937157084545,-0.0006581202227001389,-0.000601502601057291,-0.0006539811729453504,-0.0005602478728784869,-0.0006052711008427044,-0.00047782176989130676,-0.0004102597401166956,-0.0005952859452615181,-0.0006061695360889038,-0.0009371651491771141,-0.0010225535370409489,-0.0013557328881385426,-0.00132375486039867,-0.0014793346635997295,-0.0012171123914110165,-0.0014180711635466043,-0.0009759390765490631,-0.001131951401475817,-0.0009597444247143964,-0.0008845759148243815,-0.0010219599256136764,-0.0007884907730234166,-0.0010451404959894717,-0.0007246054810821079,-0.0006776443139339486,-0.0006320983011391945,-0.0004707192226002614,-0.0005155871525251617,-0.000675444234123764,-0.000574893473337094,-0.0006314727943390608,-0.0002590749063529074,-0.0004841644937793414,-0.0006136520144840082,-0.0005898641732831796,-0.0005888530674080054,-0.000533242845752587,-0.0008197954933469495,-0.0009324288306136926,-0.0009158357279375196,-0.0010669047478586435,-0.0011675015557557344,-0.0010128732731876273,-0.0010425906657474115,-0.0007925042882561684,-0.0006843399605713785,-0.0004554059705697,-0.0004262086780120929,-0.00045090948697179556,-0.00038368111321081716,-0.0003020180156454444,-0.0004832456276441614,-0.000210613749610881,-6.247001389662425e-6,0.00018546022086714706,0.00026524367664630216,0.00040766109790032107,0.0002886311898085599,0.00044196277546385926,0.00048042783843508613,0.0003930318440931539,0.00010287606467803319,-0.00014984778681537136,-0.00030967832086995867,-0.0006137920039085051,-0.0004423140975025793,-0.00039417044414828223,-0.0005841776728630066,-0.00040128482699704665,-0.0005988585956705114,-0.0006555254755464072,-0.0006750333899011215,-0.0005468229452768961,-0.0003028745074213172,-0.0003771470665014931,-0.0005703335846192203,-0.0004825704697092685,-0.0006315354451847573,-0.0006826362223364413,-0.0004767041439966609,-0.000850106841729333,-0.0007316746875100458,-0.00068822240185303,-0.0003553123096935451,-0.00035762997382941347,-0.00007537229491087298,-0.000011099740125549337,-0.00008296120116331925,-0.00015322134519616762,-0.000029386992537183687,0.0000804106384748593,-0.00019935522383699814,-0.00003320188261568546,-0.00017758843023329973,0.00008902470775259037,-0.00229992822278291,0.0018855627664985757,0.0014657735494741548,0.0016555750820164878,0.001967637644459804,0.0014313426800072193,0.001729395550986131,0.001230574135358135,0.001564798488592108,0.0012837715136508148,0.0011134720019375284,0.00139469129499048,0.001207140740007162,0.001087646388138334,0.0010753098176792264,-1.5163289693494637e-6,-0.0012453186403339107,-0.0011476604364967595,-0.0009846093598753214,-0.000912897870875895,-0.0008964450098574162,-0.0007551578261579076,-0.0005464213027153164,-0.0004347900018425814,-0.00015466384744892517,-0.00014801499977086982,0.00009896039652327697,0.0002708379179239273,0.00022849279533450803,-0.0003019531238048027,-0.0004446901536236207,-0.0004006579207877318,-0.0004696949617937207,-0.0007676526535457621,-0.000914088023516039,-0.0007458187756128609,-0.0009388741261015335,-0.0006095263136861225,-0.0009745184797793627,-0.00046814036128732067,-0.0009313787644108137,-0.0008873255380118886,-0.0007742323214188218,-0.0007306028589179429,-0.0008167425791422526,-0.0007099747939112907,-0.00040712209496026236,-0.0005841599583315352,-0.0004982142709195614,-0.00015301192130815858,0.000021735298408505816,-3.2088719308376312e-6,0.000028970806548992794,0.0004071945634980996,0.00048152888969828683,0.000531066965777427,0.000636534687752525,0.0004978420135254661,0.0003121316743393739,0.00019527542948101959,-0.00004685757448896766,-0.000042664437690594546,-0.00006415194366127253,-0.00011942686978727579,-0.0001347394815335671,-0.0003431575217594703,-0.00035437080077826977,-0.0004280326732744773,-0.0005417068799336752,-0.00037771960099538166,-0.000597287512694796,-0.0008003870025277138,-0.0006702539976686239,-0.0009271654610832533,-0.0007244301959872246,-0.0007150373421609402,-0.0007910875913997492,-0.0008861895961066087,-0.0007931596289078394,-0.0008811739583810171,-0.0011530579067766666,-0.0008416924004753431,-0.0011732019484043121,-0.0008344178398450216,-0.0008817220417161783,-0.000013081667323907217,-0.0010013577217857044,-0.0005375568677360812,-0.001202050024100269,-0.0006106349950035413,-0.0009242111506561438,-0.0009209598259379467,-0.00024216979121168455,4.358900090058644e-6,0.00010151066817343235,-0.00020302521685759226,0.00002435874193906784,0.0004869137580196063,-0.0013908434387606878,0.0001520305716743072,0.0021197215343515077,0.0023821176340182624,0.0018148326392595966,0.0015566096020241578,0.001085246525083979,0.0008806886229043206,0.0003653429448604584,0.000024842369991044205,-0.0003756218551037212,-0.0002111521316692233,-0.0009332419334289929,-0.000975172258525466,-0.0022556970361620188,-0.002914390837152799,-0.002247022872325033,-0.002886387965797136,-0.0018184174890241895,-0.0021309518876175084,-0.00254234349510322,-0.0019744762297098837,-0.0013806062440077465,-0.00152977230027318,-0.0020474519891043506,-0.0017057131044566631,-0.0019940690447886786,-0.00235569446037213,-0.0023028533905744553,-0.0036106940048436322,-0.002764212704884509,-0.003168601697931687,-0.002812736357251803,-0.0029110279865562916,-0.0031051794067025185,-0.002303419013818105,-0.002526347835858663,-0.0019642524421215057,-0.004220398763815562,-0.0026712937590976558,-0.002936934120953083,-0.002341194192316228,-0.0029701602179557085,-0.0021602644119411707,-0.0020082425326108932,-0.002136522651805232,-0.0021348700392991304,-0.002822677333218356,-0.0017133949246878426,-0.0019820984452962875,-0.0013026700277502339,-0.0013973982228587072,-0.001052006147801876,-0.0007129259950791796,-0.0009742101246956736,-0.0008184115091959635,-0.0006638501460353533,-0.001117114753772815,-0.0009545640011007587,-0.0011447099968791008,-0.002123328081021706,-0.0016496788205889363,-0.002387623011600226,-0.002581869290831188,-0.0016265058269103367,-0.0019640367633352676,-0.0022207569951812425,-0.0018569567085554202,-0.0028285575099289417,-0.0014648699046423037,-0.0016494059624771278,-0.0013479141828914483,-0.0010996912509047736,-0.000737283747488012,-0.0007893688452895731,-0.00033138801033298176,-0.00034307890261212987,0.00029417884070426226,0.00023061073928450546,0.0005952940167238315,0.00045124348253011703,0.0003782241413622008,0.00004598416853696108,0.00013814063277095556,0.00022042511651913324,0.0004630549034724633,0.00034433034791921574,0.0003684723293796803,0.00018621978718632212,-6.242364179342985e-6,-0.0005450547420574973,-0.0008663754366959134,-0.00108672387432307,-0.0020362511665249863,-0.0016646261519781547,-0.0023182033328339458,-0.0021524771970386305,-0.00287717852431039,-0.003160287315646807,-0.0033217021506667757,-0.0032987557739640274,-0.003604429744882509,-0.002982582770831262,-0.002405468374490738,-0.002811368554830551,-0.0025430393870919943,-0.002745032931367556,-0.0024494256358593702,-0.0025628474541008472,-0.0027390388034594557,-0.0032003731466829777,-0.0029309738019946963,-0.0027889559666315713,-0.0023622378357686102,-0.002149682278589656,-0.0018345618930955727,-0.0019123462649683158,-0.0009402293168629209,-0.00017518944514449686,0.0004168640007264912,-0.00011412710106621186,-0.00010222686008395006,-0.00007426426357900102,-0.00014500402903649956,-0.00035324836790096015,-0.0008766854104275504,-0.0009151999062548081,-0.0008323018167478343,-0.0013421860251886149,-0.0022986390783141055,-0.001996491958076755,-0.0018217041263900076,-0.002121121680829674,-0.0013088908841988693,-0.0017929760894427698,-0.002000579455246528,-0.002189041037733356,-0.0024683590357502303,-0.0031957350050409636,-0.0031064723273933246,-0.003539864361907045,-0.0019898864751060805,-0.003163302317261696,-0.0018123284292717774,-0.002095776222025355,-0.0013976705571015675,-0.0014907226044063766,-0.0004729659801038603,-0.0008989946315220246,-0.0003410978242754936,-0.0009485371022795638,-0.00017466275797535977,-0.00021765241399407387,-0.0003805510156477491,-0.0003573645759994785,-0.0008783202210906893,-0.00045603707743187744,-0.00017842328331122795,-0.00040218410625432927,-0.0004032267703829954,-0.00022228783927857876,-0.00010489514291596909,-0.00018447940237820148,-0.00016707231407053769,3.2999093188360953e-6,0.00008025683928281069,-0.000026126450393348932,0.00016049930127337575,0.00003143164697879305,0.00014402837647746006,4.525335195163886e-6,0.0003005238371163917,0.0002401431556791067,0.00009928868773082893,0.0013611526228487492,0.0004902759489292899,0.0016886186397944887,0.0008732489465425411,0.0008702242436508337,0.002471142370874683,0.0014912876455734174,0.002300738221189628,0.0024190074764192104,0.0033222429919987917,0.0034891118217880526,0.0027672362436229983,0.0046584858985928195,0.005178961902856827,0.003212369706792136,0.0024605058133602142,0.0022049747252215943,0.0017168773726249735,0.0012424859839181106,0.0006399039799968401,0.0006779948404679695,0.0006125430421282848,0.00037095222311715287,-0.00012177725632985432,-0.0005885567904139558,-0.0002118583652190864,0.0014213222699860732,-0.001653036141457657,-0.0023347195625926056,-0.0031934548169374466,-0.0022419181962807975,-0.0014015748165547848,0.0002775967198734482,-0.0012181016306082408,-0.0017376419467230637,-0.001828232469658057,-0.003740829376814266,-0.002789806885023912,-0.0006975662933352093,-0.0030944112998743853,0.0018695471808314323,0.0016031723547105987,0.0010657520033419132,-0.002115407337745031,0.0008384319953620434,-0.0008508582056189576,0.00045636404926578206,0.0012556646252050996,0.0026307410250107446,0.0027736181703706584,0.002336698215610037,0.0029573495655010142,0.002480499679222703,0.0015830956496453534,0.0002270617502896736,0.00019214031635783613,-0.0008840021522094806,-0.0013608710141852498,-0.0010723532177507877,-0.0017831938651700814,-0.001546311192214489,-0.0011396905562529962,-0.001310172607190907,-0.0018952875398099422,0.00017137670268615088,-0.0010515637307738264,-0.001473673425304393,-0.0017071224671478074,-0.0016965296817943454,-0.0017158411452934768,-0.0015560615186889966,-0.001733989454805851,-0.0024530028846735754,-0.002533229310453559,-0.0012434844005232055,-0.002765680954325944,0.0004989326892731091,-0.0004371964799550672,-0.0018083898661037285,-0.000019145819048086803,-0.0016677773091942072,0.0031597001046369164,0.0014496019575744867,0.0014532012088845174,-0.001997011170412103,0.0012065150464574497,0.0008613818790763617,0.0005808230489492416,-0.0006256536968673269,-0.00026235093052188557,0.0006096417394777139,-0.0012023205248018105,0.001022458386917909,-0.0010394924320280552,-0.0035790212374801436,-0.0007768068462610245,-0.0018083666606495778,0.00041649855362872284,0.0020689070224761963,0.000720377778634429,0.00167456001508981,0.0020501536394779882,0.0013577221931579213,0.001978995220270008,0.0023191134290148816,0.0016790948381337027,0.003147809145351251,0.0027652805438265204,0.002205005381256342,0.0028424222643176713,0.0013227551923288654,0.0026859656597177186,0.001563509499343733,0.0008932152607788643,0.0014412240125238895,0.0008650811699529489,-0.00031985947862267494,-0.00046716958361988265,-0.0017524488115062316,-0.0014684816511968772,-0.0007459938836594423,-0.0009089912976681566,-0.0015658622529978554,-0.001793181834121545,-0.0028741261921823025,-0.0019712759337077537,-0.0024247418623417616,-0.0021967393501351276,-0.00026054323340455693,-0.0002684833016246557,-0.0004421609143416087,-0.000491144290814797,-0.0011155080477086206,-0.0008289156636844078,-0.000978409021627158,-0.0010952081065624952,-0.0010683771106414497,-0.0008619386935606599,-0.0006875560890572766,-0.0006044293210531274,-0.0006222299610575041,-0.0006873979582451284,-0.000558455086623629,-0.00045657309237867594,0.00014603200058142343,0.00040422892197966576,0.0003582988477622469,0.00044263352174311876,0.0007275910660003623,0.0014532465332498152,0.000562634765325735,-0.00016163336113095284,0.00010161598523457845,-0.0010285803582519293,-0.0005018728164335092,-0.001062731258571148,-0.0005458678545740744,-0.002571599635606011,-0.004598630437006553,-0.0014023616095073521,-0.004804471352448066,-0.0025750594213604927,-0.00540366272131602,-0.005529445751259725,0.00028070492650537443,0.0012902025288591783,0.0010024151221538584,0.0020703054033219814,0.002182912779971957,0.0021218234517922006,0.003824486668842534,0.0036611060301462808,0.005228835778931777,0.006218710138152043,0.004818565367410581,0.005191998245815436,0.003852225374430418,0.003045528933095435,0.001844770119835933,0.0023198018316179514,0.002203926599274079,0.0003417512246718009,0.00032488786382600665,0.00006580569121676187,-0.00008549490788330634,-0.00014706551640604934,0.0003287738994307195,0.00042056880677895,0.000027635078974223386,0.0000214977753785206,0.0001290188568721836,0.00006853187611947457,0.00006790406314394204,0.00017491066440319022,0.00020945109038924178,0.00022123579401522875,0.00016614420261854926,0.000176711551224192,-0.00020087088826888552,-0.00013631201970080534,-0.00032358083020274836,-0.00047054335785408813,-0.00043248181949214387,-0.0007569497489991287,-0.0009750402144466838,0.00019303212563196817,-0.0008499392521722863,-0.001128783100284636,-0.0009125347084288175,-0.0007550884814312061,-0.0004426077551518877,-0.0001930776828279098,0.00042017741361632943,0.000837466247806636,0.0015077950180663418,0.0018299047369509935,0.0021880599670112133,0.0021857895286908993,0.002616694033955961,0.002530762809328735,0.0026374256631243043,0.002922785507204632,0.002353960085504999,0.0026291337174673877,0.0019289754951993625,0.0015301001355207215,-0.0010645862591142456,0.0009741987256954113,0.0008807828805098931,-0.00013711499438310662,-0.00013246006953219572,-0.000652220235982289,-0.0008953866587641338,-0.001369532032792146,-0.0016996357978011172,-0.0023423737535874047,-0.003035748222221931,-0.0026756874285638332,-0.0021096155202637115,-0.0019433400593698025,0.0008274229476228356,0.0013224811603625615,-0.0006212859104077021,-0.0020493740836779275,-0.0014555839200814564,-0.00230604720612367,-0.0016271422306696575,-0.00011558302988608678,0.00010274257510900497,0.0002269006217829883,0.00028630567248910666,0.0000915863395979007,-0.0002273510229618599,-0.0005082532685870925,-0.0008148757042363286,-0.0010193951893597841,-0.000830210434893767,-0.0012599962453047435,-0.0016993264434859157,-0.0018254209620257218,-0.0016863687972848613,-0.0017505551222711802,-0.0016496772101769845,-0.002830896604185303,-0.000028760405257344246,-0.000012063595931977034,-0.0005113308628400167,-0.0004897295342137417,-0.00041861665279914934,-0.0008723632781766355,-0.0008570309728384018,-0.001006552018225193,-0.0019067647905709844,-0.0019361623950923483,-0.0021198532291843244,-0.0030207454498546817,-0.002137266448698938,-0.001423320888231198,-0.0020461935394754014,-0.0011967244014764826,-0.00068289740011096,-0.0001951086645325025,-0.00014534018312891325,0.001267496554646641,-0.0027173674025107175,0.001219254030729644,0.002086106687784195,0.00020646713771081218,0.0007946648935709769,-0.00016141498538975915,-0.0005591284667995448,-0.000044395875496168934,-0.000777290357897679,-0.00008798569130400817,-0.0011871775592832516,-0.0004548731667455286,-0.0005752170109190047,-0.0012901618999118607,-0.00027719543625911075,-0.00008325234133129318,-0.00007747579365968704,0.00009563289737949769,0.001813320491540556,0.0015998327871784568,0.0032159207524576536,0.002666240014756719,0.0074830392841249704,0.008228944561172588,0.00723962268481652,0.007779165481527646,0.008007395702103773,0.006965186320788537,0.006591003203842168,0.006656616926193237,0.0053854404638210935,0.004482726256052653,0.0033061670449872813,0.0035237035093208155,0.0019577682639161744,0.002148056208776931,0.0010088060128812988,0.0009547128962973753,0.0003286197315901518,-0.0008556536243607601,-0.0008663824604203304,-0.0022618960744390884,-0.001985288690775633,-0.003224496729671955,-0.0016523975258072217,-0.00013596723632266125,-0.000286774704970109,0.00017850756800423065,0.00004675658419728279,0.000670108439711233,0.0005909842050944766,0.000794629154067176,0.0009530445119404855,0.001535698810281853,0.0012855419772677124,0.0017661773599684238,0.00190646194096189,0.0014831282411857198,0.0019352364906808361,-0.0004836607937856267,-0.003647030641635259,-0.0032219247271617255,-0.0024227015674114227,-0.0018680874879161518,-0.0006120663213854035,-0.0003988451013962428,0.0007466932681078712,0.0005698216458161672,0.0012027531241377194,0.0004705640021711588,0.0003740685060620308,-0.0006608118225509921,-0.0011225756412992876,0.0012247512737909954,-0.0010375802715619404,-0.00010885181836783886,0.0014544428947071235,-0.001335747850437959,-0.0008730599462675551,-0.0036848351980249086,0.00004277044596771399,-0.00034219585359096527,-0.000012997460241119066,0.0013924607386191685,0.0025572550172607103,0.0015891511769344409,0.003558095447563877,0.001957264185572664,0.002540762555630257,0.002171579903612534,0.0018722767708823085,0.000680124193119506,0.00001529674045741558,-0.0017990476529424388,-0.0014834075312440593,-0.00040456132652858895,-0.0004828101567303141,-0.00044565934998293716,0.0002621085538218419,-0.00014586702794379866,0.00012906497674218068,0.00014634886611020193,0.00017510761972516775,0.0000998381947283633,0.00005550431645436523,-0.00002611934360174928,-0.000017721683737666655,0.0001691568516738092,0.0001797407700602586,0.0006005124499400457,0.0007413976903383931,0.0010521829050655167,0.0009697214506256083,0.0015474968046570818,0.001072955026756972,0.001638188085053116,0.0009826301441838343,0.0012672899756580591,0.0003197386395186186,0.0002445844778170188,-0.00007289601489901543,0.00015016627730801702,5.304502944151561e-6,0.0005960684890548388,0.000711048332353433,0.001601661245028178,0.002196369537462791,0.004334951130052407,0.0043326242205997305,0.00419936329126358,0.005581969890045002,0.005572066642343998,0.004498989476511876,0.0040370057977270335,0.0031901107480128608,0.0017551475514968236,0.0010126008419319987,0.000641587384355565,-0.00053530252383401,-0.00040435988921672106,-0.0009262702272584041,-0.0005552868048350016,0.00034013074279452365,0.0013987397930274408,-0.002002800174523145,-0.002570201021929582,-0.003175144665874541,-0.003186940603579084,-0.0029380008733520904,-0.003183391566077868,-0.00221577698054413,-0.002512368063131968,-0.001689201220870018,-0.0022163476484517255,-0.0010507697394738595,-0.0013577211648225784,-0.0015852657767633598,-0.0024986891075968742,-0.0012220063557227452,-0.0026537434314377606,-0.002254225934545199,-0.0037693561268194267,-0.0043474213453009725,-0.003668468135098616,0.002326425320158402,0.0015766227928300698,0.0011740870346936088,0.0004573274248590072,-0.00015786228080590567,0.00031128013506531715,-0.0007217388677721223,0.00041219405829906464,-0.0005317111499607563,0.0005492633790709078,-0.000709299580194056,0.000010226950204620758,-0.0008904278511181474,-0.00019316026009619236,-0.0011611826873073976,-0.0000696734447653095,-0.00015220470959320664,-0.00029390859223591786,-0.00010961873340420425,-0.0002902801303813855,-0.0002460067723101626,-0.00013518500297019878,-0.00003682315582409501,-0.000015563952426115673,0.00011922003856549661,0.00013522862112343623,-6.570791204770406e-6,0.0000397312930241848,-0.000024769493999580543,-0.000053865136578679085,-0.00004223291762173176,-0.0001543261071977516,-0.00021839572582393885,-0.00004736694972962141,-0.00009171656953791778,0.00006727567415509839,0.0001403704130401214,-0.000715563694636027,-0.00007631815969944,-0.0003072861582040787,0.0011774767190217972,0.0012958794832229614,0.0031023438399036727,0.0012322785332798958,0.0037712352350354195,0.0015195147134363651,0.0031375541196515164,0.0019023101776838303,0.0011916263028979301,0.0007857806049287319,0.00014566286699846387,-0.0001297137544800838,-0.00012346104873965183,-0.0005613230944921573,-0.0008152449736371636,-0.001138971822607952,-0.0012471745333944757,-0.0013803277009477217,-0.004672608648737271,-0.005401742644608021,-0.004724775130550067,-0.004959302954375744,-0.004125858967502912,-0.004210385183493297,-0.002746117922166983,-0.0025061044531563916,-0.00012603035429492593,-0.0008185187665124735,0.0011854635085910559,0.00039519478256503743,0.0018267746393879254,0.0007264227606356144,0.001382689457386732,0.000527561700437218,0.0008233597812553247,-0.000116962978305916,0.0005351471287819246,0.00028513434032599133,0.00021745212143287063,0.0010068610620995362,0.001236092687274019,-0.0013661263510584831,-0.002734139251212279,-0.0021575549617409706,-0.0036787629748384156,-0.0026109980729719004,-0.003559515345841646,-0.002420483467479547,-0.0030979541673635444,-0.0024321175878867507,-0.0032938920776359737,-0.002616965677589178,-0.0025993591795365014,-0.0023609447137763104,-0.0022496944293379784,0.0015846085734665394,0.0011986388417426497,0.0013306409333987783,0.00011775239060322444,-0.0001046970331420501,-0.0004131776125480731,-0.001046946117033561,-0.001292883651331067,-0.001620557911034363,-0.0014277494240862627,-0.0014913103077560663,-0.0010876180216049154,-0.0011912960229286302,-0.0005644796959434947,0.002076624116549889,-0.00016468961257487535,0.001094429773123314,0.000389853841625154,0.0006436839078863462,0.0007550413138233125,0.000835505702222387,-0.0003885468274044494,-0.0003317654482088983,0.00019495130982249975,0.0003128946215535204,0.0011120653944090009,0.00019509570362667242,0.0014032968319952488,-0.00013747462071478367,0.0011382466424644615,-0.0002644893053608636,0.00034514178211490315,0.00030607681643838686,0.000282429081077377,0.0002535696839913726,-0.00020593909236292043,-0.0007045199939360222,-0.0006834151378522316,-0.0005400016282995542,0.0007500489009544253,0.0004892748159666856,0.001220281022445609,0.0009453059756197035,0.0006419680818604926,0.0006031388960157832,0.00021492346422746778,0.00013670492141197124,0.00004843174489603067,-0.00031932084433113533,-0.00018125073984265327,-0.000785378972068429,-0.0009511726287504038,-0.0010974671070774396,-0.0014618773323794205,-0.0007928220244745413,-0.001739400438964367,-0.0007246960934329157,-0.0007647147091726462,-0.0002073659561574459,-0.0002453331059465806,-0.0005414070716748635,-0.0006764042967309555,0.0022257379411409297,0.0003335269478460153,-0.00006365733376393716,0.0003266492470478018,-0.00022456352598965168,-0.0001855228426090131,-0.00033778464421629906,-0.00037406043459971744,-0.0007322502788156271,-0.00016035900140802065,-0.00033212251340349513,-0.00025456073247672367,0.0007457424265642961,0.0008076352532953024,0.0013535598603387673,0.0010756405148034294,0.0007579649488131205,0.0009552184492349625,0.0006987314360837141,0.0005699452788879474,0.0009365850128233433,0.0002660338068380952,-0.0006833867325137059,-0.0009507846552878618,-0.0032471888698637486,-0.0020530889742076397,-0.004424075906475385,-0.0028925239263723292,-0.0033772405392179885,-0.0035706861138654253,-0.003795015625655651,-0.00393497000914067,-0.0036979916815956435,-0.002996809819402794,-0.0032229070008421936,0.001021618644396464,0.0012511181024213631,0.0009514208601710076,0.0010234518946769338,0.0003150097618345171,0.0003333143152606984,-0.00032288716950764257,-0.0002726016682572663,-0.0017224967402095597,-0.0025859788293018937,-0.0032352651081358394,-0.0031426607941587767,-0.0035938456809769073,-0.0032638382787505784,-0.0032847289306422076,-0.003106115541110436,-0.002352675888687372,-0.0015583070926368237,-0.001985289001216491,-0.0018806188988188903,-0.0017432222763697307,-0.0023606823136409125,-0.0018069338984787464,-0.003312478307634592,-0.000201563680699716,-0.0004931236617267132,-0.0005481257297409078,-0.0005318949988577515,-0.0003329233732074499,-0.0003346734350391974,-0.00017098312188560763,-0.00013596510204176107,-0.00016198291753729185,-0.0002050223023009797,-0.00025346882951756317,-0.000044559361413121223,-0.00021577537215004364,-0.000054813906899653375,-0.000012876039060453573,0.00009104182633260886,0.00007255313297112782,0.00040829226297015947,0.0007957948837429285,0.0012226522667333484,-0.005447344641045977,0.0025245535653084517,0.0027588334244986377,0.002639145590364933,0.002490804142629107,0.0021496887784451246,0.0020240988427152238,0.001252895997216304,0.0018071781378239393,0.0011713921558111906,0.0010910282532374065,0.0010662161124249299,0.0005018690135329962,0.0009516887366771698,-0.0035801673463235297,-0.0034166087862104177,-0.00347153233209004,-0.003446614369750023,-0.0034314479368428388,-0.004194892244413495,-0.006609741530458753,-0.007017658402522405,-0.003962982329539955,-0.005064888469253977,-0.005033782838533322,-0.005051381187513471,-0.004556327514971296,-0.004111886102085312,-0.004372676097167035,-0.003115297993645072,-0.003399432054720819,-0.00308807659894228,-0.0035220620532830558,-0.00319565685155491,-0.003519830720809599,-0.0030417765180269876,-0.0032845341484062374,-0.0019597246622045836,-0.002591099745283524,-0.0010996082176764805,-0.0014864294595705967,-0.0003436753201337221,0.0002405306634803613,0.0007750275350796679,-0.008494023078431686,-0.001967495928208033,-0.0019412691084047158,-0.002107783375928799,-0.0014873959201698501,-0.0013106950015450518,-0.0009466301028927168,-0.0006762736787398657,-0.0004616302127639453,-0.0005342536314856261,-0.000645060713092486,-0.0006809391779825091,-0.0008457417910297712,-0.0006863407324999571,-0.0011225527462859948,0.00019172166260735443,-0.0004486851248657331,-0.00017870167115082344,-0.00008873777308811744,-3.543270092147092e-6,0.00019165462193389735,0.00006723563516667734,0.0005492797936312854,0.0005216590943746269,0.0005780990468338132,0.0011258421000093222,0.0010027273674495518,0.0011631996118618797,0.0010363322702081252,-0.006254665864010652,-0.004890472705786427,-0.005637806879046063,-0.004541699153681596,-0.007157127217700084,-0.004787651511530082,-0.0070278212369885296,-0.0019290106914316614,-0.0020767198875546455,-0.0019640361424535513,-0.0018342874633769195,-0.0026689573423936963,-0.0013856443692930043,-0.002810087016162773,-0.001717383274808526,-0.0028147926665648506,-0.0018312262351779889,-0.002159241548118492,-0.0014771004280191846,-0.0019900744009646587,-0.0011956543118382494,-0.0006664696071917812,0.00042695200924451154,0.0002927631721831858,0.00041688304433288675,0.00025718090425167855,0.0002190927528621008,0.00011201755842193961,5.497411621036008e-6,0.000027187144344983,-0.00002541404925674821,-2.0721060233578705e-6,-7.1988822962036165e-6,6.809324380204392e-6],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y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z\":[0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.57254905,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.5803922,0.56078434,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5254902,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.49803922,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.52156866,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.3372549,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.49019608,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5019608,0.5058824,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.5372549,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49411765,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.5411765,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.49803922,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.4862745,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.4862745,0.49019608,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.52156866,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.50980395,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5529412,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.4862745,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49019608,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5176471,0.49411765,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.5372549,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.5019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.50980395,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.5058824,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4627451,0.4862745,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5647059,0.45882353,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.40784314,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.6,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.5529412,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.5137255,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5019608,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.59607846,0.34117648,0.34117648,0.34117648,0.34117648,0.34117648,0.34117648,0.34117648,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5137255,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.58431375,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5764706,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.5686275,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5921569,0.5529412,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.5254902,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.6117647,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.3647059,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.5803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.6117647,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.5882353,0.46666667,0.43137255,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.627451,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.41960785,0.42352942,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.47843137,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.62352943,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.58431375,0.54901963,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.5568628,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.41568628,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.3882353,0.7529412,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5176471,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.4117647,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.57254905,0.5764706,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.40392157,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.38039216,0.38039216,0.38039216,0.38039216,0.38039216,0.38039216,0.38039216,0.38039216,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.44705883,0.7882353,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.54509807,0.4509804,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.3764706,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"Channel\"}},\"zaxis\":{\"title\":{\"text\":\"Signal\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"SHAP Importance\"}},\"colorscale\":[[0.0,\"#000004\"],[0.1111111111111111,\"#1b0c41\"],[0.2222222222222222,\"#4a0c6b\"],[0.3333333333333333,\"#781c6d\"],[0.4444444444444444,\"#a52c60\"],[0.5555555555555556,\"#cf4446\"],[0.6666666666666666,\"#ed6925\"],[0.7777777777777778,\"#fb9b06\"],[0.8888888888888888,\"#f7d13d\"],[1.0,\"#fcffa4\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"4D EMG SHAP Visualization\"}},                        {\"responsive\": true}                    ).then(function(){\\n                            \\nvar gd = document.getElementById(\\'1e198563-72c9-48c0-891f-e214352cac57\\');\\nvar x = new MutationObserver(function (mutations, observer) {{\\n        var display = window.getComputedStyle(gd).display;\\n        if (!display || display === \\'none\\') {{\\n            console.log([gd, \\'removed!\\']);\\n            Plotly.purge(gd);\\n            observer.disconnect();\\n        }}\\n}});\\n\\n// Listen for the removal of the full notebook cells\\nvar notebookContainer = gd.closest(\\'#notebook-container\\');\\nif (notebookContainer) {{\\n    x.observe(notebookContainer, {childList: true});\\n}}\\n\\n// Listen for the clearing of the current output cell\\nvar outputEl = gd.closest(\\'.output\\');\\nif (outputEl) {{\\n    x.observe(outputEl, {childList: true});\\n}}\\n\\n                        })                };                            </script>        </div>\\n</body>\\n</html>'}\n",
            "[INFO] Saved fallback HTML plot: 4D_EMG_SHAP_Visualization.html\n",
            "[INFO] Saved 4D SHAP surface plot to: shap_4d_surface.html\n",
            "[SHAP4D] Channel Variance: 0.0000\n",
            "[SHAP4D] Temporal Entropy: 2.2121\n",
            "[SHAP4D] Mutual Info: 0.2545\n",
            "[SHAP4D] PCA Alignment: 0.0000\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "Figure(640x480)\n",
            "[INFO] Saved SHAP heatmap to: shap_temporal_heatmap.png\n",
            "\n",
            "📊 Training baseline model for SHAP comparison...\n",
            "[INFO] Saved SHAP heatmap to: shap_heatmap_baseline.png\n",
            "\n",
            "🔍 Running ablation: shuffling SHAP-important segments...\n",
            "[Ablation] Accuracy post SHAP shuffle: 1.0000\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_histograms_impl.py:895: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n",
            "[SHAP Ablation] KL Divergence (Original vs Post-Ablation): nan\n",
            "/content/extddivesify/diversify/train.py:260: MatplotlibDeprecationWarning:\n",
            "\n",
            "The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "\n",
            "Figure(800x500)\n",
            "Figure(1000x500)\n",
            "[SHAP vs Confidence] Pearson Correlation: 0.2595 (p=0.469)\n",
            "Figure(600x500)\n",
            "\n",
            "🛠 Real-world Context: EMG classification can support gesture-based interfaces in prosthetics or rehabilitation systems, and insights from SHAP improve trust in deployed models.\n",
            "Figure(1200x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./data/ \\\n",
        "  --task cross_people \\\n",
        "  --test_envs 3 \\\n",
        "  --dataset emg \\\n",
        "  --algorithm diversify \\\n",
        "  --latent_domain_num 5 \\\n",
        "  --alpha1 5.0 \\\n",
        "  --alpha 0.1 \\\n",
        "  --lam 0.0 \\\n",
        "  --local_epoch 5 \\\n",
        "  --max_epoch 30 \\\n",
        "  --lr 0.01 \\\n",
        "  --output ./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01 \\\n",
        "  --enable_shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvXePZs8j3h0",
        "outputId": "275a2976-db51-4082-8395-0139ee0b63ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 2.0.2\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm:diversify\n",
            "alpha:0.1\n",
            "alpha1:5.0\n",
            "batch_size:32\n",
            "beta1:0.5\n",
            "bottleneck:256\n",
            "checkpoint_freq:100\n",
            "classifier:linear\n",
            "data_file:\n",
            "dataset:emg\n",
            "data_dir:./data/\n",
            "dis_hidden:256\n",
            "gpu_id:0\n",
            "layer:bn\n",
            "lam:0.0\n",
            "latent_domain_num:5\n",
            "local_epoch:5\n",
            "lr:0.01\n",
            "lr_decay1:1.0\n",
            "lr_decay2:1.0\n",
            "max_epoch:30\n",
            "model_size:median\n",
            "N_WORKERS:4\n",
            "old:False\n",
            "seed:0\n",
            "task:cross_people\n",
            "test_envs:[3]\n",
            "output:./data/train_output/act/cross_people-emg-Diversify-0-10-1-1-0-3-50-0.01\n",
            "weight_decay:0.0005\n",
            "enable_shap:True\n",
            "resume:None\n",
            "steps_per_epoch:10000000000\n",
            "select_position:{'emg': [0]}\n",
            "select_channel:{'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list:{'emg': 1000}\n",
            "act_people:{'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "num_classes:6\n",
            "input_shape:(8, 1, 200)\n",
            "grid_size:10\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "\n",
            "======== ROUND 0 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6404360533    \n",
            "1                0.4502171874    \n",
            "2                0.4584585130    \n",
            "3                0.3683220744    \n",
            "4                0.6071632504    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.4663226008     0.4508441091     0.0154785058    \n",
            "1                0.7191579342     0.7172326446     0.0019252730    \n",
            "2                0.8242744803     0.8234343529     0.0008401487    \n",
            "3                0.7568367124     0.7563217282     0.0005149910    \n",
            "4                0.9308152795     0.9303490520     0.0004662391    \n",
            "Counter({np.int64(1): 1275, np.int64(0): 1123, np.int64(4): 906, np.int64(3): 571, np.int64(2): 277})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "0                0.3351663649     0.5059692264     0.8411356211     0.8718689788     0.8391136802     0.7737743650     1.8344581127    \n",
            "1                0.2991127968     0.6845844984     0.9836972952     0.8631984586     0.8150289017     0.8050797401     3.6054222584    \n",
            "2                0.3101972342     0.7498507500     1.0600479841     0.8865606936     0.8304431599     0.7790903721     5.3732128143    \n",
            "3                0.2806151509     0.5965588689     0.8771740198     0.8747591522     0.8265895954     0.8103957472     7.4022059441    \n",
            "4                0.2808694839     0.6487420797     0.9296115637     0.8921001927     0.8323699422     0.7909037212     9.3983514309    \n",
            "\n",
            "======== ROUND 1 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.8303719759    \n",
            "1                0.6601614952    \n",
            "2                0.7230733633    \n",
            "3                0.5438936949    \n",
            "4                0.5622680783    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1344448328     0.7661948800     0.3682499230    \n",
            "1                1.1148890257     0.7254530191     0.3894360065    \n",
            "2                1.3906558752     0.8000824451     0.5905734301    \n",
            "3                1.3739595413     0.9210118651     0.4529476762    \n",
            "4                1.2669179440     0.8855267763     0.3813911378    \n",
            "Counter({np.int64(1): 1135, np.int64(0): 945, np.int64(4): 928, np.int64(3): 833, np.int64(2): 311})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "5                0.3226520717     0.5604815483     0.8831336498     0.9128131021     0.8670520231     0.7897223863     1.7820243835    \n",
            "6                0.2228206843     0.6407586932     0.8635793924     0.9159441233     0.8583815029     0.7749556999     3.5647358894    \n",
            "7                0.2662073970     0.5607950091     0.8270024061     0.8940269750     0.8506743738     0.7454223272     5.3324284554    \n",
            "8                0.2834327817     0.6985006928     0.9819334745     0.9279865125     0.8718689788     0.7974010632     7.1092524529    \n",
            "9                0.1882978380     0.5355290174     0.7238268852     0.8017822736     0.7398843931     0.6863555818     8.9066340923    \n",
            "\n",
            "======== ROUND 2 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7259122729    \n",
            "1                0.5850302577    \n",
            "2                0.6840108037    \n",
            "3                0.5216552615    \n",
            "4                0.4759189188    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.4744858742     1.1772953272     1.2971906662    \n",
            "1                1.4597871304     0.8679770231     0.5918100476    \n",
            "2                1.3341811895     0.8123376966     0.5218434930    \n",
            "3                1.1048767567     0.8189548850     0.2859218121    \n",
            "4                1.0573519468     0.7564184666     0.3009334803    \n",
            "Counter({np.int64(1): 1168, np.int64(4): 972, np.int64(3): 796, np.int64(0): 778, np.int64(2): 438})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "10               0.2652148008     0.7643235326     1.0295383930     0.9291907514     0.8526011561     0.7832250443     2.1996889114    \n",
            "11               0.2825681269     0.5222574472     0.8048255444     0.8964354528     0.8217726397     0.7442409923     4.0921928883    \n",
            "12               0.2774348259     0.7490245700     1.0264594555     0.9116088632     0.8352601156     0.7897223863     5.8802731037    \n",
            "13               0.2262633443     0.5398421288     0.7661054730     0.9405105973     0.8535645472     0.8121677496     7.6878981590    \n",
            "14               0.2193619609     0.6496459842     0.8690079451     0.9520712909     0.8651252408     0.7920850561     9.5185174942    \n",
            "\n",
            "======== ROUND 3 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7434871197    \n",
            "1                0.6670502424    \n",
            "2                0.5388706326    \n",
            "3                0.5630207658    \n",
            "4                0.4231255651    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3496932983     0.9473863840     0.4023069441    \n",
            "1                1.1474558115     0.7528890967     0.3945666850    \n",
            "2                1.1475405693     0.8710092306     0.2765313387    \n",
            "3                1.0529669523     0.7856649160     0.2673020363    \n",
            "4                1.2683351040     0.7713138461     0.4970212877    \n",
            "Counter({np.int64(1): 1084, np.int64(4): 913, np.int64(3): 842, np.int64(0): 711, np.int64(2): 602})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "15               0.1909096539     0.6340662837     0.8249759674     0.9561657033     0.8776493256     0.7737743650     1.7872231007    \n",
            "16               0.1376403570     0.5582183003     0.6958586574     0.9583333333     0.8660886320     0.7702303603     3.8558673859    \n",
            "17               0.1527891159     0.6821759939     0.8349651098     0.9552023121     0.8680154143     0.7743650325     5.8710677624    \n",
            "18               0.2087922394     0.7278023362     0.9365946054     0.8998073218     0.8477842004     0.7135262847     7.6834428310    \n",
            "19               0.2230655998     0.8633964658     1.0864620209     0.9585741811     0.8603082852     0.7932663910     9.5031034946    \n",
            "\n",
            "======== ROUND 4 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4277641475    \n",
            "1                0.3985738456    \n",
            "2                0.5335695744    \n",
            "3                0.4620889723    \n",
            "4                0.5163618326    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0609167814     0.7205268145     0.3403899670    \n",
            "1                1.9930353165     1.2292439938     0.7637913227    \n",
            "2                1.2959806919     0.8296594620     0.4663212001    \n",
            "3                0.9057488441     0.6714577675     0.2342911065    \n",
            "4                1.4595181942     0.9281800985     0.5313380361    \n",
            "Counter({np.int64(1): 1169, np.int64(4): 1004, np.int64(3): 824, np.int64(0): 630, np.int64(2): 525})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "20               0.1713407338     0.5638719201     0.7352126837     0.9725433526     0.8506743738     0.7696396929     1.7899253368    \n",
            "21               0.1246347204     0.6938142776     0.8184490204     0.9660404624     0.8699421965     0.7820437094     3.6075928211    \n",
            "22               0.1620329469     0.7455698848     0.9076028466     0.8740366089     0.7880539499     0.6958062611     5.4000127316    \n",
            "23               0.1912005395     0.7478774786     0.9390780330     0.9597784200     0.8381502890     0.7725930301     7.5962336063    \n",
            "24               0.1662761569     0.7016414404     0.8679175973     0.9766377649     0.8458574181     0.7649143532     9.4772343636    \n",
            "\n",
            "======== ROUND 5 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5027737617    \n",
            "1                0.4152048230    \n",
            "2                0.7463674545    \n",
            "3                0.3146103323    \n",
            "4                0.3445733488    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1152478456     0.7462598681     0.3689879775    \n",
            "1                1.5709190369     1.0791771412     0.4917418957    \n",
            "2                0.9835848808     0.6884272695     0.2951575816    \n",
            "3                1.3302769661     0.8349407911     0.4953361750    \n",
            "4                1.7978192568     0.9433526397     0.8544666171    \n",
            "Counter({np.int64(4): 1245, np.int64(1): 1098, np.int64(3): 826, np.int64(0): 543, np.int64(2): 440})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "25               0.1468874663     0.7811470628     0.9280345440     0.9776011561     0.8709055877     0.7909037212     1.8267726898    \n",
            "26               0.1121711731     0.7625423670     0.8747135401     0.9857899807     0.8766859345     0.7855877141     3.6288206577    \n",
            "27               0.0985441431     0.7488048077     0.8473489285     0.9759152216     0.8574181118     0.7495569994     5.4397330284    \n",
            "28               0.0691523924     0.6555581093     0.7247105241     0.9816955684     0.8651252408     0.7867690490     7.2440667152    \n",
            "29               0.0630298331     0.8363074660     0.8993372917     0.9682080925     0.8660886320     0.7737743650     9.4158217907    \n",
            "\n",
            "======== ROUND 6 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5903976560    \n",
            "1                0.4028506875    \n",
            "2                0.3828771114    \n",
            "3                0.4125387669    \n",
            "4                0.3042088449    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5432589054     1.1068652868     0.4363936484    \n",
            "1                1.2697407007     0.8959316611     0.3738090396    \n",
            "2                1.3216001987     0.8244054317     0.4971947372    \n",
            "3                0.8463883400     0.6384916306     0.2078967392    \n",
            "4                1.1062541008     0.9039610028     0.2022930384    \n",
            "Counter({np.int64(4): 1033, np.int64(1): 1007, np.int64(3): 924, np.int64(2): 649, np.int64(0): 539})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "30               0.0771555305     0.7444942594     0.8216497898     0.9812138728     0.8564547206     0.7595983461     1.9249641895    \n",
            "31               0.0754858255     0.7031250000     0.7786108255     0.9821772640     0.8535645472     0.7595983461     3.7862896919    \n",
            "32               0.0925434530     0.9177420735     1.0102854967     0.9665221580     0.8265895954     0.7477849970     5.5992307663    \n",
            "33               0.0702540874     0.6915846467     0.7618387341     0.9857899807     0.8420038536     0.7761370348     7.4106793404    \n",
            "34               0.1281382740     0.9264775515     1.0546158552     0.9657996146     0.8285163776     0.7554636740     9.2248952389    \n",
            "\n",
            "======== ROUND 7 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4595356882    \n",
            "1                0.3319693506    \n",
            "2                0.2716227472    \n",
            "3                0.4633376598    \n",
            "4                0.3521060050    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0885055065     0.7704151273     0.3180904090    \n",
            "1                1.5979950428     1.2143127918     0.3836821914    \n",
            "2                1.2011175156     0.7573412061     0.4437763691    \n",
            "3                1.2128661871     0.8576013446     0.3552648127    \n",
            "4                1.2570177317     1.0003232956     0.2566944659    \n",
            "Counter({np.int64(1): 1145, np.int64(4): 1016, np.int64(3): 794, np.int64(2): 692, np.int64(0): 505})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "35               0.0733445287     0.7105245590     0.7838690877     0.9657996146     0.8429672447     0.7501476669     2.0940835476    \n",
            "36               0.0870470181     0.6848369241     0.7718839645     0.9853082852     0.8603082852     0.7855877141     4.3588223457    \n",
            "37               0.0785455406     0.7134471536     0.7919926643     0.9783236994     0.8545279383     0.7708210278     6.2045497894    \n",
            "38               0.1072382033     0.8657329082     0.9729710817     0.9241329480     0.8034682081     0.7188422918     8.0402538776    \n",
            "39               0.1089930162     0.6751597524     0.7841527462     0.9617052023     0.8429672447     0.7542823390     10.3704769611   \n",
            "\n",
            "======== ROUND 8 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4845820367    \n",
            "1                0.3132491410    \n",
            "2                0.3402175903    \n",
            "3                0.2103525251    \n",
            "4                0.1588233411    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2719109058     0.9955997467     0.2763110995    \n",
            "1                1.0521819592     0.7849032283     0.2672787607    \n",
            "2                1.1255604029     0.7603723407     0.3651880622    \n",
            "3                1.2655559778     0.9553048015     0.3102511466    \n",
            "4                1.1223934889     0.8002048135     0.3221886754    \n",
            "Counter({np.int64(1): 1198, np.int64(4): 955, np.int64(3): 764, np.int64(2): 752, np.int64(0): 483})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "40               0.1323079467     0.6323294640     0.7646374106     0.9626685934     0.8294797688     0.7353809805     1.8520541191    \n",
            "41               0.0759604722     0.6850960255     0.7610564828     0.9542389210     0.8323699422     0.7241582989     4.0647482872    \n",
            "42               0.0606506430     0.6524482965     0.7130989432     0.8634393064     0.7466281310     0.6686355582     5.9917304516    \n",
            "43               0.0782129690     0.7506488562     0.8288618326     0.9701348748     0.8526011561     0.7666863556     7.8227102757    \n",
            "44               0.0444791503     0.6086647511     0.6531438828     0.9838631985     0.8448940270     0.7643236858     9.6581575871    \n",
            "\n",
            "======== ROUND 9 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4654708803    \n",
            "1                0.4212549031    \n",
            "2                0.4535264969    \n",
            "3                0.4067516923    \n",
            "4                0.1943987608    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0830960274     0.7853686810     0.2977273464    \n",
            "1                1.4320970774     1.0136141777     0.4184828699    \n",
            "2                1.3167353868     0.9777817130     0.3389537036    \n",
            "3                1.5192965269     1.1309254169     0.3883711100    \n",
            "4                1.9521423578     1.4958925247     0.4562498033    \n",
            "Counter({np.int64(2): 1661, np.int64(1): 990, np.int64(0): 639, np.int64(3): 476, np.int64(4): 386})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "45               0.1472146660     0.9373890758     1.0846037865     0.9780828516     0.8564547206     0.7590076787     1.8876426220    \n",
            "46               0.0695721433     0.9512963295     1.0208684206     0.9913294798     0.8795761079     0.7796810396     3.7028620243    \n",
            "47               0.0630273148     0.9142654538     0.9772927761     0.9937379576     0.8728323699     0.7867690490     5.6053755283    \n",
            "48               0.0443745218     0.9871827960     1.0315573215     0.9872350674     0.8468208092     0.7720023627     7.8306994438    \n",
            "49               0.0519477837     0.8793289065     0.9312766790     0.9910886320     0.8477842004     0.7684583579     9.6740498543    \n",
            "\n",
            "======== ROUND 10 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6252163053    \n",
            "1                0.5011071563    \n",
            "2                0.3754092157    \n",
            "3                0.3014819920    \n",
            "4                0.5041915774    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.2227177620     0.9588996768     0.2638180256    \n",
            "1                1.3286750317     0.9384924173     0.3901826739    \n",
            "2                1.3143616915     0.9204908609     0.3938708007    \n",
            "3                2.3791859150     1.9372614622     0.4419244528    \n",
            "4                1.1642415524     0.9084041715     0.2558374107    \n",
            "Counter({np.int64(2): 1381, np.int64(1): 882, np.int64(4): 788, np.int64(0): 650, np.int64(3): 451})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "50               0.0755576193     0.9393901825     1.0149477720     0.9862716763     0.8516377649     0.7542823390     1.8428938389    \n",
            "51               0.0751042292     1.0405861139     1.1156903505     0.9942196532     0.8554913295     0.7566450089     3.6620707512    \n",
            "52               0.0810028911     1.1067442894     1.1877472401     0.9942196532     0.8487475915     0.7643236858     5.5637755394    \n",
            "53               0.0403211303     1.0770007372     1.1173218489     0.9877167630     0.8429672447     0.7619610159     7.4140217304    \n",
            "54               0.0334187038     0.8155087233     0.8489274383     0.9718208092     0.8150289017     0.7371529829     9.6950497627    \n",
            "\n",
            "======== ROUND 11 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6660280228    \n",
            "1                0.3829033077    \n",
            "2                0.2951074541    \n",
            "3                0.2672902048    \n",
            "4                0.2864694893    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0451126099     1.3949881792     0.6501243114    \n",
            "1                1.1067736149     0.6977292299     0.4090444148    \n",
            "2                1.3013612032     1.0867810249     0.2145802081    \n",
            "3                1.4351698160     1.1007336378     0.3344361782    \n",
            "4                1.3129442930     0.9923606515     0.3205836713    \n",
            "Counter({np.int64(2): 1043, np.int64(1): 949, np.int64(3): 750, np.int64(0): 716, np.int64(4): 694})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "55               0.0741198882     0.9640369415     1.0381568670     0.9869942197     0.8227360308     0.7424689900     1.8393740654    \n",
            "56               0.0430255756     1.0106455088     1.0536711216     0.9879576108     0.8487475915     0.7430596574     3.6717677116    \n",
            "57               0.0308563150     0.9547842145     0.9856405258     0.9908477842     0.8304431599     0.7412876551     5.4901263714    \n",
            "58               0.0531648397     1.0346225500     1.0877873898     0.9857899807     0.8420038536     0.7619610159     7.3530185223    \n",
            "59               0.0569549799     1.0081889629     1.0651439428     0.9894026975     0.8352601156     0.7560543414     9.2001152039    \n",
            "\n",
            "======== ROUND 12 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5518574715    \n",
            "1                0.5514709949    \n",
            "2                0.3004976511    \n",
            "3                0.1774669886    \n",
            "4                0.1815576404    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.1980730295     0.9918504953     0.2062224895    \n",
            "1                1.2094737291     0.8744841218     0.3349896371    \n",
            "2                1.4277276993     0.9666298032     0.4610978663    \n",
            "3                1.1969971657     0.9581547976     0.2388423383    \n",
            "4                1.6516826153     1.1954739094     0.4562086761    \n",
            "Counter({np.int64(2): 1177, np.int64(1): 1049, np.int64(4): 803, np.int64(0): 636, np.int64(3): 487})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "60               0.1344113797     0.8597498536     0.9941612482     0.9891618497     0.8641618497     0.7672770230     2.2624597549    \n",
            "61               0.1141194925     0.9163017273     1.0304212570     0.9816955684     0.8583815029     0.7655050207     4.1915533543    \n",
            "62               0.0707454532     0.9203596711     0.9911051393     0.9959055877     0.8641618497     0.7690490254     6.0319333076    \n",
            "63               0.0488408320     0.9467557669     0.9955965877     0.9927745665     0.8574181118     0.7560543414     7.8850836754    \n",
            "64               0.0533051565     0.9207274914     0.9740326405     0.9605009634     0.8294797688     0.7365623154     9.7340970039    \n",
            "\n",
            "======== ROUND 13 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.3676463068    \n",
            "1                0.3342305720    \n",
            "2                0.2693575621    \n",
            "3                0.2713507712    \n",
            "4                0.2541202009    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.0943412781     0.8316899538     0.2626512945    \n",
            "1                1.8314274549     1.3749576807     0.4564697444    \n",
            "2                1.5207920074     1.2184189558     0.3023729920    \n",
            "3                1.3290262222     1.1043126583     0.2247136086    \n",
            "4                1.3435978889     1.0026028156     0.3409950137    \n",
            "Counter({np.int64(2): 1037, np.int64(4): 917, np.int64(1): 875, np.int64(3): 662, np.int64(0): 661})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "65               0.0392801911     0.9052587748     0.9445389509     0.9824181118     0.8400770713     0.7430596574     1.8521730900    \n",
            "66               0.1003934890     0.9971603751     1.0975538492     0.9867533719     0.8535645472     0.7601890136     4.1019999981    \n",
            "67               0.0849647149     1.0021308661     1.0870956182     0.9860308285     0.8574181118     0.7560543414     6.0257515907    \n",
            "68               0.0455431268     1.0554761887     1.1010192633     0.9761560694     0.8246628131     0.7288836385     7.8455212116    \n",
            "69               0.0450458229     0.9979373813     1.0429831743     0.9913294798     0.8526011561     0.7460129947     9.6669566631    \n",
            "\n",
            "======== ROUND 14 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6299360991    \n",
            "1                0.3140737116    \n",
            "2                0.3111423552    \n",
            "3                0.2465268970    \n",
            "4                0.2019330412    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7051374912     1.2079129219     0.4972245991    \n",
            "1                1.4283874035     1.1178262234     0.3105611205    \n",
            "2                1.2252550125     0.9485597014     0.2766953707    \n",
            "3                1.3205690384     1.1176811457     0.2028879225    \n",
            "4                1.4482028484     1.1384823322     0.3097205758    \n",
            "Counter({np.int64(4): 1024, np.int64(2): 942, np.int64(1): 909, np.int64(0): 664, np.int64(3): 613})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "70               0.0977599248     0.9125952721     1.0103552341     0.9891618497     0.8535645472     0.7548730065     1.8884227276    \n",
            "71               0.0485864170     0.8370662928     0.8856527209     0.9783236994     0.8294797688     0.7152982871     3.7177095413    \n",
            "72               0.0945281386     0.9423375130     1.0368657112     0.9824181118     0.8612716763     0.7406969876     5.7273433208    \n",
            "73               0.0262275077     1.0313794613     1.0576069355     0.9913294798     0.8545279383     0.7436503249     7.9324936867    \n",
            "74               0.0354448035     0.8206666112     0.8561114073     0.9925337187     0.8603082852     0.7460129947     9.7779057026    \n",
            "\n",
            "======== ROUND 15 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4443741739    \n",
            "1                0.2892244160    \n",
            "2                0.3203566074    \n",
            "3                0.1843378395    \n",
            "4                0.1570330858    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8396160603     1.2591593266     0.5804567933    \n",
            "1                1.9950587749     1.4761714935     0.5188872814    \n",
            "2                1.2050746679     1.0055295229     0.1995451599    \n",
            "3                1.4031127691     1.1675125360     0.2356002480    \n",
            "4                1.2562640905     0.8523957133     0.4038683474    \n",
            "Counter({np.int64(4): 1068, np.int64(2): 906, np.int64(1): 840, np.int64(0): 705, np.int64(3): 633})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "75               0.0975534990     1.1098426580     1.2073961496     0.9855491329     0.8342967245     0.7312463083     1.8499853611    \n",
            "76               0.0734171718     1.0319582224     1.1053754091     0.9942196532     0.8564547206     0.7613703485     3.6788985729    \n",
            "77               0.0450386554     0.8530207276     0.8980593681     0.9973506744     0.8477842004     0.7619610159     5.5320444107    \n",
            "78               0.0663246140     0.8747156262     0.9410402179     0.9906069364     0.8323699422     0.7477849970     7.4097084999    \n",
            "79               0.0309698898     0.8726720214     0.9036419392     0.9951830443     0.8323699422     0.7601890136     9.7404389381    \n",
            "\n",
            "======== ROUND 16 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4101008475    \n",
            "1                0.3853161335    \n",
            "2                0.3405269086    \n",
            "3                0.2898687422    \n",
            "4                0.2778227925    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.1201839447     1.3329497576     0.7872343063    \n",
            "1                1.1131683588     0.8670868874     0.2460814267    \n",
            "2                1.6469151974     1.3595887423     0.2873264253    \n",
            "3                1.1854572296     0.9687819481     0.2166752517    \n",
            "4                1.7571851015     1.3432136774     0.4139713943    \n",
            "Counter({np.int64(2): 964, np.int64(4): 902, np.int64(1): 772, np.int64(3): 766, np.int64(0): 748})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "80               0.0607973076     1.0448186398     1.1056159735     0.9898843931     0.8516377649     0.7631423509     1.8375589848    \n",
            "81               0.0495294407     1.1557838917     1.2053133249     0.9927745665     0.8420038536     0.7554636740     3.6770420074    \n",
            "82               0.0736561716     1.0385894775     1.1122456789     0.9934971098     0.8612716763     0.7613703485     5.5093834400    \n",
            "83               0.0363797843     1.0752414465     1.1116212606     0.9824181118     0.8285163776     0.7353809805     7.3360576630    \n",
            "84               0.0566885024     1.2750762701     1.3317648172     0.9874759152     0.8439306358     0.7377436503     9.2049436569    \n",
            "\n",
            "======== ROUND 17 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5907478333    \n",
            "1                0.3299719691    \n",
            "2                0.3209241331    \n",
            "3                0.2660079896    \n",
            "4                0.2276678085    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4578514099     1.0126776695     0.4451737106    \n",
            "1                1.7578072548     1.3183150291     0.4394922853    \n",
            "2                1.3967387676     1.0301947594     0.3665439785    \n",
            "3                1.4941998720     1.1979856491     0.2962141931    \n",
            "4                1.9667729139     1.5365282297     0.4302446842    \n",
            "Counter({np.int64(2): 1360, np.int64(3): 773, np.int64(0): 724, np.int64(1): 701, np.int64(4): 594})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "85               0.0392178781     1.1847840548     1.2240018845     0.9932562620     0.8593448940     0.7525103367     2.2557759285    \n",
            "86               0.0272280127     1.1599855423     1.1872135401     0.9913294798     0.8660886320     0.7513290018     4.2164394855    \n",
            "87               0.0592152774     1.1766448021     1.2358601093     0.9173892100     0.7707129094     0.6875369167     6.0785903931    \n",
            "88               0.0371210687     1.2085411549     1.2456622124     0.9963872832     0.8660886320     0.7708210278     7.9258654118    \n",
            "89               0.0345296338     1.2302933931     1.2648230791     0.9942196532     0.8622350674     0.7584170112     9.7957093716    \n",
            "\n",
            "======== ROUND 18 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.7462691069    \n",
            "1                0.3492658138    \n",
            "2                0.3072077930    \n",
            "3                0.3025254011    \n",
            "4                0.3242919445    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4964225292     1.2355245352     0.2608979940    \n",
            "1                1.8418862820     1.3461103439     0.4957758784    \n",
            "2                1.4917464256     1.2093185186     0.2824279070    \n",
            "3                1.6141308546     1.2782958746     0.3358349502    \n",
            "4                1.8360172510     1.6595045328     0.1765127331    \n",
            "Counter({np.int64(2): 1118, np.int64(3): 908, np.int64(1): 848, np.int64(0): 643, np.int64(4): 635})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "90               0.0292621013     1.0917556286     1.1210176945     0.9761560694     0.8593448940     0.7483756645     1.8371453285    \n",
            "91               0.0165705979     1.1763483286     1.1929188967     0.9845857418     0.8583815029     0.7436503249     4.1151022911    \n",
            "92               0.0375670195     1.1972007751     1.2347677946     0.9944605010     0.8477842004     0.7690490254     6.0650367737    \n",
            "93               0.0197477490     1.1717737913     1.1915215254     0.9915703276     0.8545279383     0.7507383343     7.9431777000    \n",
            "94               0.0146028334     1.2049994469     1.2196022272     0.9954238921     0.8516377649     0.7625516834     9.7837529182    \n",
            "\n",
            "======== ROUND 19 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5318230391    \n",
            "1                0.4129571021    \n",
            "2                0.2965959609    \n",
            "3                0.3985212743    \n",
            "4                0.3329799175    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5466748476     1.1465319395     0.4001428783    \n",
            "1                1.7679898739     1.3775300980     0.3904597461    \n",
            "2                1.9093242884     1.5927340984     0.3165901601    \n",
            "3                1.7515401840     1.3533041477     0.3982360363    \n",
            "4                1.3570011854     1.1510341167     0.2059670687    \n",
            "Counter({np.int64(2): 1246, np.int64(1): 852, np.int64(3): 771, np.int64(4): 659, np.int64(0): 624})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "95               0.0649835169     1.1736987829     1.2386822701     0.9884393064     0.8516377649     0.7554636740     1.8279936314    \n",
            "96               0.0214586537     1.2028913498     1.2243499756     0.9867533719     0.8545279383     0.7519196692     3.6933083534    \n",
            "97               0.0142517230     1.1342039108     1.1484556198     0.9978323699     0.8564547206     0.7548730065     5.8594019413    \n",
            "98               0.0577520765     1.2498011589     1.3075532913     0.9881984586     0.8381502890     0.7241582989     7.8412780762    \n",
            "99               0.0428468883     1.1057101488     1.1485570669     0.9939788054     0.8391136802     0.7277023036     9.6682448387    \n",
            "\n",
            "======== ROUND 20 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4869080484    \n",
            "1                0.3792448342    \n",
            "2                0.2430875152    \n",
            "3                0.2991883755    \n",
            "4                0.2543957531    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8856095076     1.3364595175     0.5491499901    \n",
            "1                1.7734966278     1.4187833071     0.3547132909    \n",
            "2                1.6838500500     1.3292258978     0.3546242118    \n",
            "3                1.8050553799     1.4654031992     0.3396521211    \n",
            "4                1.4624238014     1.1716610193     0.2907628119    \n",
            "Counter({np.int64(2): 1622, np.int64(1): 753, np.int64(3): 707, np.int64(0): 570, np.int64(4): 500})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "100              0.0878383592     1.2216119766     1.3094503880     0.9877167630     0.8400770713     0.7607796810     1.8237533569    \n",
            "101              0.0368386544     1.2030825615     1.2399212122     0.9978323699     0.8564547206     0.7702303603     3.6486146450    \n",
            "102              0.0527450107     1.3393024206     1.3920474052     0.9055876686     0.7369942197     0.6680448907     5.4744901657    \n",
            "103              0.0628187209     1.0672122240     1.1300309896     0.9385838150     0.8053949904     0.6999409333     7.2928631306    \n",
            "104              0.0247511324     1.2502508163     1.2750020027     0.9927745665     0.8448940270     0.7595983461     9.5568258762    \n",
            "\n",
            "======== ROUND 21 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.5134119987    \n",
            "1                0.3677787483    \n",
            "2                0.2484923601    \n",
            "3                0.2955744863    \n",
            "4                0.3039018512    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5913469791     1.1485507488     0.4427962005    \n",
            "1                1.6298142672     1.2790530920     0.3507612050    \n",
            "2                1.9107179642     1.4342805147     0.4764374793    \n",
            "3                1.5437767506     1.2650505304     0.2787262499    \n",
            "4                1.2695624828     1.0491522551     0.2204102725    \n",
            "Counter({np.int64(2): 1531, np.int64(1): 786, np.int64(3): 707, np.int64(4): 591, np.int64(0): 537})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "105              0.0598227791     1.2041184902     1.2639412880     0.9966281310     0.8583815029     0.7631423509     1.8470263481    \n",
            "106              0.0391894840     1.2685687542     1.3077582121     0.9701348748     0.8208092486     0.7377436503     3.6545450687    \n",
            "107              0.0358702242     1.2734715939     1.3093417883     0.9937379576     0.8439306358     0.7454223272     5.5059516430    \n",
            "108              0.0462912507     1.2127439976     1.2590352297     0.9812138728     0.8314065511     0.7377436503     7.3323786259    \n",
            "109              0.0327537246     1.1524306536     1.1851843596     0.9956647399     0.8477842004     0.7666863556     9.1758685112    \n",
            "\n",
            "======== ROUND 22 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6055202484    \n",
            "1                0.3718140125    \n",
            "2                0.2527921498    \n",
            "3                0.2486363500    \n",
            "4                0.2343401611    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8407605886     1.3832825422     0.4574780166    \n",
            "1                1.7416087389     1.3979152441     0.3436934948    \n",
            "2                1.5392738581     1.3308759928     0.2083978653    \n",
            "3                1.6345841885     1.4190565348     0.2155276388    \n",
            "4                1.3472088575     1.1669363976     0.1802724600    \n",
            "Counter({np.int64(2): 1715, np.int64(1): 708, np.int64(3): 622, np.int64(4): 580, np.int64(0): 527})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "110              0.0812134743     1.2452614307     1.3264749050     0.9898843931     0.8506743738     0.7666863556     2.2440373898    \n",
            "111              0.0302671511     1.2591416836     1.2894088030     0.9648362235     0.8102119461     0.7223862965     4.2016391754    \n",
            "112              0.0131995957     1.1959234476     1.2091230154     0.9951830443     0.8564547206     0.7466036621     6.0596261024    \n",
            "113              0.0437417179     1.1948531866     1.2385948896     0.9867533719     0.8429672447     0.7501476669     7.9045176506    \n",
            "114              0.0365341380     1.0670779943     1.1036121845     0.9990366089     0.8526011561     0.7590076787     9.7625403404    \n",
            "\n",
            "======== ROUND 23 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4171639383    \n",
            "1                0.4170199335    \n",
            "2                0.3388846517    \n",
            "3                0.3613504469    \n",
            "4                0.2191888392    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.4260149002     1.1731528044     0.2528620958    \n",
            "1                1.9140496254     1.3448095322     0.5692400932    \n",
            "2                1.4854279757     1.2182688713     0.2671590745    \n",
            "3                1.6454371214     1.4731670618     0.1722700596    \n",
            "4                1.5270916224     1.3510422707     0.1760493517    \n",
            "Counter({np.int64(2): 1653, np.int64(1): 704, np.int64(4): 639, np.int64(3): 604, np.int64(0): 552})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "115              0.1284878254     1.2665616274     1.3950494528     0.9951830443     0.8564547206     0.7625516834     1.8679423332    \n",
            "116              0.0182750989     1.2718561888     1.2901313305     0.9922928709     0.8554913295     0.7619610159     4.0960764885    \n",
            "117              0.0502474196     1.2441418171     1.2943892479     0.9848265896     0.8516377649     0.7389249852     6.1208007336    \n",
            "118              0.0417151153     1.2718439102     1.3135590553     0.9920520231     0.8400770713     0.7454223272     7.9675421715    \n",
            "119              0.0931839794     1.3025349379     1.3957189322     0.9959055877     0.8554913295     0.7554636740     9.8145878315    \n",
            "\n",
            "======== ROUND 24 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4375751019    \n",
            "1                0.2557802498    \n",
            "2                0.3301789463    \n",
            "3                0.2024746239    \n",
            "4                0.2484593838    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.5550074577     1.2419887781     0.3130187094    \n",
            "1                1.7037820816     1.4201940298     0.2835880816    \n",
            "2                1.6450376511     1.4579106569     0.1871269643    \n",
            "3                1.4610211849     1.3003810644     0.1606401205    \n",
            "4                1.4124259949     1.2629646063     0.1494613737    \n",
            "Counter({np.int64(2): 1272, np.int64(1): 810, np.int64(3): 713, np.int64(4): 703, np.int64(0): 654})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "120              0.0317803919     1.2259219885     1.2577023506     0.9841040462     0.8468208092     0.7371529829     1.8208503723    \n",
            "121              0.0376263037     1.3049755096     1.3426017761     0.9959055877     0.8574181118     0.7483756645     3.6369776726    \n",
            "122              0.0196358133     1.3653807640     1.3850165606     0.9641136802     0.8063583815     0.7265209687     5.6487364769    \n",
            "123              0.0300527401     1.3344335556     1.3644863367     0.9966281310     0.8420038536     0.7395156527     7.7474999428    \n",
            "124              0.0324622951     1.3817993402     1.4142615795     0.9985549133     0.8526011561     0.7560543414     9.5671002865    \n",
            "\n",
            "======== ROUND 25 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.6102521420    \n",
            "1                0.2579030097    \n",
            "2                0.4678181708    \n",
            "3                0.1614055932    \n",
            "4                0.1894587278    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.7995328903     1.3622028828     0.4373299479    \n",
            "1                1.9633486271     1.5650751591     0.3982734084    \n",
            "2                1.5103133917     1.3673006296     0.1430127174    \n",
            "3                1.7180474997     1.5588593483     0.1591881514    \n",
            "4                1.4861979485     1.3376128674     0.1485851407    \n",
            "Counter({np.int64(2): 1051, np.int64(1): 819, np.int64(4): 798, np.int64(3): 750, np.int64(0): 734})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "125              0.0345603712     1.3785661459     1.4131264687     0.9966281310     0.8516377649     0.7796810396     1.8334980011    \n",
            "126              0.0592075475     1.3432081938     1.4024157524     0.9959055877     0.8410404624     0.7714116952     3.6812245846    \n",
            "127              0.0295762103     1.3945645094     1.4241406918     0.9973506744     0.8410404624     0.7613703485     5.5286707878    \n",
            "128              0.0878299102     1.3945329189     1.4823628664     0.9944605010     0.8448940270     0.7513290018     7.3730647564    \n",
            "129              0.0305635966     1.3245583773     1.3551219702     0.9896435453     0.8246628131     0.7330183107     9.6855380535    \n",
            "\n",
            "======== ROUND 26 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4666134715    \n",
            "1                0.2683725953    \n",
            "2                0.2177486867    \n",
            "3                0.3751848042    \n",
            "4                0.1904054582    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3953180313     1.1692206860     0.2260974050    \n",
            "1                1.7753140926     1.4573742151     0.3179398775    \n",
            "2                1.8498632908     1.5437599421     0.3061034083    \n",
            "3                1.3814723492     1.2297368050     0.1517356038    \n",
            "4                1.7127735615     1.5522731543     0.1605003476    \n",
            "Counter({np.int64(2): 1050, np.int64(1): 887, np.int64(3): 768, np.int64(0): 750, np.int64(4): 697})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "130              0.0532672405     1.3051007986     1.3583680391     0.9836223507     0.8400770713     0.7566450089     1.8831455708    \n",
            "131              0.0428221561     1.2197746038     1.2625967264     0.9978323699     0.8381502890     0.7383343178     3.6966676712    \n",
            "132              0.0926333889     1.4123841524     1.5050175190     0.9968689788     0.8323699422     0.7489663320     5.5160682201    \n",
            "133              0.0323606245     1.2831680775     1.3155287504     0.9920520231     0.8410404624     0.7365623154     7.3110275269    \n",
            "134              0.0718358234     1.3295655251     1.4014014006     0.9869942197     0.8381502890     0.7466036621     9.1178600788    \n",
            "\n",
            "======== ROUND 27 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.3784574270    \n",
            "1                0.2948208451    \n",
            "2                0.1796488166    \n",
            "3                0.2190932035    \n",
            "4                0.1909725666    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.3858177662     1.2303886414     0.1554290950    \n",
            "1                1.4975748062     1.3466354609     0.1509393305    \n",
            "2                1.5351212025     1.3519406319     0.1831806153    \n",
            "3                1.5583295822     1.3762114048     0.1821181476    \n",
            "4                1.4787802696     1.2768304348     0.2019498646    \n",
            "Counter({np.int64(2): 1345, np.int64(0): 777, np.int64(3): 684, np.int64(1): 678, np.int64(4): 668})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "135              0.0434240401     1.3265038729     1.3699278831     0.9884393064     0.8381502890     0.7513290018     2.1741762161    \n",
            "136              0.0372920893     1.2399598360     1.2772519588     0.9983140655     0.8477842004     0.7507383343     4.1106324196    \n",
            "137              0.0312927514     1.2951221466     1.3264149427     0.9971098266     0.8429672447     0.7536916716     5.9263064861    \n",
            "138              0.0160329193     1.3289532661     1.3449862003     0.9867533719     0.8275529865     0.7377436503     7.7526443005    \n",
            "139              0.0838110149     1.2288547754     1.3126658201     0.9831406551     0.8236994220     0.7093916125     9.5665240288    \n",
            "\n",
            "======== ROUND 28 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4572335780    \n",
            "1                0.2835714519    \n",
            "2                0.2901476920    \n",
            "3                0.2325855494    \n",
            "4                0.1510089040    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.0658013821     1.6555577517     0.4102435410    \n",
            "1                1.6705076694     1.4319678545     0.2385398149    \n",
            "2                1.6704843044     1.4789929390     0.1914913654    \n",
            "3                1.4577910900     1.3033075333     0.1544835120    \n",
            "4                1.6803961992     1.4613320827     0.2190641165    \n",
            "Counter({np.int64(2): 1369, np.int64(4): 768, np.int64(0): 731, np.int64(1): 676, np.int64(3): 608})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "140              0.0719820708     1.2872083187     1.3591903448     0.9942196532     0.8535645472     0.7395156527     1.8120167255    \n",
            "141              0.0190510061     1.2588083744     1.2778593302     0.9939788054     0.8612716763     0.7371529829     3.7054221630    \n",
            "142              0.0353544354     1.2020149231     1.2373692989     0.9879576108     0.8371868979     0.7294743060     5.9229316711    \n",
            "143              0.0938690677     1.3022081852     1.3960772753     0.9942196532     0.8429672447     0.7359716480     7.7700095177    \n",
            "144              0.0783110261     1.3002462387     1.3785572052     0.9987957611     0.8487475915     0.7489663320     9.5746877193    \n",
            "\n",
            "======== ROUND 29 ========\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                0.4073671103    \n",
            "1                0.2799221873    \n",
            "2                0.2623788416    \n",
            "3                0.2421851903    \n",
            "4                0.1734956503    \n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.8049014807     1.3318095207     0.4730919302    \n",
            "1                1.7385822535     1.4668745995     0.2717076838    \n",
            "2                1.6221863031     1.2888803482     0.3333059251    \n",
            "3                1.8023266792     1.4907869101     0.3115397990    \n",
            "4                1.4895330667     1.3627126217     0.1268204898    \n",
            "Counter({np.int64(2): 1045, np.int64(4): 885, np.int64(1): 805, np.int64(0): 709, np.int64(3): 708})\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time \n",
            "145              0.0940940604     1.3438185453     1.4379125834     0.9918111753     0.8535645472     0.7347903131     1.8126852512    \n",
            "146              0.0312743299     1.4006186724     1.4318929911     0.9963872832     0.8564547206     0.7430596574     3.6150152683    \n",
            "147              0.0248462111     1.3870612383     1.4119074345     0.9968689788     0.8429672447     0.7347903131     5.4032423496    \n",
            "148              0.0172122475     1.3932985067     1.4105107784     0.9901252408     0.8246628131     0.7265209687     7.6448135376    \n",
            "149              0.0147303110     1.3218843937     1.3366147280     0.9983140655     0.8477842004     0.7442409923     9.5804955959    \n",
            "\n",
            "🎯 Final Target Accuracy: 0.7797\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "[WARN] Adjusting flat_inputs from 1600 to 9600\n",
            "/content/extddivesify/diversify/shap_utils.py:47: FutureWarning:\n",
            "\n",
            "The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "\n",
            "[SHAP] Accuracy Drop: 0.0000\n",
            "[SHAP] Flip Rate: 0.0000\n",
            "[SHAP] Confidence Δ: -0.1327\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 1.6249\n",
            "[SHAP] Coherence: 0.0469\n",
            "[SHAP] Jaccard: 0.0000\n",
            "[SHAP] Kendall’s Tau: -0.0385\n",
            "[SHAP] Cosine Sim: -0.0388\n",
            "Signal shape before reshape: (8, 1, 200)\n",
            "SHAP value shape before reshape: (8, 1, 200, 6)\n",
            "{'text/html': '<html>\\n<head><meta charset=\"utf-8\" /></head>\\n<body>\\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: \\'local\\'};</script>\\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4428005a-4958-4d18-926e-89ade4e0c24a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4428005a-4958-4d18-926e-89ade4e0c24a\")) {                    Plotly.newPlot(                        \"4428005a-4958-4d18-926e-89ade4e0c24a\",                        [{\"hovertemplate\":\"Time=%{x}\\\\u003cbr\\\\u003eChannel=%{y}\\\\u003cbr\\\\u003eSignal=%{z}\\\\u003cbr\\\\u003eSHAP Importance=%{marker.color}\\\\u003cextra\\\\u003e\\\\u003c\\\\u002fextra\\\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0.0000970452213853908,0.0003118494738979886,0.0004153167634891967,0.0006911590074499449,0.0012401570565998554,0.00043721563027550775,0.0008666151746486624,0.000801078689012987,0.0008213770731041828,0.0003974992432631552,0.00003052801669885715,0.0002639695691565673,9.407522156834602e-6,0.0012499278721710045,0.002044686038667957,0.0023359419816794493,0.0025757578356812396,0.002207336093609532,0.0019078239953766267,0.000831254020643731,-0.0005258366776009401,0.0004696655087172985,0.00009153348704179128,0.0014188897039275616,0.001257744540149967,0.0008873623252535859,0.0010939075921972592,0.0006317900066884855,0.0002251261321362108,0.0006449479454507431,0.0014714257170756657,-0.00007372318456570308,0.0008060761416951815,0.0003399766671160857,0.0018841233104467392,0.0019393169010678928,0.0007377563742920756,0.0011267053390232225,0.0012325964635238051,0.0008236705713594953,0.0018066160846501589,0.0015099853044375777,0.0013928696668396394,0.0009957478614524007,-0.000038261525332927704,-0.000789180863648653,-0.0028202716882030168,-0.003587657275299231,-0.003053837145368258,-0.002129439807807406,-0.0004756706766784191,0.0007576825252423683,0.0020597257340947786,0.0014297036298861106,0.0014951568640147646,0.0010543873844047387,0.0005395434175928434,0.0010152865822116535,0.0017305219468350213,0.0016873287968337536,0.0014917297133555014,0.0018265724647790194,0.0019568746987109384,0.0020907827420160174,0.0017281644977629185,0.0011443584226071835,0.0014908293960615993,0.0013124356434370081,0.0024043219164013863,0.003804519772529602,0.0032163997335980334,0.0012233989691594616,0.0005968601908534765,-0.00028664319931219023,-0.002145849478741487,-0.0015286240231944248,-0.0013069408014416695,-0.0013876029018623133,-0.0006312397114622096,-0.0006523455182711283,-0.00013530855479378565,0.00014601034733156362,0.00015628950980802378,-0.0002023221750278026,-0.0005863294936716557,0.000190939133365949,-0.0005523664876818657,0.0003215366353591283,0.0007185736709895233,0.0003329590811821011,0.0004877344084282716,0.0007471651770174503,-0.00037063565105199814,-0.0008561137753228346,-0.0014745845983270556,-0.001283398092103501,-0.0013564700008525203,-0.0007473948644474149,-0.001044732208053271,-0.00028518606753398973,0.0012376243248581886,0.00005105494831999143,0.00036945214378647506,-5.211137856046359e-6,0.00047184328044143814,0.0008847550100957354,0.0011017971005458094,0.0017018812553336222,0.0015396616654470563,0.001886906645571192,0.002338899163684497,0.002833307875941197,0.0028977695231636367,0.003676803898997605,0.0029253594111651182,0.0024023956114736698,0.001284110980729262,-0.0000633801876877745,-0.0007942494315405687,-0.0003636916517280042,-0.000049350472788016,0.0011461891311531265,0.001662636020531257,0.0013947519570744287,0.001350268643970291,-0.00004369969246909022,0.0009146932667742173,0.0006133412631849448,8.086111241330703e-7,-0.00020654934147993723,-0.003397780004888773,-0.002752123099829381,-0.005006092910965283,-0.003332793712615967,-0.0016746789527436097,-0.0014010731441279252,0.0015162267760994534,0.0019822303341546408,0.002908945238838593,0.0005210649687796831,0.0009266571141779423,0.0006718280104299387,-0.0021052485972177237,-0.0005293256338821569,-0.001853575685042112,-0.0012961551547050476,-0.0009539779275655746,-0.0005910674420495828,0.000630758935585618,0.0014283711013073723,0.001551141186306874,0.002010845385181407,0.00166656996589154,0.002002054068725556,0.0012492829361387219,0.001457618937517206,-0.00014703146492441496,-0.0001946218932668368,0.0008765192081530889,0.000719572805489103,0.0008624358257899681,0.001159299553061525,0.00221143647407492,0.0013525357159475486,0.003046576244135698,0.0024855673934022584,0.00446642159173886,0.004901630260671179,0.005602417862974107,0.0038482331049939,0.003138916023696462,0.0011849754955619574,-0.0005530831404030323,-0.00021750980522483587,-0.0003770962357521057,-0.0007528732530772686,-0.001098018450041612,-0.0012895776114116113,-0.00022684534390767416,0.0010295983714361985,0.0013132259870568912,0.0007766927592456341,0.0006909518657873074,-0.00012650631833821535,-0.0008408028030923257,-0.002024987518476943,-0.0019904403015971184,-0.0009837465671201546,-0.0010326670793195565,0.0007223868548559645,0.0006219590917074432,0.0019901061120132604,0.0018696233649582912,0.0012310435631661676,0.00040268416826923686,-0.00009189965203404427,-0.00023558131821725206,-0.00023566886860256395,-0.00029979398095747456,0.00001848486620777597,0.00001723609420878347,0.00008643557763813685,0.0001816657604649663,0.0004487553572592636,0.0005871435472120842,0.0004351689082492764,0.0004941447793195645,0.0005979510024189949,0.0004997939104214311,0.00039224176240774494,0.000636417263497909,0.0005248710513114929,0.00046098983148112893,0.0000612714405482014,0.00009320816025137901,0.00018797489368201545,-0.00004818705686678489,0.0002304373774677515,-2.847673992315928e-6,0.00008944841101765633,-0.00013559799602565667,0.0005406257696449757,0.00016722610841194788,0.00036392686888575554,-0.0006910067362089952,-0.0009350417725120982,-0.0012152037234045565,0.001463965279981494,0.0014346623793244362,0.0020696939900517464,0.002462166671951612,0.0034093836632867656,0.002840136798719565,0.0019314871169626713,0.0007810569756353895,0.00025179268171389896,-0.00007160139891008536,-0.0002150117652490735,0.0011323568566391866,0.0008398346447696289,0.0016879682273914416,0.0009034400184949239,0.0003217709599994123,-0.00014642767685775956,-0.0008694864809513092,-0.00042929576011374593,-0.0004742982673633378,-0.00012177289560592423,0.0010400657386829455,0.0004493089218158275,0.0017658542686452468,0.0014829153660684824,0.0011315409404536088,0.0005479621856162945,0.00023735411620388427,-0.00017043858921776214,-0.00012173977059622605,0.0006809061111804718,0.0005283448942160854,0.0008146434168641766,0.0006758057570550591,0.00025159089515606564,0.0002872697077691555,-0.00001582861295901239,-0.00015936426158683995,-0.0008019571347783009,0.0004596476986383398,0.0013958826505889494,0.0017496209281186263,0.0025535382640858493,0.0006394633091986179,-0.00020899976758907238,-0.00005893657604853312,-0.00024180316055814424,-0.0005590083116355041,0.0006668322409192721,0.0011655440709243219,0.001349609810858965,0.0009479390185636779,0.0006018998877455791,0.00034912322492649156,0.0003058841684833169,-0.0006142936181277037,-0.0008103307724619905,-0.0005399030342232436,-0.00016045701340772212,-0.0002958528930321336,0.000014148089879502853,0.0006340452043029169,0.00111641072241279,0.0008832882934560379,0.0013187566073611379,0.0006396876997314394,0.0007502903463318944,-0.00010634500843783219,-0.0002529007979319431,-0.0007853948045521975,-0.0011439872711586456,-0.0008763896767050028,-0.0004641609702957794,-0.00008467344256738822,-0.00019969841620574394,0.00040970882400870323,0.0009625509070853392,0.001457982095113645,0.001582557645936807,0.0008676471188664436,0.0008531427301932126,0.0011729404989940424,0.0003308404702693224,-0.00008490933881451686,-0.00046175667860855657,0.0007829816895537078,0.0013201566665278126,0.0017121653072535992,0.0012325446338460704,0.000884686402666072,0.00025724237396692234,0.00017079663424131772,0.00034483229683246464,0.00046884617768228054,0.0005636869076018532,-0.0019226579461246729,-0.002578688809686961,-0.0031236689731789133,-0.002399376879717844,-0.003369301867981752,-0.0018380760836104553,-0.0024501464795321226,-0.0006213580879072348,-0.002486088157941898,-0.0014990562049206346,-0.0018200621319313843,-0.00038294897725184757,-0.000027226400561630726,0.00010085818939842284,-0.00005870220290186504,-0.000674519314391849,-0.000956000837807854,-0.0015693208357940118,-0.0009082412968079249,-0.0011667016272743542,-0.0005586478897991279,0.0009118294498572747,0.0007632760486255089,0.000259377130229647,-8.692344029744467e-6,-0.000857138613355346,-0.0006928871152922511,-0.0010747956112027168,-0.000730480377872785,-0.0004971409992625316,-0.00017902918625622988,-0.0001661369363622119,-0.0000530641603594025,-0.00032995811974008876,-0.0005719189163452635,-0.0007642207201570272,-0.0007723772820706168,-0.0005570217229736348,-0.0003129691249341704,0.000017758536462982494,0.0003469900693744421,0.001145716446141402,0.0008485365542583168,0.0007946732997273406,0.0005327061953721568,-0.00020651678399493298,-0.00028456780516232055,0.00007392533007077873,0.00014234447735361755,-0.00002506515011191368,-0.0002555030320460598,-0.0005704184877686203,-0.00031578292449315387,-0.0004996169979373614,-0.00017967467041065296,-0.00008377099099258582,0.0000603807857260108,-0.00016489640499154726,-0.0013608264659220974,-0.00036013762777050334,-0.00025478921209772426,0.0005470290780067444,0.00022744744395216307,0.00020135093169907728,-0.0007771028516193231,-0.0006969251941579083,-0.000685393849077324,-0.0004404820113753279,-0.00029455470697333414,-0.00021169290024166307,0.0001048602644004859,0.00014630154085656008,0.00045552902156487107,0.00019074206162864962,-0.00018305260164197534,-0.00030402707246442634,0.000384248691261746,-0.00022583692043554038,0.00006143278854627472,0.00010598916560411453,0.00016307526918050522,0.00015725040308704288,0.00036382263836761314,0.00042400632810313255,0.00047827667246262234,0.0004849813024823864,0.0005608605812691773,0.0005335679694932575,0.00045187076708922785,0.0005531107017304748,0.00022515074427550039,0.00021055040512389192,0.00014457541207472482,0.0005517678121880939,0.0008376485978563627,0.0001910310141587009,-0.002164647875664135,-0.0035760890847692886,-0.0029379312957947454,-0.0009705081271628538,-0.002426772223164638,-0.0032067845265070596,-0.0031252033853282533,-0.0025556513379948833,-0.002513146998050312,0.00173170135046045,0.0018804259210204084,0.0019986328261438757,0.002507726817081372,0.002720990994324287,0.0012033992291738589,-0.00031960181271036464,-0.0013996864824245374,-0.002494871849194169,-0.001464404941846927,-0.0005832332341621319,0.0011255044179658096,0.001803146170762678,0.0016137271498640378,0.001304086297750473,-0.001025961401561896,-0.0010059563792310655,-0.0018282268041123946,-0.0024150338140316308,-0.0025562235387042165,-0.0017154428035913345,-0.00003055029083043337,0.0006450020397702853,0.0013723887968808413,0.0008007021388038993,0.00045894028153270483,-0.0004595713956708399,-0.0008922392347206672,-0.000861750760426124,-0.0012542980257421732,-0.0007898748541871706,-0.0009867551270872355,-0.00045037753200934577,-9.43971099331975e-6,0.00003546744119375944,0.00009435553996202846,-0.00034285740305980045,-0.0004492926721771558,-0.001016535796225071,-0.0003630160354077816,0.00005700995582931986,-0.00016493319223324457,-0.00025201378351387876,-0.0010442553805963446,-0.001268606409818555,-0.0014498616413523753,-0.0010638570723434289,-0.00041534639118860167,-0.000039443684120972954,0.00039546069456264377,0.0006923055819546183,0.0007924320525489748,0.0007139406710242232,0.00022053252905607224,0.0005113100633025169,0.00001229565047348539,-0.0002254605060443282,-0.0006987536326050758,-0.00026755624761184055,0.00025759901230533916,0.00014612875141513845,-0.0003940755656609933,-0.00032085446097577613,-0.0005106922859946886,-0.0002609060223524769,-0.00014054966353190443,-0.0003009604406543076,-0.00011157151311635971,-0.00013111036969348788,0.000017021142411977053,0.0001766002387739718,0.0008001261545966069,0.0015153259349366028,-0.0017712665721774101,-0.0018058020311097305,-0.0012397028040140867,-0.0014179683445642393,-0.0012338976763809721,-0.0016857977025210857,-0.0013469588690592598,-0.001696303952485323,-0.0003992215885470311,-0.0013214774274577696,1.0904235144456227e-6,-0.0018805672104159992,-0.0016274137888103724,-0.0025680922456861786,-0.0011149767475823562,-0.0033091554262985787,-0.0015554530546069145,-0.0019658395322039723,-0.00223670721364518,-0.0015107394428923726,-0.0004745343079169591,0.00015834412382294735,-0.0005775169314195713,-0.0009637823483596245,-0.0006293329934123904,-0.000792325435516735,-0.0007701752134986842,-0.0004475101789770027,-0.0007298010944699248,-0.00033560397181039053,-0.0012042070738971233,-0.0009234138900258889,-0.0009514812651711205,-0.00014899492574234804,0.0006523189464739213,0.000791072923069199,0.0007939334706558535,0.00007601222023367882,-0.00020507785181204477,-0.00005916126732093593,-0.00028629659209400415,-0.00003555358853191137,-0.0007989893395764133,-0.00002617386053316295,-0.0005631160941751053,-0.0001685459865257144,-0.0004499367860262282,-0.0005368622490398897,-0.00046037492575123906,-0.0006811773249258598,-0.0008505398630707836,-0.0005042563037325939,-0.0005014699224072198,-0.00003154253742347161,0.00020516500808298588,-0.00008040179576103886,-0.00009617803152650595,5.316202683995167e-7,0.0000613607505025963,-0.0007401138233641783,-0.00011557336741437514,0.000023138786976536114,0.0001941824060243865,0.00017386069521307945,-0.0002533506291608016,0.00012116366997361183,-0.00025879021268337965,-0.00004073594269963602,0.00039583168108947575,0.00033940535892422,0.0013397529643649857,0.0006620056325724969,0.0012050522879386942,0.0005447365886842211,0.000966504584842672,0.0008509935869369656,0.0007838991684063027,0.0006426565426712235,0.0008441937776903311,0.0008642594330012798,0.0007500931387767196,0.0005471973321012532,-0.00005333983184148868,-0.0001169451279565692,0.0003181589612116416,0.0008765804183591778,0.001171080434990775,0.0007853456772863865,0.0008558239011714855,0.0006868673954159021,0.0006930522698288163,0.0008003565453691408,0.0006455888894076148,0.00008575306856073439,0.000146079168189317,0.00019664412441973886,-0.00019422774494159967,-0.000042971912383412324,-0.00004106603349403789,-0.001006396011992668,0.00019037526120276502,0.00019116893357325657,-0.00003936809904795761,-0.0002743527681256334,-0.0007377581981321176,0.00025813844695221633,0.00047058204654604197,0.0003920599604801585,-0.0000899751903489232,-0.0007066613373657068,-0.0010176203989734252,-0.0014431597276901205,-0.001581755777200063,-0.00039471817823747796,-0.0007067976985126734,-0.0005693209823220968,-0.0014532276351625721,-0.001673929626122117,0.008166494236017266,0.012927398085594177,0.007401302767296632,0.02264716352025668,0.025518186390399933,0.01894943423879643,0.0001810723915696144,-0.010909787068764368,-0.0039055291563272476,0.008931831456720829,0.0069451093052824335,0.011922957530866066,0.015104440351327261,0.018665576353669167,0.0212978340156648,0.01795664196833968,0.01985427923500538,0.007399816686908404,0.005610308920343717,-0.002620067447423935,-0.001956130067507426,-0.006282518307367961,-0.010219984066983065,-0.0011316877789795399,-0.0009048574138432741,0.00004955070714155833,-0.0003282865509390831,0.0003263736143708229,0.0003395585808902979,-0.00011926372584033136,0.00007859710603952408,0.0002418889586503307,-0.0004998476943001151,-0.00022459720882276693,-0.00040157162584364414,0.00011853554072634627,-0.00035232464627673227,-0.00010446210702260335,0.0005419941347402831,-0.00011323203216306865,0.0004813031603892644,-0.0012283357015500467,-0.001403393146271507,-0.001220034862247606,-0.0019362601451575756,-0.0018843444995582104,-0.002210373825316007,0.0013967057069142659,-0.0028954357840120792,-0.001308352026777963,-0.0041446681910504895,-0.003808617281417052,-0.0033011143774880716,-0.0026246580916146436,-0.0032211360909665623,-0.0011923284425089757,-0.003981896831343572,-0.002087210387495967,-0.0032419495594998202,-0.0024512012799580893,-0.0032758666202425957,-0.001611380255781114,-0.0023092770328124366,-0.00264025138070186,-0.0029446046877031526,-0.002213806379586458,-0.0012583368613074224,0.00041267073053556186,0.0004292159768131872,0.0004627479550739129,0.0004702804144471884,0.0005848474878196915,0.0008252812937522928,0.00044029260364671546,-0.0000336901672805349,-0.00011074806873997052,-0.00020848005078732967,-0.00028766059161474306,-0.000811483293849354,-0.00012539998654877613,-0.001146157078134517,-0.0008263565638723472,-0.0006869372058038911,-0.0009090324165299535,0.0002136063218737642,-0.00039692050389324624,0.000011929543688893318,0.00001442870901276668,0.00040070854205017287,0.00050488573712452,0.00016952859004959464,-0.00018917726508031288,-0.001368447517355283,-0.0006515713951860865,-0.0013244164098675053,-0.00021110423646556833,-0.0013806208541306357,-0.0002701770863495767,-0.00015240370218331614,-0.00025999910819033783,-0.0010288685637836654,-0.0017204660301407178,-0.0028285010096927485,0.006215760173896949,-0.0008526230230927467,-0.00028938862184683484,-0.002470198320224881,-0.00107435019648013,-0.004025066349034508,-0.0015408955514431,-0.0043349177576601505,-0.0019018960883840919,-0.0047388803989936905,-0.0031191362844159207,-0.004666460328735411,-0.004026089212857187,-0.00599837931804359,-0.004000336319829027,-0.004814395176557203,-0.0026342299145956836,-0.003103572603625556,-0.0017665812144211184,-0.001439534865009288,-0.0006202445947565138,-0.00003388912106553713,0.00006184866651892662,-0.0028407042069981494,0.0012237871220956247,0.0011074709085126717,0.0009307971340604126,0.000027021820036073525,-0.0003808359809530278,-0.0016335557544759165,-0.0010742310745020707,-0.0021183158193404474,-0.0010732434845219057,-0.002319501480087638,-0.0014876339895029862,-0.002059044704462091,-0.0007358644700919589,-0.0009920312246928613,-0.0012793838007686038,-0.0019596835991251282,-0.0015093423426151276,-0.0020266250551988683,-0.000611758092418313,-0.0002311120042577386,0.0007371932151727378,0.0004618021969993909,-0.0007039566601937016,-0.0007542324795698127,0.00003015118030210336,-0.0007237433067833384,-0.00012003597415362795,-0.0015377198190738757,-0.0005163879541214556,-0.0016681495277831952,-0.0003121213521808386,-0.0017276955768465996,-0.0006400904482385764,-0.001488925578693549,-0.0007009537269671758,-0.0015427963808178902,-0.001652039277056853,0.00001077377237379551,0.0006392563227564096,0.00043495086720213294,0.0009462181866789857,0.00046479310064266127,0.0003770894933647166,0.0005635302513837814,0.0004977450395623843,0.00033516720092544955,0.0017851438218106825,0.0013771621646204342,0.0022439635261738053,0.0015782769381379087,0.0011217499462266762,0.0003897137939929962,0.000023226670843238633,-0.00042499820119701326,-0.0004532422171905637,0.00012190218937272827,0.0014320328288401167,0.0004445575177669525,-0.0008383057332442453,-0.0015066756556431453,0.0007249890671422085,0.003951620737401147,0.002356307542261978,0.0009209838851044575,-0.0008036984751621882,-0.00315230463941892,-0.003393646019200484,-0.0021674297749996185,0.0012880182669808467,-0.0004622635121146838,0.008337710052728653,0.0030705276876688004,0.014004200076063475,-0.036687241246302925,-0.016399105079472065,-0.020024314522743225,-0.004676092726488908,-0.0005340815211335818,0.006207931786775589,-0.008356908957163492,-0.003467732031519214,0.0006271352370580038,0.0005284983975191911,0.00019510548251370588,0.0007910446729511023,0.001554107681537668,0.0033119722114255032,0.0022393413043270507,0.002367670259748896,0.0029180015747745833,0.003503588493913412,0.0024384986221169433,0.0021292975482841334,0.00041774512889484566,0.0017885836617400248,0.0007056746787081162,0.0006064203723023335,-0.000817090078877906,0.004038174636662006,0.004278754815459251,0.0031053743635614714,0.00349350463754187,-0.001023888432731231,-0.0006023618722489724,-0.001422075554728508,0.001510170754045248,0.0045005517701307935,0.0038640468070904412,0.004991163305627803,0.001884530686462919,0.0029761301508794227,0.002045808359980583,0.0003440463915467262,-0.002303995114440719,-0.007774651205788056,-0.005367067254458864,-0.0030646398663520813,-0.0018218329641968012,-0.0014746100641787052,0.0020266543918599686,-0.0008962315817674001,0.0023695166843632856,0.003318275169779857,0.002726677805185318,0.00034134039500107366,0.0008799252100288868,-0.002678516631325086,-0.0028023874231924615,0.00007838243618607521,-0.006168808011958997,-0.000836918285737435,-0.005219055184473594,-0.004584751033689827,-0.006591111732025941,-0.0025895765672127404,-0.008266209935148558,-0.003832465037703514,-0.00485731599231561,0.0005941431348522505,0.001683055500810345,0.0013357804467280705,-0.00019882479682564735,0.0010922742076218128,0.0009994598610016208,0.000943462053934733,0.0012767743319272995,0.0011865991400554776,0.0003287787937248747,0.00001607586940129598,0.00023483558713148037,-0.000060811483611663185,-0.0005162686381178597,-0.0013629845343530178,-0.00033586456750830013,-0.004400931764394045,0.00736817220846812,0.004723353621860345,0.004237774546102931,0.012874739632631341,-0.0000515615101903677,-0.0033639160295327506,0.002558551806335648,-0.003346789628267288,0.004678566629687945,0.007213525474071503,0.0037617765677471957,0.006278582848608494,0.001805805911620458,-0.0008348154369741678,-0.0018514470818142097,-0.0028224202493826547,-0.003545610699802637,0.004720364425641795,0.009900058309237162,-0.0026724754522244134,0.012342127660910288,-0.004694521116713683,0.0022415369749069214,0.008780801047881445,0.0038451234189172587,0.00398646155372262,0.007427936769090593,-0.0002302828555305799,0.0004884620817999045,-0.012760195337856809,-0.00687167684858044,-0.00880797574063763,-0.00025450003643830615,-0.002866012044250965,-0.0013503200219323237,-0.003271819713215033,0.0017578564584255219,-0.004496469783286254,0.00023907826592524847,0.00826300859140853,0.004513767082244158,0.0028162333571041622,0.0029020399476091066,0.002417685774465402,0.000657740815465028,0.002495990445216497,-0.004112069689047833,-0.005274667715032895,-0.006302738135370116,-0.003763349183524648,0.0025527290999889374,-0.0006381596128145853,-0.0008543855510652065,-0.003666010064383348,-0.003550292031529049,-0.0044275209462891025,-0.008216436234458039,0.0016254428774118423,0.0058280392549932,0.0012096470842758815,-0.004026224836707115,0.0032407324761152267,0.004032158603270848,0.0027168852587540946,0.000568430017059048,0.010321755583087603,0.0019237104182442029,0.007437377547224362,-0.0001354332392414411,-0.006917371725042661,0.0010406328365206718,-0.005651284164438645,0.006147265434265137,-0.0005789011095960935,-0.004592825348178546,-0.003193622144560019,-0.018278454740842182,0.000696348298030595,-0.0008102158705393473,0.002327646128833294,0.005351328446219365,0.005121639619270961,0.0012680101208388805,-0.00036020507104694843,-0.0003072551917284727,-0.004079633081952731,-0.001407437026500702,-0.0012372767863174279,0.0009818991335729759,0.00374578715612491,0.0035410300673296056,0.0037578579892093935,0.003663412993773818,-0.001005014559874932,0.000631001079455018,-0.0003386956329147021,-0.0033249194966629148,-0.0005683459943005195,0.00011332391295582056,-0.0006291944591794163,0.00009558350817921261,0.00009047410033720855,-0.000047462633422886334,-0.000046321273354503013,-0.00014588698589553437,-0.000478576504974626,0.0008054018253460526,0.0006555965325484673,0.0004495529380316536,0.0003021406688882659,2.192799001932144e-6,-0.00009551628803213437,-0.0005584647102902333,-0.00021389324683696032,-0.000986315853272875,-0.0006371086249904087,-0.00077316954654331,-0.00024439980431149405,-0.00021995735975603262,0.0004640402039512992,0.00047362428934623796,0.0002520366882284482,0.00020567933097481728,0.0005028274608775973,0.0006923150504007936,0.0007983342899630467,0.0009321589022874832,-0.0008512897572169701,-0.00007561314851045609,-0.00029458075247627374,-0.00016845056476692358,0.00013956519736287495,0.001071444033489873,0.0006241800729185343,0.000836703305443128,0.0008459163060858069,0.00016962030592064062,0.0006141941218326489,0.00020305517440040907,-0.00029252946842461824,-0.000854789512231946,-0.0033883998015274606,-0.0038550448371097445,-0.0002682725025806576,-0.0005953444245581826,-0.0004127838146814611,-0.0004172672634012997,-0.0007106404518708587,-0.001097992683450381,-0.0011160419477770727,-0.0006688744761049747,-0.0008073671100040277,-0.0011260829245050747,-0.0005431166694809993,-0.0012410467024892569,-0.0010394015698693693,-0.0013949768617749214,-0.0012973384000360966,-0.001366648202141126,-0.0015551191463600844,-0.0016282589834493895,-0.0012443042360246181,-0.0009688331435124079,-0.0011618811792383592,-0.000794457271695137,-0.00003955579207589229,0.0006240804214030504,-0.00007176806684583426,0.00020097545348107815,0.0005298909575988849,0.0004123866868515809,-0.0004742154075453679,0.00024697052625318366,-0.0005321681965142488,-0.00020363647490739822,-0.00005690342125793298,-0.0001715842712049683,-0.0006063970892379681,-0.00020476620799551407,-0.00005571916699409485,-0.0010444989117483299,-0.0007507400975252191,-0.001073778063679735,-0.0010439831142624219,-0.0012067620797703664,-0.00031633271525303525,0.000540781378125151,0.0006192103028297424,0.0011508218400801222,0.0008901478722691536,0.0016234430174032848,0.0010358116899927456,0.0006816413855024924,0.0008397730998694897,0.00025356770493090153,-0.00024057393117497364,-0.00007371053410073121,-0.000347710563801229,-0.0004816710522087912,-0.0006728163522590572,-0.0008377036623035868,-0.0006280950619839132,-8.950553213556608e-6,0.00014879731073354682,-0.00030700082425028086,0.000059021132377286754,-0.00020681022094019377,-0.0005191072220137963,0.0002728118561208248,-0.0007761072677870592,0.00041382918910433847,-0.00011366372928023338,0.00001841116075714429,0.0002973842977856596,0.00034677844572191435,-0.00016030277765821666,0.0007229931458520392,-0.0005454199078182379,-0.0000798441469669342,-0.00004810418856019775,-0.00038106448482722044,0.00018890151598801216,-0.0008614995361616214,-0.00016361845579619208,-0.0008039543560395638,-0.0008535871747881174,-0.0010336615766088169,-0.00067012927805384,-0.0011409067083150148,-0.0009947800426743925,-0.0009064357727766037,-0.0005023123909874508,-0.0009484856564085931,-0.0007418546980867783,-0.0007749347520681719,-0.0009500409166018168,-0.0011231511792478461,-0.0010223115871970851,-0.0006313065144543847,-0.0007879165835523357,-0.0006781267778327068,-0.00033066264586523175,-0.000957345551190277,-0.0006547889594609538,-0.0006161176036888113,-0.0005240862568219503,-0.0006804625930575033,-0.0009173542882005373,-0.0011562211439013481,-0.0011448017127501469,-0.0012551578499066334,-0.0018241207969064515,-0.0019653075529883304,-0.0023626620726039014,-0.002394882933003828,-0.0028882321203127503,-0.0030627746406632164,-0.0027557603704432645,-0.0024111771878475943,-0.001502129714936018,-0.0012047935742884874,0.0002881263887199263,-0.002705296967178583,-0.002290672001739343,-0.0028881837303439775,-0.0009161553656061491,0.0004760781691099207,0.0006058796619375547,-0.0010791216045618057,-0.001112706648806731,-0.0012555698243280251,-0.00029574350143472355,0.0017247647047042847,0.00135379812369744,0.001798297589023908,0.000751300094028314,-0.0015824166281769674,-0.00018517327650139728,-0.0019921710093816123,-0.00020944620094572505,-0.0004036217578686774,-0.0003327273443574086,-0.0012724462430924177,-0.0010833432897925377,-0.0011961602916320164,-0.0008040721683452526,-0.00017868366558104753,-8.791452273726463e-6,-0.00035944257009153563,-0.0005949654926856359,-0.0007963470222118,-0.0005466492536167303,-0.0005931796428437034,-0.00012981961481273174,-0.0005636205654203271,-0.000260647531831637,-0.0001898221962619573,-0.00048090830629613873,-0.00016268428104619184,-0.000026721002844472725,-0.00003842817386612296,0.000021396386728156358,0.00004024363685554514,0.00002020536824905624,0.00006682436408785482,0.00024697524107371766,0.00034228047904131625,0.00030650522482271,0.00035063408116305556,0.0004258629633113742,0.0005442824525137743,0.00017730986777072152,0.000025988750470181305,-0.000030400386701027553,-0.0002624577997873227,-0.0002081518565925459,0.00003163993824273348,-0.0004214496196558078,-0.0017069766763597727,-0.0008166610496118665,-0.0009560209388534228,-0.0006753252819180489,-0.0006003171826402346,-0.0008105704522070786,-0.000942764338105917,-0.0018806110601872206,-0.0014053612637023132,0.0006664529209956527,0.00006086301679412524,-0.00008647163243343432,-0.0001042477476100127,0.00010237749665975571,0.0003569030959624797,0.0003191418557738264,0.0002008321462199092,0.00042660262746115524,-0.00018326965315888324,-0.00032630048614616197,-0.0008704051142558455,-0.00018832884961739182,-0.0004176824780491491,0.00004735701562215885,-0.00020745916602512202,0.000029157691945632298,0.00008204703529675801,0.00033327890560030937,0.00016942332634547105,0.0003208266571164131,0.0003920303036769231,0.0006701289288078746,0.0004553885664790869,-0.0003032931126654148,-0.0003172922200368096,-0.0007321982217642168,-0.0002628363048036893,0.000025909558947508533,0.000012995675206184387,0.00021314895517813662,0.00014694895556507012,-0.0005695708484078447,-0.0007204127808411916,-0.0009564701467752457,-0.0003083520956958334,-0.0004427585129936536,-0.00032512083028753597,-0.001676093942175309,-0.0008239218150265515,-0.0022389901763138673,-0.0017367595185836155,-0.002375156502239406,-0.0023085797050346932,-0.002656766213476658,-0.0004919991479255259,-6.608199328184128e-6,0.00018312915926799178,-0.0004196654384334882,-0.00009417370893061161,-0.00015420125176509222,-0.0012138500072372456,-0.0012851283730318148,-0.0020116009594251714,-0.0009807709138840437,-0.0015458424265185993,0.00036241797109444934,-0.00001529666284720103,-0.001113997830543667,-0.0013091445434838533,-0.001246352563612163,-0.0015075859070445101,-0.002400997376147037,-0.0017381213838234544,-0.002776971251781409,-0.0017983903526328504,-0.001771697774529457,-0.0012855683453381062,-0.001490669014553229,-0.0009322147937685562,-0.0011806361386940505,-0.0006035355618223548,-0.00009712378960102797,0.00005314755253493786,-0.0001726006130411406,-0.0003332034102641046,-0.0013374517729971558,-0.00037963310023769736,-0.0010248786226535838,-0.00011428647364179294,-0.0002880542694280545,-0.00041323754703626037,-0.0016283123986795545,-0.0010148713869663577,-0.0015903195599094033,-0.000861421178948755,-0.0003728566225618124,-0.00020086020231246948,-0.0001228508675315728,0.00012960971798747778,3.6380564173062644e-6,0.000032691129793723427,0.00005972897633910179,-0.000019714546700318653,0.00024156412109732628,-0.000023947563022375107,-0.0003691982904759546,-0.0013346545165404677,-0.0006274404004216194,-0.0002060603922776257,0.00018345502515633902,-0.0004950815346091986,-0.00029579814872704446,-0.0005341157084330916,0.00007524310300747554,0.0006463919999077916,0.00021886778995394707,0.0012035919353365898,0.000019501661881804466,0.0001746252334366242,-0.0003470121106753747,-0.00048680293063322705,-0.0009707123584424456,-0.0016066742246039212,-0.0017383370529084157,0.0031178341014310718,0.0027998494430600354,0.002239754772745073,0.00016949698328971863,0.0000411780783906579,0.00007102264013762276,-0.00021342999146630368,-0.0009426440131695321,-0.001175549967835347,-0.00147642728794987,-0.001978128799237311,-0.0021384465120111904,-0.0018200874328613281,-0.0016746373536686103,-0.0011817087264110644,-0.0007072777176896731,-0.001439472990265737,-0.0011666979795942705,-0.0010256581129700255,-0.0014607311847309272,-0.002551441236088673,-0.00108027970418334,-0.0009588093574469289,0.000272338860668242,0.00024103280156850815,-0.0004859527883430322,-0.0006615752742315332,-0.0015254382354517777,-0.0006800734748442968,-0.001478250805424371,-0.0017434065618241827,-0.0012256741368522246,-0.001182381296530366,-0.0006921957635010282,-0.00038097251672297716,-0.0003597964532673359,-0.0006306148910274109,-0.0019661769232091806,-0.0014606443389008443,-0.001350364958246549,-0.0010407442847887676,-0.001301392602423827,-0.00048401990594963234,-0.0006986670196056366,-0.00025209712718303007,-0.0005936323044200739,-0.0008081301348283887,-0.0013182575542790194,-0.0013167272554710507,-0.0016775824333308265,-0.001236170472111553,-0.0009626922740911444,-0.0002717564154105882,0.0002414305975738292,0.0003135651058983058,0.00018483465343403319,-0.0002251864061690867,-0.0003831792006773564,0.000059465125862819455,-0.0002640958409756422,0.00009924162683698039,-0.0007164819398894906,-0.0006224443980803093,-0.0010121423401869833,-0.0015012859754885237,-0.001570724571744601,-0.001124745118431747,-0.0007912826646740238,0.00016390074839970717,-0.0002629517888029416,0.0001980100835983952,0.0002545444100784759,0.0002653249927485983,0.00033376280528803665,0.000429636740591377,0.00041476225790878135,-0.001564030263883372,-0.001655814005061984,-0.0011215541356553633,-0.00040789701355000335,-0.000102707805732886,-0.00039259290012220543,-0.0003741899660478036,-0.0007662103356172641,0.00017338228644803166,0.00004719104617834091,0.00006130065958132036,0.00024062173906713724,0.0002880383205289642,-0.0006280307037134966,-0.0005038545641582459,0.000021683207402626675,-0.0015486271198218067,-0.00148096257665505,-0.0027699964897086224,-0.002445728847912202,-0.0025559887484026453,-0.0025774478369082012,-0.0016678774651760857,0.00010642549023032188,0.0004182824244101842,0.0011794338934123516,0.0011416641063988209,0.0003904233065744241,0.0004849958543976148,-0.00017841804462174574,-0.0001365786495928963,-0.0011799675412476063,-0.0007783503970131278,-0.0010686734070380528,-0.00024353736080229282,-0.00003301817923784256,0.00004722216787437598,-0.0002198982983827591,0.000033088416482011475,0.001063744033065935,0.0012095505371689796,0.0013734197806722175,0.0022623763070441782,0.0015376577309022348,0.0013669953914359212,0.0007903468795120716,-0.0002221161654839913,0.0018133691822489102,0.002175172984910508,0.0027067761693615466,0.0020146655927722654,0.00208498732051036,0.0008894363806272546,0.0003471039041566352,0.0007147064704137543,0.0017100929593046506,0.002134399915424486,0.001513026189059019,0.0014193484870096047,0.0013573199951982435,0.001851302416374286,0.0016853978935008247,0.0010964451503241435,0.0010860853653866798,0.0009504909006257852,0.000577292637899518,0.0005864105963458618,0.0011707205170144637,0.00048679607425583526,0.0004709905479103327,0.00003572176986684402,0.00031760174412435543,-0.00016118931428839764,0.00035353457496967167,0.0001268112100660801,-0.00012298745180790624,-0.00023080486183365187,-0.001231300993822515,-0.0014171191724017262,-0.0012271733430679888,-0.00044701388105750084,0.00002323292816678683,0.0015116441063582897,0.001690591297422846,0.0016554205988844235,0.001932048238813877,0.0008493760833516717,0.0015232976681242387,0.0009535086282994598,0.0017836872721090913,0.001106539275497198,0.0014164734942217667,0.0013055520442624886,0.0014778822660446167,0.0015912391245365143,0.0021833323019867143,0.0017161012680541414,0.0016338723071385175,0.001163125775444011,0.0017142368014901876,0.0010012260948618252,0.001647539339804401,0.0014136986574158072,0.0018873284958923857,0.0019286848449458678,0.002182434235389034,0.002137137926183641,0.001234622672200203,0.0013447892076025407,0.0010258732363581657,0.0004785869872042288,-0.000022546097170561552,-0.0007288658137743672,-0.0006175331751971195,-0.0015205702317568164,-0.00017693875027665248,-0.00007912351672227184,0.0003500891277023281,0.0005956943690155944,0.0013062156407007326,0.0020527918435012302,0.0026886441434423127,0.003315562071899573,0.003820813416192929,-0.0021252332371659577,-0.0021368846064433455,-0.0005563408291588227,0.0000608637152860562,0.000024766195565462112,0.00020029558800160885,0.00003996896945560972,0.00039516038183743757,0.00023531968084474406,0.0008309788342254857,0.0008638739551921996,0.0011356219959755738,0.0008003844801957408,0.0003269569327433904,-0.00021281080747333667,-0.0002246504979363332,1.1363978652904432e-6,0.0002439631304393212,0.0005746715275260309,0.00032971213416506845,0.0006513743040462335,-0.000832742351728181,-0.0011376325079860787,-0.0008889400245000919,-0.0007960721268318594,0.0009194953599944711,0.0022006852086633444,0.0018880192462044458,0.0020357073905567327,0.001230421767104417,-0.00036254765776296455,0.0002188024421532949,-0.00021570540654162565,0.00008434443346535166,0.00005765657018249234,0.00006698232997829716,-0.00019610739157845578,0.0013152050281253953,0.0006496999218749503,0.000636461346099774,0.0007389399688690901,-0.00006462183470527331,-0.0001760424735645453,-0.0015830080956220627,-0.0015854242568214734,-0.0013739321536074083,-0.001004385839526852,0.00018489360809326172,0.0008038996020331979,0.0008288488412896792,0.0010745863934668403,0.0005622597721715769,0.0003582033483932416,0.00021277515528102717,0.00011816005765770872,0.000026580217915276687,-0.0000729848458528674,-0.00012099756228659923],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y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z\":[0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.50980395,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.47058824,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.52156866,0.5254902,0.5254902,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.53333336,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.47843137,0.47843137,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.52156866,0.52156866,0.52156866,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49803922,0.49803922,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.50980395,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.5058824,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.53333336,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5372549,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.47843137,0.47843137,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5137255,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.52156866,0.52156866,0.52156866,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49803922,0.49803922,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5176471,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.5058824,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.43137255,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5529412,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5294118,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.48235294,0.48235294,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.54901963,0.50980395,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5176471,0.5176471,0.5176471,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.50980395,0.50980395,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5411765,0.4745098,0.4745098,0.4745098,0.4745098,0.4745098,0.52156866,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.23529412,0.23529412,0.23529412,0.23529412,0.23529412,0.23529412,0.23529412,0.23529412,0.23529412,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.69411767,0.5372549,0.5372549,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5921569,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.50980395,0.50980395,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.6039216,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.5372549,0.6039216,0.6039216,0.6039216,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.52156866,0.52156866,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5411765,0.5764706,0.3372549,0.3372549,0.3372549,0.3372549,0.3372549,0.5176471,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.67058825,0.16078432,0.16078432,0.16078432,0.16078432,0.16078432,0.16078432,0.16078432,0.16078432,0.16078432,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.4509804,0.5411765,0.5411765,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.2784314,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.45882353,0.6666667,0.6666667,0.6666667,0.6666667,0.6666667,0.6666667,0.6666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.42745098,0.42745098,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.37254903,0.40392157,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.68235296,0.5529412,0.5529412,0.5529412,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.6156863,0.57254905,0.57254905,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.3529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.43529412,0.3647059,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4627451,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.4392157,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.5294118,0.54509807,0.54509807,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.41960785,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.46666667,0.49411765,0.49411765,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5019608,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5058824,0.5058824,0.5058824,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.47843137,0.47843137,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.47058824,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.4862745,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.44313726,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.53333336,0.53333336,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.5254902,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49411765,0.49411765,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.5137255,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.47843137,0.52156866,0.52156866,0.52156866,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.49411765,0.4862745,0.4862745,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.50980395,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.5019608,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.45490196,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5254902,0.5372549,0.5372549,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.49019608,0.5254902,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5176471,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.5058824,0.50980395,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.48235294,0.5137255,0.5137255,0.5137255,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.49803922,0.5019608,0.5019608,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.50980395,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.4862745,0.50980395],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"Channel\"}},\"zaxis\":{\"title\":{\"text\":\"Signal\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"SHAP Importance\"}},\"colorscale\":[[0.0,\"#000004\"],[0.1111111111111111,\"#1b0c41\"],[0.2222222222222222,\"#4a0c6b\"],[0.3333333333333333,\"#781c6d\"],[0.4444444444444444,\"#a52c60\"],[0.5555555555555556,\"#cf4446\"],[0.6666666666666666,\"#ed6925\"],[0.7777777777777778,\"#fb9b06\"],[0.8888888888888888,\"#f7d13d\"],[1.0,\"#fcffa4\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"4D EMG SHAP Visualization\"}},                        {\"responsive\": true}                    ).then(function(){\\n                            \\nvar gd = document.getElementById(\\'4428005a-4958-4d18-926e-89ade4e0c24a\\');\\nvar x = new MutationObserver(function (mutations, observer) {{\\n        var display = window.getComputedStyle(gd).display;\\n        if (!display || display === \\'none\\') {{\\n            console.log([gd, \\'removed!\\']);\\n            Plotly.purge(gd);\\n            observer.disconnect();\\n        }}\\n}});\\n\\n// Listen for the removal of the full notebook cells\\nvar notebookContainer = gd.closest(\\'#notebook-container\\');\\nif (notebookContainer) {{\\n    x.observe(notebookContainer, {childList: true});\\n}}\\n\\n// Listen for the clearing of the current output cell\\nvar outputEl = gd.closest(\\'.output\\');\\nif (outputEl) {{\\n    x.observe(outputEl, {childList: true});\\n}}\\n\\n                        })                };                            </script>        </div>\\n</body>\\n</html>'}\n",
            "[INFO] Saved fallback HTML plot: 4D_EMG_SHAP_Visualization.html\n",
            "[INFO] Saved 4D SHAP surface plot to: shap_4d_surface.html\n",
            "[SHAP4D] Channel Variance: 0.0000\n",
            "[SHAP4D] Temporal Entropy: 2.1875\n",
            "[SHAP4D] Mutual Info: 0.1859\n",
            "[SHAP4D] PCA Alignment: 0.0000\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "Figure(640x480)\n",
            "[INFO] Saved SHAP heatmap to: shap_temporal_heatmap.png\n",
            "\n",
            "📊 Training baseline model for SHAP comparison...\n",
            "[INFO] Saved SHAP heatmap to: shap_heatmap_baseline.png\n",
            "\n",
            "🔍 Running ablation: shuffling SHAP-important segments...\n",
            "[Ablation] Accuracy post SHAP shuffle: 1.0000\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_histograms_impl.py:895: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n",
            "[SHAP Ablation] KL Divergence (Original vs Post-Ablation): nan\n",
            "/content/extddivesify/diversify/train.py:260: MatplotlibDeprecationWarning:\n",
            "\n",
            "The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "\n",
            "Figure(800x500)\n",
            "Figure(1000x500)\n",
            "[SHAP vs Confidence] Pearson Correlation: 0.3705 (p=0.2919)\n",
            "Figure(600x500)\n",
            "\n",
            "🛠 Real-world Context: EMG classification can support gesture-based interfaces in prosthetics or rehabilitation systems, and insights from SHAP improve trust in deployed models.\n",
            "Figure(1200x800)\n"
          ]
        }
      ]
    }
  ]
}
